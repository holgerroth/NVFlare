{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Protein Downstream Fine-tuning\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>NOTE</b> This notebook was tested on a single A1000 GPU and is compatible with BioNeMo Framework v2.3. To leverage additional or higher-performance GPUs, you can modify the configuration files and simulation script to accommodate multiple devices and increase thread utilization respectively.</div>\n",
    "\n",
    "The example datasets used here are made available by [Therapeutics Data Commons](https://tdcommons.ai/) through PyTDC.\n",
    "\n",
    "This example shows three different downstream tasks for fine-tuning a BioNeMo ESM-style model on different datasets.\n",
    "We separate the scripts and job configurations into three folders based on the dataset names:\n",
    "\n",
    "\n",
    "1. `tap`: therapeutic antibody profiling\"\n",
    "2. `sabdab`: SAbDab: the structural antibody database\"\n",
    "3. `scl`: \"subcellular location prediction\"\n",
    "\n",
    "## Setup\n",
    "\n",
    "Ensure that you have read through the Getting Started section, can run the BioNeMo Framework docker container, and have configured the NGC Command Line Interface (CLI) within the container. It is assumed that this notebook is being executed from within the container.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>NOTE</b> Some of the cells below generate long text output.  We're using <pre>%%capture --no-display --no-stderr cell_output</pre> to suppress this output.  Comment or delete this line in the cells below to restore full output.</div>\n",
    "\n",
    "### Import and install all required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/looseversion-1.3.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/opt_einsum-3.4.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/dill-0.3.9-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_utilities-0.12.0.dev0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/nvfuser-0.2.23a0+6627725-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.0.dev0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting fuzzywuzzy\n",
      "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting PyTDC\n",
      "  Downloading pytdc-1.1.14.tar.gz (151 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25done\n",
      "\u001b[?25hDownloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
      "Building wheels for collected packages: PyTDC\n",
      "  Building wheel for PyTDC (setup.py) ... \u001b[done\n",
      "\u001b[?25h  Created wheel for PyTDC: filename=pytdc-1.1.14-py3-none-any.whl size=189444 sha256=b05cd67fbf06f014b23b1aefe67db80cfd42fa128050b7017a580b91d3ec393f\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-wq_z7j5j/wheels/3e/34/ba/e5a6104fb7c17d69daeed0e73d413f296b2971ca6255a77650\n",
      "Successfully built PyTDC\n",
      "Installing collected packages: PyTDC, fuzzywuzzy\n",
      "Successfully installed PyTDC-1.1.14 fuzzywuzzy-0.18.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/looseversion-1.3.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/opt_einsum-3.4.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/dill-0.3.9-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_utilities-0.12.0.dev0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/nvfuser-0.2.23a0+6627725-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.0.dev0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "\u001b[31mERROR: Ignored the following versions that require a different python version: 1.1.0 Requires-Python >=3.8,<3.9; 1.1.1 Requires-Python ==3.8.10; 2.1.0 Requires-Python >=3.7,<3.9; 2.1.1 Requires-Python >=3.7,<3.9; 2.1.2 Requires-Python >=3.7,<3.9; 2.1.3 Requires-Python >=3.7,<3.9; 2.1.4 Requires-Python >=3.7,<3.9; 2.2.0 Requires-Python >=3.7,<3.9; 2.2.0rc1 Requires-Python >=3.7,<3.9; 2.2.1 Requires-Python >=3.7,<3.9; 2.2.1rc1 Requires-Python >=3.7,<3.9; 2.2.1rc4 Requires-Python >=3.7,<3.9; 2.2.1rc5 Requires-Python >=3.7,<3.9; 2.2.1rc6 Requires-Python >=3.7,<3.9; 2.2.1rc7 Requires-Python >=3.7,<3.9; 2.2.1rc8 Requires-Python >=3.7,<3.9; 2.2.1rc9 Requires-Python >=3.7,<3.9; 2.2.2 Requires-Python >=3.7,<3.9; 2.2.3 Requires-Python >=3.7,<3.9; 2.2.4 Requires-Python >=3.7,<3.9; 2.2.5 Requires-Python >=3.7,<3.9; 2.2.6 Requires-Python >=3.7,<3.9\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement nvflare~=2.6 (from versions: 0.1.3, 0.9.0, 1.0.0, 1.0.1, 1.0.2, 2.0.0, 2.0.1, 2.0.2, 2.0.3, 2.0.4, 2.0.5, 2.0.6, 2.0.7, 2.0.8, 2.0.9, 2.0.10, 2.0.11, 2.0.12, 2.0.13, 2.0.14, 2.0.15, 2.0.16, 2.0.18, 2.0.19, 2.3.0rc1, 2.3.0rc2, 2.3.0rc3, 2.3.0rc4, 2.3.0rc5, 2.3.0rc6, 2.3.0rc7, 2.3.0rc8, 2.3.0, 2.3.1, 2.3.2, 2.3.3, 2.3.4, 2.3.5, 2.3.6, 2.3.7, 2.3.8, 2.3.9, 2.3.10, 2.3.11, 2.3.12, 2.4.0rc1, 2.4.0rc2, 2.4.0rc3, 2.4.0rc4, 2.4.0rc5, 2.4.0rc6, 2.4.0rc7, 2.4.0rc8, 2.4.0rc9, 2.4.0, 2.4.1rc1, 2.4.1rc2, 2.4.1rc3, 2.4.1rc4, 2.4.1rc5, 2.4.1rc6, 2.4.1rc7, 2.4.1rc8, 2.4.1, 2.4.2rc3, 2.4.2, 2.5.0rc1, 2.5.0rc2, 2.5.0rc3, 2.5.0rc4, 2.5.0rc5, 2.5.0rc6, 2.5.0rc7, 2.5.0rc8, 2.5.0rc9, 2.5.0rc10, 2.5.0rc11, 2.5.0rc12, 2.5.0, 2.5.1rc1, 2.5.1rc2, 2.5.1, 2.5.2)\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for nvflare~=2.6\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# %%capture --no-display --no-stderr cell_output\n",
    "! pip install fuzzywuzzy PyTDC --no-dependencies  # install tdc without dependencies to avoid version conflicts in the BioNeMo container\n",
    "! pip install nvflare~=2.6\n",
    "#! pip install biopython\n",
    "#! pip install scikit-learn\n",
    "#! pip install matplotlib\n",
    "#! pip install protobuf==3.20\n",
    "#! pip install huggingface-hub==0.22.0\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Task 1: Cross-endpoint multi-task fitting\n",
    "\n",
    "#### Data: Five computational developability guidelines for therapeutic antibody profiling\n",
    "See https://tdcommons.ai/single_pred_tasks/develop/#tap\n",
    "- 241 Antibodies (both chains)\n",
    "\n",
    "#### Task Description: *Regression*. \n",
    "Given the antibody's heavy chain and light chain sequence, predict its developability. The input X is a list of two sequences where the first is the heavy chain and the second light chain.\n",
    "\n",
    "Includes five metrics measuring developability of an antibody: \n",
    " - Complementarity-determining regions (CDR) length - Trivial (excluded)\n",
    " - patches of surface hydrophobicity (PSH)\n",
    " - patches of positive charge (PPC)\n",
    " - patches of negative charge (PNC)\n",
    " - structural Fv charge symmetry parameter (SFvCSP)\n",
    "\n",
    "In the data preparation script, one can choose between uniform sampling of the data among clients and\n",
    "heterogeneous data splits using a Dirichlet sampling strategy. \n",
    "Here, different values of alpha control the level of heterogeneity. Below, we show a Dirichlet sampling of `alpha=1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found local copy...\n",
      "Loading...\n",
      "Done!\n",
      "Found local copy...\n",
      "Loading...\n",
      "Done!\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\", line 3805, in get_loc\n",
      "    return self._engine.get_loc(casted_key)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"index.pyx\", line 167, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"index.pyx\", line 196, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7081, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7089, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "KeyError: 'labels'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/bionemo_nvflare_examples/downstream/tap/prepare_tap_data.py\", line 192, in <module>\n",
      "    main()\n",
      "  File \"/bionemo_nvflare_examples/downstream/tap/prepare_tap_data.py\", line 94, in main\n",
      "    train_df[label_name] = train_split[\"labels\"]\n",
      "                           ~~~~~~~~~~~^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\", line 4102, in __getitem__\n",
      "    indexer = self.columns.get_loc(key)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\", line 3812, in get_loc\n",
      "    raise KeyError(key) from err\n",
      "KeyError: 'labels'\n"
     ]
    }
   ],
   "source": [
    "! cd /bionemo_nvflare_examples/downstream/tap && python prepare_tap_data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|                                Uniform sampling                                 |                                    Dirichlet sampling                                     |\n",
    "|:-------------------------------------------------------------------------------:|:-----------------------------------------------------------------------------------------:|\n",
    "| <img src=\"./tap/figs/tap_uniform.svg\" alt=\"Uniform data sampling\" width=\"150\"/> | <img src=\"./tap/figs/tap_alpha1.0.svg\" alt=\"Dirichlet sampling (alpha=1.0)\" width=\"150\"/> |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run training (central, local, & FL)**\n",
    "\n",
    "You can change the FL job that's going to be simulated by changing the arguments of `run_sim_tap.py` script. You can choose which ESM2 model to download (8M or 650M parameters). The ESM2 finetuning arguments such as learning rate and others can be modified inside the script itself.\n",
    "\n",
    "First, let's check its arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: run_sim_tap.py [-h] [--num_clients NUM_CLIENTS]\n",
      "                      [--num_rounds NUM_ROUNDS] [--local_steps LOCAL_STEPS]\n",
      "                      [--train_script TRAIN_SCRIPT] [--exp_name EXP_NAME]\n",
      "                      [--model {8m,650m,3b}] [--sim_gpus SIM_GPUS]\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --num_clients NUM_CLIENTS\n",
      "                        Number of clients\n",
      "  --num_rounds NUM_ROUNDS\n",
      "                        Number of rounds\n",
      "  --local_steps LOCAL_STEPS\n",
      "                        Number of rounds\n",
      "  --train_script TRAIN_SCRIPT\n",
      "                        Training script\n",
      "  --exp_name EXP_NAME   Job name prefix\n",
      "  --model {8m,650m,3b}  ESM2 model\n",
      "  --sim_gpus SIM_GPUS   GPU indexes to simulate clients, e.g., '0,1,2,3' if\n",
      "                        you want to run 4 clients, each on a separate GPU. By\n",
      "                        default run all clients on the same GPU 0.\n"
     ]
    }
   ],
   "source": [
    "!cd /bionemo_nvflare_examples/downstream/tap && python run_sim_tap.py --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Central training**\n",
    "\n",
    "To simulate central training, we use one client, running one round of training for several steps. Note that if the `--exp_name` argument contains `\"central\"`, the combined training dataset is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/bionemo_nvflare_examples/downstream/sabdab/run_sim_sabdab.py\", line 15, in <module>\n",
      "    from nvflare.job_config.script_runner import BaseScriptRunner\n",
      "ModuleNotFoundError: No module named 'nvflare'\n"
     ]
    }
   ],
   "source": [
    "!python run_sim_tap.py --num_clients=1 --num_rounds=1 --local_steps=300 --exp_name central"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Local training**\n",
    "\n",
    "To simulate central training, we use six clients, each running one round of training for several steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/bionemo_nvflare_examples/downstream/sabdab/run_sim_sabdab.py\", line 15, in <module>\n",
      "    from nvflare.job_config.script_runner import BaseScriptRunner\n",
      "ModuleNotFoundError: No module named 'nvflare'\n"
     ]
    }
   ],
   "source": [
    "!python run_sim_tap.py --num_clients=6 --num_rounds=1 --local_steps=300 --exp_name local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Federated training with FedAvg**\n",
    "\n",
    "To simulate federated training, we use six clients, running several rounds with FedAvg, each with a smaller number of local steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/bionemo_nvflare_examples/downstream/sabdab/run_sim_sabdab.py\", line 15, in <module>\n",
      "    from nvflare.job_config.script_runner import BaseScriptRunner\n",
      "ModuleNotFoundError: No module named 'nvflare'\n"
     ]
    }
   ],
   "source": [
    "!python run_sim_tap.py --num_clients=6 --num_rounds=30 --local_steps=10 --exp_name fedavg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Task 2: Cross-compound task fitting\n",
    "\n",
    "#### Data: Predicting Antibody Developability from Sequence using Machine Learning\n",
    "See https://tdcommons.ai/single_pred_tasks/develop/#sabdab-chen-et-al\n",
    "- 2,409 Antibodies (both chains)\n",
    "\n",
    "#### Task Description: *Binary classification*. \n",
    "Given the antibody's heavy chain and light chain sequence, predict its developability. The input X is a list of two sequences where the first is the heavy chain and the second light chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "100%|███████████████████████████████████████| 601k/601k [00:00<00:00, 2.35MiB/s]\n",
      "Loading...\n",
      "Done!\n",
      "Sampling with alpha=1.0\n",
      "Save 80 training proteins for site-1 (frac=0.041)\n",
      "Save 365 training proteins for site-2 (frac=0.190)\n",
      "Save 216 training proteins for site-3 (frac=0.112)\n",
      "Save 578 training proteins for site-4 (frac=0.300)\n",
      "Save 568 training proteins for site-5 (frac=0.295)\n",
      "Save 119 training proteins for site-6 (frac=0.062)\n",
      "Saved 1927 training and 482 testing proteins.\n",
      "  TRAIN Pos/Neg ratio: neg=1561, pos=366: 0.234\n",
      "  TRAIN Trivial accuracy: 0.190\n",
      "  TEST Pos/Neg ratio: neg=366, pos=116: 0.317\n",
      "  TEST Trivial accuracy: 0.241\n",
      "[[       nan 0.04657534 0.02314815 0.0449827  0.04929577 0.03361345]\n",
      " [       nan        nan 0.18055556 0.17128028 0.19542254 0.18487395]\n",
      " [       nan        nan        nan 0.11591696 0.10211268 0.08403361]\n",
      " [       nan        nan        nan        nan 0.28521127 0.32773109]\n",
      " [       nan        nan        nan        nan        nan 0.28571429]\n",
      " [       nan        nan        nan        nan        nan        nan]]\n",
      "Avg. overlap: 14.20%\n"
     ]
    }
   ],
   "source": [
    "# you may need to fix these paths to your own scripts\n",
    "! cd /bionemo_nvflare_examples/downstream/sabdab && python prepare_sabdab_data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we are using the Dirichlet sampling strategy to generate heterogeneous data distributions among clients.\n",
    "Lower values of `alpha` generate higher levels of heterogeneity.\n",
    "\n",
    "|                                            Alpha 10.0                                             |                                            Alpha 1.0                                            |\n",
    "|:-------------------------------------------------------------------------------------------------:|:-----------------------------------------------------------------------------------------------:|\n",
    "| <img src=\"./sabdab/figs/sabdab_alpha10.0.svg\" alt=\"Dirichlet sampling (alpha=10.0)\" width=\"150\"/> | <img src=\"./sabdab/figs/sabdab_alpha1.0.svg\" alt=\"Dirichlet sampling (alpha=1.0)\" width=\"150\"/> |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Run training (central, local, & FL)**\n",
    "\n",
    "You can change the FL job that's going to be simulated by changing the arguments of `run_sim_sabdab.py` script. You can choose which ESM2 model to download (8M or 650M parameters). The ESM2 finetuning arguments such as learning rate and others can be modified inside the script itself.\n",
    "\n",
    "First, let's check its arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/bionemo_nvflare_examples/downstream/sabdab/run_sim_sabdab.py\", line 15, in <module>\n",
      "    from nvflare.job_config.script_runner import BaseScriptRunner\n",
      "ModuleNotFoundError: No module named 'nvflare'\n"
     ]
    }
   ],
   "source": [
    "!python run_sim_sabdab.py --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Central training**\n",
    "\n",
    "To simulate central training, we use one client, running one round of training for several steps. Note that if the `--exp_name` argument contains `\"central\"`, the combined training dataset is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/bionemo_nvflare_examples/downstream/sabdab/run_sim_sabdab.py\", line 15, in <module>\n",
      "    from nvflare.job_config.script_runner import BaseScriptRunner\n",
      "ModuleNotFoundError: No module named 'nvflare'\n"
     ]
    }
   ],
   "source": [
    "!python run_sim_sabdab.py --num_clients=1 --num_rounds=1 --local_steps=300 --exp_name central"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Local training**\n",
    "\n",
    "To simulate central training, we use six clients, each running one round of training for several steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/bionemo_nvflare_examples/downstream/sabdab/run_sim_sabdab.py\", line 15, in <module>\n",
      "    from nvflare.job_config.script_runner import BaseScriptRunner\n",
      "ModuleNotFoundError: No module named 'nvflare'\n"
     ]
    }
   ],
   "source": [
    "!python run_sim_sabdab.py --num_clients=6 --num_rounds=1 --local_steps=300 --exp_name local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Federated training with FedAvg**\n",
    "\n",
    "To simulate federated training, we use six clients, running several rounds with FedAvg, each with a smaller number of local steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/bionemo_nvflare_examples/downstream/sabdab/run_sim_sabdab.py\", line 15, in <module>\n",
      "    from nvflare.job_config.script_runner import BaseScriptRunner\n",
      "ModuleNotFoundError: No module named 'nvflare'\n"
     ]
    }
   ],
   "source": [
    "!python run_sim_sabdab.py --num_clients=6 --num_rounds=30 --local_steps=10 --exp_name fedavg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results with heterogeneous data sampling (alpha=10.0)\n",
    "| Setting | Accuracy  |\n",
    "|:-------:|:---------:|\n",
    "|  Local  |   0.821   |\n",
    "|   FedAvg    | **0.833** |\n",
    "\n",
    "#### Results with heterogeneous data sampling (alpha=1.0)\n",
    "| Setting | Accuracy  |\n",
    "|:-------:|:---------:|\n",
    "|  Local  |   0.813   |\n",
    "|   FedAvg    | **0.835** |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Task 3. Subcellular location prediction with ESM2nv 650M\n",
    "In this example, we use the `--encoder-frozen` option inside the `run_sim_scl.py` script. You can specify different base ESM2 models using the `--model` option.\n",
    "Follow the data download and preparation in [task_fitting.ipynb](../task_fitting/task_fitting.ipynb).\n",
    "\n",
    "Here, we use a heterogeneous sampling with `alpha=1.0`.\n",
    "\n",
    "<img src=\"./scl/figs/scl_alpha1.0.svg\" alt=\"Dirichlet sampling (alpha=10.0)\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Local training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd /bionemo_nvflare_examples/downstream/scl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for this to work run the task_fitting notebook first in ../nvflare_with_bionemo/task_fitting/task_fitting.ipynb in order to download the SCL dataset\n",
    "!python run_sim_scl.py --num_clients=3 --num_rounds=1 --local_steps=300 --exp_name local --model \"650m\" --sim_gpus=\"0,1,2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Federated training with FedAvg**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python run_sim_scl.py --num_clients=3 --num_rounds=30 --local_steps=10 --exp_name fedavg --model \"650m\" --sim_gpus=\"0,1,2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, you can switch between local and FL jobs by modifying the `run_sim_scl.py` script.\n",
    "\n",
    "#### Results with heterogeneous data sampling (alpha=10.0)\n",
    "| Setting | Accuracy  |\n",
    "|:-------:|:---------:|\n",
    "|  Local  |   0.773   |\n",
    "|   FedAvg    | **0.776** |\n",
    "\n",
    "\n",
    "<img src=\"./scl/figs/scl_results.svg\" alt=\"Dirichlet sampling (alpha=1.0)\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
