{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "469aebc8",
   "metadata": {},
   "source": [
    "# Federated Protein Embeddings and Task Model Fitting with BioNeMo\n",
    "\n",
    "This example notebook shows how to obtain protein learned representations in the form of embeddings using the ESM-1nv pre-trained model. The model is trained with NVIDIA's BioNeMo framework for Large Language Model training and inference. For more details, please visit NVIDIA BioNeMo Service at https://www.nvidia.com/en-us/gpu-cloud/bionemo.\n",
    "\n",
    "This notebook will walk you through the task fitting workflow in the following sections:\n",
    "\n",
    "* \n",
    "*\n",
    "*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b62a6d",
   "metadata": {},
   "source": [
    "### Install requirements\n",
    "Let's start by installing and importing library dependencies. We'll use requests to interact with the BioNeMo service, BioPython to parse FASTA sequences into SeqRecord objects, scikit-learn for classification tasks, and matplotlib for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1772ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt\n",
    "!pip install -e /media/hroth/NVIDIA/home_old/hroth/Code2/nvflare/bionemo_nvflare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0af80f8",
   "metadata": {},
   "source": [
    "### Obtaining the protein embeddings using the BioNeMo ESM-1nv model\n",
    "Using BioNeMo, users can obtain numerical vector representations of protein sequences called embeddings. Protein embeddings can then be used for visualization or making downstream predictions.\n",
    "\n",
    "Here we are interested in training a neural network to predict subcellular location from an embedding.\n",
    "\n",
    "The data we will be using comes from the paper [Light attention predicts protein location from the language of life](https://academic.oup.com/bioinformaticsadvances/article/1/1/vbab035/6432029) by StÃ¤rk et al. In this paper, the authors developed a machine learning algorithm to predict the subcellular location of proteins from sequence through protein langage models that are similar to those hosted by BioNeMo. Protein subcellular location refers to where the protein localizes in the cell, for example a protein my be expressed in the Nucleus or in the Cytoplasm. Knowing where proteins localize can provide insights into the underlying mechanisms of cellular processes and help identify potential targets for drug development. The following image includes a few examples of subcellular locations in an animal cell:\n",
    "\n",
    "\n",
    "(Image freely available at https://pixabay.com/images/id-48542)\n",
    "\n",
    "### Dataset sourcing\n",
    "For our target input sequences, we will point to FASTA sequences in a benchmark dataset called Fitness Landscape Inference for Proteins (FLIP). FLIP encompasses experimental data across adeno-associated virus stability for gene therapy, protein domain B1 stability and immunoglobulin binding, and thermostability from multiple protein families."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0700e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example protein dataset location\n",
    "fasta_url= \"http://data.bioembeddings.com/public/FLIP/fasta/scl/mixed_soft.fasta\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacb9600",
   "metadata": {},
   "source": [
    "First, we define the source of example protein dataset with the FASTA sequences. This data follows the [biotrainer](https://github.com/sacdallago/biotrainer/blob/main/docs/data_standardization.md) standard, so it includes information about the class in the FASTA header, and the protein sequence. Here are two example sequences in this file:\n",
    "\n",
    "```\n",
    ">Sequence1 TARGET=Cell_membrane SET=train VALIDATION=False\n",
    "MMKTLSSGNCTLNVPAKNSYRMVVLGASRVGKSSIVSRFLNGRFEDQYTPTIEDFHRKVYNIHGDMYQLDILDTSGNHPFPAM\n",
    "RRLSILTGDVFILVFSLDSRESFDEVKRLQKQILEVKSCLKNKTKEAAELPMVICGNKNDHSELCRQVPAMEAELLVSGDENC\n",
    "AYFEVSAKKNTNVNEMFYVLFSMAKLPHEMSPALHHKISVQYGDAFHPRPFCMRRTKVAGAYGMVSPFARRPSVNSDLKYIKA\n",
    "KVLREGQARERDKCSIQ\n",
    ">Sequence4833 TARGET=Nucleus SET=train VALIDATION=False\n",
    "MARTKQTARKSTGGKAPRKQLATKAARKSAPATGGVKKPHRFRPGTVALREIRKYQKSTELLIRKLPFQRLVREIAQDFKTDL\n",
    "RFQSSAVAALQEAAEAYLVGLFEDTNLCAIHAKRVTIMPKDIQLARRIRGERA\n",
    "Note the following attributes in the FASTA header:\n",
    "```\n",
    "\n",
    "* `TARGET` attribute holds the subcellular location classification for the sequence, for instance Cell_membrane and Nucleus. This dataset includes a total of ten subcellelular location classes -- more on that below.\n",
    "* `SET` attribute defines whether the sequence should be used for training (train) or testing (test)\n",
    "* `VALIDATION` attribute defines whether the sequence should be used for validation (all sequences where this is True are also in set=train)\n",
    "\n",
    "### Downloading the protein sequences and subcellular location annotations\n",
    "In this step we download the FASTA file defined above and parse the sequences into a list of BioPython SeqRecord objects.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7829e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import requests\n",
    "from Bio import SeqIO\n",
    "\n",
    "# Download the FASTA file from FLIP: https://github.com/J-SNACKKB/FLIP/tree/main/splits/scl\n",
    "fasta_content = requests.get(fasta_url, headers={\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x86)'\n",
    "}).content.decode('utf-8')\n",
    "fasta_stream = io.StringIO(fasta_content)\n",
    "\n",
    "# Obtain a list of SeqRecords/proteins which contain sequence and attributes\n",
    "# from the FASTA header\n",
    "proteins = list(SeqIO.parse(fasta_stream, \"fasta\"))\n",
    "print(f\"Downloaded {len(proteins)} sequences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92aedc48",
   "metadata": {},
   "source": [
    "### Data splitting\n",
    "Next, we prepare the data for simulating federated learning using `n_clients`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf4bd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clients = 3\n",
    "# limiting to the proteins with sequence length<512 for embedding queries\n",
    "MAX_SEQUENCE_LEN = 512\n",
    "seed=0\n",
    "out_dir = \"/tmp/fasta/mixed_soft\"\n",
    "split_alpha = 1.0  # moderate label heterogeneity of alpha=1.0\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import uuid\n",
    "\n",
    "from importlib import reload\n",
    "import split_data\n",
    "reload(split_data)\n",
    "from split_data import split, list_to_dataframe\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Extract meta data and split\n",
    "data = []\n",
    "for i, x in enumerate(proteins):\n",
    "        if len(str(x.seq)) > MAX_SEQUENCE_LEN:\n",
    "            continue\n",
    "            \n",
    "        entry = {key: value for key, value in re.findall(r\"([A-Z_]+)=(-?[A-z0-9]+[.0-9]*)\", x.description)}\n",
    "        entry[\"sequence\"] = str(x.seq)\n",
    "        entry[\"id\"] = str(i)\n",
    "       \n",
    "        data.append(entry)\n",
    "print(f\"Read {len(data)} valid sequences.\")\n",
    "               \n",
    "# Split the data and save for each client\n",
    "# Note, test_data is kept the same on each client and is not split\n",
    "split(proteins=data, num_sites=n_clients, split_dir=out_dir, alpha=split_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d51256",
   "metadata": {},
   "source": [
    "### Federated embedding extraction\n",
    "Running inference of the ESM-1nv model to extract embeddings requires a GPU with at least 12 GB memory. Here we run inference on each client sequentially using one thread to preserve GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0320100f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nvflare import SimulatorRunner    \n",
    "\n",
    "simulator = SimulatorRunner(\n",
    "    job_folder=\"jobs/embeddings\",\n",
    "    workspace=\"/tmp/nvflare/bionemo/embeddings\",\n",
    "    n_clients=n_clients,\n",
    "    threads=1  # due to memory constraints, we run the client execution sequentially in one thread\n",
    ")\n",
    "run_status = simulator.run()\n",
    "print(\"Simulator finished with run_status\", run_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeab6389",
   "metadata": {},
   "source": [
    "### Inspecting the embeddings and labels\n",
    "Embeddings returned from the BioNeMo server are vectors of fixed size for each input sequence. In other words, if we input 10 sequences, we will obtain a matrix `10xD`, where `D` is the size of the embedding (in the case of ESM-1nv, `D=768`). At a glance, these real-valued vector embeddings don't show any obvious features (see the printout in the next cell). But these vectors do contain information that can be used in downstream models to reveal properties of the protein, for example the subcellular location as we'll explore below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c8bfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embeddings from site-1\n",
    "import pickle\n",
    "protein_embeddings = pickle.load(open(os.path.join(out_dir, \"data_site-1.pkl\"), \"rb\"))\n",
    "print(f\"Loaded {len(protein_embeddings)} embeddings from site-1.\")\n",
    "\n",
    "for i in range(4):\n",
    "    protein_embedding = protein_embeddings[i]\n",
    "    print(f\"Inference result contains {list(protein_embedding.keys())}\")\n",
    "    x = protein_embedding[\"embeddings\"]\n",
    "    print(f\"{protein_embedding['id']}: range {np.min(x)}-{np.max(x)}, mean={np.mean(x)}, shape={x.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d167d1b6",
   "metadata": {},
   "source": [
    "Let's enumerate the labels corresponding to potential subcellular locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f2c7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also print all the labels\n",
    "\n",
    "labels = set([entry['TARGET'] for entry in data])\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    print(f\"{i+1}. {label.replace('_', ' ')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ef1575",
   "metadata": {},
   "source": [
    "### Training a MLP to predict subcellular location\n",
    "To be able to classify proteins for their subcellular location, we train a simple scikit-learn Multi-layer Perceptron (MPL) classifier. The MLP model uses a network of hidden layers to fit the input embedding vectors to the model classes (the cellular locations above). In the call below, we define the MLP to use the Adam optimizer with a network of 32 hidden layers, defining a random state (or seed) for reproducibility, and trained for a maximum of 500 iterations.\n",
    "\n",
    "### Local training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaefe1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"SIM_LOCAL\"] = \"True\"\n",
    "\n",
    "simulator = SimulatorRunner(\n",
    "    job_folder=\"jobs/fedavg\",\n",
    "    workspace=f\"/tmp/nvflare/bionemo/local_alpha{split_alpha}\",\n",
    "    n_clients=n_clients,\n",
    "    threads=n_clients\n",
    ")\n",
    "run_status = simulator.run()\n",
    "print(\"Simulator finished with run_status\", run_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89ad0a7",
   "metadata": {},
   "source": [
    "### Federated learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92fda31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"SIM_LOCAL\"] = \"False\"\n",
    "\n",
    "simulator = SimulatorRunner(\n",
    "    job_folder=\"jobs/fedavg\",\n",
    "    workspace=f\"/tmp/nvflare/bionemo/fedavg_alpha{split_alpha}\",\n",
    "    n_clients=n_clients,\n",
    "    threads=n_clients\n",
    ")\n",
    "run_status = simulator.run()\n",
    "print(\"Simulator finished with run_status\", run_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed159561",
   "metadata": {},
   "source": [
    "## Finetuning ESM1nv\n",
    "#### Federated Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99c251a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-02 04:28:28,430 - SimulatorRunner - INFO - Create the Simulator Server.\n",
      "2023-12-02 04:28:28,433 - CoreCell - INFO - server: creating listener on tcp://0:42271\n",
      "2023-12-02 04:28:28,449 - CoreCell - INFO - server: created backbone external listener for tcp://0:42271\n",
      "2023-12-02 04:28:28,450 - ConnectorManager - INFO - 14412: Try start_listener Listener resources: {'secure': False, 'host': 'localhost'}\n",
      "2023-12-02 04:28:28,452 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connector [CH00002 PASSIVE tcp://0:31952] is starting\n",
      "2023-12-02 04:28:28,953 - CoreCell - INFO - server: created backbone internal listener for tcp://localhost:31952\n",
      "2023-12-02 04:28:28,956 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connector [CH00001 PASSIVE tcp://0:42271] is starting\n",
      "2023-12-02 04:28:29,039 - nvflare.fuel.hci.server.hci - INFO - Starting Admin Server localhost on Port 37861\n",
      "2023-12-02 04:28:29,040 - SimulatorRunner - INFO - Deploy the Apps.\n",
      "2023-12-02 04:28:29,045 - SimulatorRunner - INFO - Create the simulate clients.\n",
      "2023-12-02 04:28:29,049 - ClientManager - INFO - Client: New client site-1@192.168.0.34 joined. Sent token: 83818ec2-8a68-4e34-a7a5-11c5cde11e04.  Total clients: 1\n",
      "2023-12-02 04:28:29,050 - FederatedClient - INFO - Successfully registered client:site-1 for project simulator_server. Token:83818ec2-8a68-4e34-a7a5-11c5cde11e04 SSID:\n",
      "2023-12-02 04:28:29,051 - SimulatorRunner - INFO - Set the client status ready.\n",
      "2023-12-02 04:28:29,052 - SimulatorRunner - INFO - Deploy and start the Server App.\n",
      "2023-12-02 04:28:29,056 - Cell - INFO - Register blob CB for channel='server_command', topic='*'\n",
      "2023-12-02 04:28:29,057 - Cell - INFO - Register blob CB for channel='aux_communication', topic='*'\n",
      "2023-12-02 04:28:29,059 - ServerCommandAgent - INFO - ServerCommandAgent cell register_request_cb: server.simulate_job\n",
      "NOTE! Installing ujson may make loading annotations faster.\n",
      "2023-12-02 04:28:31,457 - torch.distributed.nn.jit.instantiator - INFO - Created a temporary directory at /tmp/tmpg5h9i6zl\n",
      "2023-12-02 04:28:31,458 - torch.distributed.nn.jit.instantiator - INFO - Writing /tmp/tmpg5h9i6zl/_remote_module_non_scriptable.py\n",
      "2023-12-02 04:28:32,436 - IntimeModelSelector - INFO - model selection weights control: None\n",
      "2023-12-02 04:28:32,438 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job]: Server runner starting ...\n",
      "2023-12-02 04:28:32,440 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job]: starting workflow scatter_and_gather (<class 'nvflare.app_common.workflows.scatter_and_gather.ScatterAndGather'>) ...\n",
      "2023-12-02 04:28:32,441 - ScatterAndGather - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: Initializing ScatterAndGather workflow.\n",
      "2023-12-02 04:28:32,442 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: Workflow scatter_and_gather (<class 'nvflare.app_common.workflows.scatter_and_gather.ScatterAndGather'>) started\n",
      "2023-12-02 04:28:32,444 - ScatterAndGather - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: Beginning ScatterAndGather training phase.\n",
      "2023-12-02 04:28:32,445 - ScatterAndGather - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: Round 0 started.\n",
      "2023-12-02 04:28:32,446 - ScatterAndGather - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: scheduled task train\n",
      "2023-12-02 04:28:33,092 - SimulatorClientRunner - INFO - Start the clients run simulation.\n",
      "2023-12-02 04:28:34,097 - SimulatorClientRunner - INFO - Simulate Run client: site-1 on GPU group: None\n",
      "2023-12-02 04:28:35,216 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connection [CN00004 127.0.0.1:42271 <= 127.0.0.1:42886] is created: PID: 14412\n",
      "2023-12-02 04:28:35,150 - ClientTaskWorker - INFO - ClientTaskWorker started to run\n",
      "2023-12-02 04:28:35,214 - CoreCell - INFO - site-1.simulate_job: created backbone external connector to tcp://localhost:42271\n",
      "2023-12-02 04:28:35,214 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connector [CH00001 ACTIVE tcp://localhost:42271] is starting\n",
      "2023-12-02 04:28:35,215 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connection [CN00002 127.0.0.1:42886 => 127.0.0.1:42271] is created: PID: 14530\n",
      "NOTE! Installing ujson may make loading annotations faster.\n",
      "2023-12-02 04:28:37,862 - torch.distributed.nn.jit.instantiator - INFO - Created a temporary directory at /tmp/tmp7x3_0kv9\n",
      "2023-12-02 04:28:37,862 - torch.distributed.nn.jit.instantiator - INFO - Writing /tmp/tmp7x3_0kv9/_remote_module_non_scriptable.py\n",
      "2023-12-02 04:28:38,933 - Cell - INFO - Register blob CB for channel='aux_communication', topic='*'\n",
      "2023-12-02 04:28:39,463 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-1, peer_run=simulate_job, task_name=train, task_id=f8432354-8374-4118-a2aa-dddd8fde2cdf]: assigned task to client site-1: name=train, id=f8432354-8374-4118-a2aa-dddd8fde2cdf\n",
      "2023-12-02 04:28:39,465 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-1, peer_run=simulate_job, task_name=train, task_id=f8432354-8374-4118-a2aa-dddd8fde2cdf]: sent task assignment to client. client_name:site-1 task_id:f8432354-8374-4118-a2aa-dddd8fde2cdf\n",
      "2023-12-02 04:28:39,466 - GetTaskCommand - INFO - return task to client.  client_name: site-1  task_name: train   task_id: f8432354-8374-4118-a2aa-dddd8fde2cdf  sharable_header_task_id: f8432354-8374-4118-a2aa-dddd8fde2cdf\n",
      "2023-12-02 04:28:39,440 - Cell - INFO - broadcast: channel='aux_communication', topic='__sync_runner__', targets=['server.simulate_job'], timeout=2.0\n",
      "2023-12-02 04:28:39,452 - ClientRunner - INFO - [identity=site-1, run=simulate_job]: synced to Server Runner in 0.5128757953643799 seconds\n",
      "2023-12-02 04:28:39,457 - ClientRunner - INFO - [identity=site-1, run=simulate_job]: client runner started\n",
      "2023-12-02 04:28:39,457 - ClientTaskWorker - INFO - Initialize ClientRunner for client: site-1\n",
      "2023-12-02 04:28:39,470 - Communicator - INFO - Received from simulator_server server. getTask: train size: 644B (644 Bytes) time: 0.012859 seconds\n",
      "2023-12-02 04:28:39,471 - FederatedClient - INFO - pull_task completed. Task name:train Status:True \n",
      "2023-12-02 04:28:39,471 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job]: got task assignment: name=train, id=f8432354-8374-4118-a2aa-dddd8fde2cdf\n",
      "2023-12-02 04:28:39,471 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=f8432354-8374-4118-a2aa-dddd8fde2cdf]: invoking task executor PTClientAPILauncherExecutor\n",
      "2023-12-02 04:28:39,471 - PTClientAPILauncherExecutor - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=f8432354-8374-4118-a2aa-dddd8fde2cdf]: execute for task (train)\n",
      "2023-12-02 04:28:39,471 - PTClientAPILauncherExecutor - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=f8432354-8374-4118-a2aa-dddd8fde2cdf]: External execution for task (train) is launched.\n",
      "[NeMo W 2023-12-02 04:28:49 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'downstream_flip_scl': Defaults list is missing `_self_`. See https://hydra.cc/docs/upgrades/1.0_to_1.1/default_composition_order for more information\n",
      "      warnings.warn(msg, UserWarning)\n",
      "    \n",
      "NOTE! Installing ujson may make loading annotations faster.\n",
      "[NeMo W 2023-12-02 04:28:49 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "    See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "      ret = run_job(\n",
      "    \n",
      "[NeMo I 2023-12-02 04:28:49 downstream_flip:34] \n",
      "    \n",
      "    ************* Finetune config ****************\n",
      "[NeMo I 2023-12-02 04:28:49 downstream_flip:35] \n",
      "    name: esm1nv_flip\n",
      "    do_training: true\n",
      "    do_testing: true\n",
      "    restore_from_path: ${oc.env:BIONEMO_HOME}/models/protein/esm1nv/esm1nv.nemo\n",
      "    trainer:\n",
      "      devices: 2\n",
      "      num_nodes: 1\n",
      "      accelerator: gpu\n",
      "      precision: 16\n",
      "      logger: false\n",
      "      enable_checkpointing: false\n",
      "      replace_sampler_ddp: false\n",
      "      max_epochs: 20\n",
      "      max_steps: 1000000\n",
      "      log_every_n_steps: 10\n",
      "      val_check_interval: 20\n",
      "      limit_val_batches: 1000\n",
      "      limit_test_batches: 1000\n",
      "      accumulate_grad_batches: 1\n",
      "      gradient_clip_val: 1.0\n",
      "      benchmark: false\n",
      "    exp_manager:\n",
      "      name: ${name}\n",
      "      exp_dir: ${oc.env:BIONEMO_HOME}/results/nemo_experiments/${.name}/${.wandb_logger_kwargs.name}\n",
      "      explicit_log_dir: ${.exp_dir}\n",
      "      create_wandb_logger: false\n",
      "      create_tensorboard_logger: true\n",
      "      wandb_logger_kwargs:\n",
      "        project: ${name}_${model.data.task_name}_finetuning\n",
      "        name: ${name}_${model.data.task_name}_finetuning_encoder_frozen_${model.encoder_frozen}\n",
      "        group: ${name}\n",
      "        job_type: Localhost_nodes_${trainer.num_nodes}_gpus_${trainer.devices}\n",
      "        notes: 'date: ${now:%y%m%d-%H%M%S}'\n",
      "        tags:\n",
      "        - ${name}\n",
      "        offline: false\n",
      "      resume_if_exists: true\n",
      "      resume_ignore_no_checkpoint: true\n",
      "      create_checkpoint_callback: true\n",
      "      checkpoint_callback_params:\n",
      "        monitor: val_loss\n",
      "        save_top_k: 10\n",
      "        mode: min\n",
      "        always_save_nemo: false\n",
      "        filename: megatron_bert--{val_loss:.2f}-{step}-{consumed_samples}\n",
      "        model_parallel_size: ${multiply:${model.tensor_model_parallel_size}, ${model.pipeline_model_parallel_size}}\n",
      "    model:\n",
      "      micro_batch_size: 64\n",
      "      tensor_model_parallel_size: 1\n",
      "      pipeline_model_parallel_size: 1\n",
      "      seq_length: 512\n",
      "      max_position_embeddings: ${.seq_length}\n",
      "      encoder_seq_length: ${.seq_length}\n",
      "      num_layers: 6\n",
      "      hidden_size: 768\n",
      "      ffn_hidden_size: 3072\n",
      "      num_attention_heads: 12\n",
      "      init_method_std: 0.02\n",
      "      hidden_dropout: 0.1\n",
      "      kv_channels: null\n",
      "      apply_query_key_layer_scaling: true\n",
      "      layernorm_epsilon: 1.0e-05\n",
      "      make_vocab_size_divisible_by: 128\n",
      "      pre_process: true\n",
      "      post_process: false\n",
      "      bert_binary_head: false\n",
      "      resume_from_checkpoint: null\n",
      "      masked_softmax_fusion: true\n",
      "      tokenizer:\n",
      "        library: sentencepiece\n",
      "        type: null\n",
      "        model: ${oc.env:BIONEMO_HOME}/tokenizers/protein/esm1nv/vocab/protein_sequence_sentencepiece.model\n",
      "        vocab_file: ${oc.env:BIONEMO_HOME}/tokenizers/vocab/protein_sequence_sentencepiece.vocab\n",
      "        merge_file: null\n",
      "      native_amp_init_scale: 4294967296\n",
      "      native_amp_growth_interval: 1000\n",
      "      fp32_residual_connection: false\n",
      "      fp16_lm_cross_entropy: false\n",
      "      seed: 1234\n",
      "      use_cpu_initialization: false\n",
      "      onnx_safe: false\n",
      "      activations_checkpoint_method: null\n",
      "      activations_checkpoint_num_layers: 1\n",
      "      data:\n",
      "        ngc_registry_target: uniref50_2022_05\n",
      "        ngc_registry_version: v23.06\n",
      "        data_prefix: ''\n",
      "        num_workers: 8\n",
      "        dataloader_type: single\n",
      "        reset_position_ids: false\n",
      "        reset_attention_mask: false\n",
      "        eod_mask_loss: false\n",
      "        masked_lm_prob: 0.15\n",
      "        short_seq_prob: 0.1\n",
      "        skip_lines: 0\n",
      "        drop_last: false\n",
      "        pin_memory: false\n",
      "        index_mapping_dir: null\n",
      "        data_impl: csv_mmap\n",
      "        data_impl_kwargs:\n",
      "          csv_mmap:\n",
      "            header_lines: 1\n",
      "            newline_int: 10\n",
      "            workers: ${model.data.num_workers}\n",
      "            sort_dataset_paths: true\n",
      "            data_sep: ','\n",
      "            data_col: 3\n",
      "        use_upsampling: true\n",
      "        seed: ${model.seed}\n",
      "        max_seq_length: ${model.seq_length}\n",
      "        dynamic_padding: false\n",
      "        dataset_path: ${model.data.preprocessed_data_path}/${model.data.task_name}\n",
      "        dataset:\n",
      "          train: x000\n",
      "          test: x000\n",
      "          val: x000\n",
      "        micro_batch_size: ${model.micro_batch_size}\n",
      "        modify_percent: 0.1\n",
      "        perturb_percent: 0.5\n",
      "        task_name: scl\n",
      "        task_type: classification\n",
      "        preprocessed_data_path: ${oc.env:BIONEMO_HOME}/data/FLIP\n",
      "        sequence_column: sequence\n",
      "        target_column:\n",
      "        - scl_label\n",
      "        target_sizes:\n",
      "        - 10\n",
      "        num_classes: 10\n",
      "        shuffle: true\n",
      "        emb_batch_size: ${model.micro_batch_size}\n",
      "      optim:\n",
      "        name: fused_adam\n",
      "        lr: 0.0002\n",
      "        weight_decay: 0.01\n",
      "        betas:\n",
      "        - 0.9\n",
      "        - 0.98\n",
      "        sched:\n",
      "          name: CosineAnnealing\n",
      "          warmup_steps: 500\n",
      "          constant_steps: 50000\n",
      "          min_lr: 2.0e-05\n",
      "      dwnstr_task_validation:\n",
      "        enabled: true\n",
      "        dataset:\n",
      "          class: bionemo.model.core.dwnstr_task_callbacks.PerTokenPredictionCallback\n",
      "          task_type: token-level-classification\n",
      "          infer_target: bionemo.model.protein.esm1nv.infer.ESM1nvInference\n",
      "          max_seq_length: ${model.seq_length}\n",
      "          emb_batch_size: 128\n",
      "          batch_size: 128\n",
      "          num_epochs: 10\n",
      "          shuffle: true\n",
      "          num_workers: 8\n",
      "          task_name: secondary_structure\n",
      "          dataset_path: ${oc.env:BIONEMO_HOME}/data/FLIP/${model.dwnstr_task_validation.dataset.task_name}\n",
      "          dataset:\n",
      "            train: x000\n",
      "            test: x000\n",
      "          sequence_column: sequence\n",
      "          target_column:\n",
      "          - 3state\n",
      "          - resolved\n",
      "          target_sizes:\n",
      "          - 3\n",
      "          - 2\n",
      "          mask_column:\n",
      "          - resolved\n",
      "          - null\n",
      "          random_seed: 1234\n",
      "          optim:\n",
      "            name: adam\n",
      "            lr: 0.0001\n",
      "            betas:\n",
      "            - 0.9\n",
      "            - 0.999\n",
      "            eps: 1.0e-08\n",
      "            weight_decay: 0.01\n",
      "            sched:\n",
      "              name: WarmupAnnealing\n",
      "              min_lr: 1.0e-05\n",
      "              last_epoch: -1\n",
      "              warmup_ratio: 0.01\n",
      "              max_steps: 1000\n",
      "      encoder_frozen: true\n",
      "      global_batch_size: null\n",
      "      loss_func: CrossEntropyLoss\n",
      "      hidden_layer_size: 256\n",
      "      finetuning_optim:\n",
      "        name: adam\n",
      "        lr: 0.0005\n",
      "        betas:\n",
      "        - 0.9\n",
      "        - 0.999\n",
      "        eps: 1.0e-08\n",
      "        weight_decay: 0.01\n",
      "        sched:\n",
      "          name: WarmupAnnealing\n",
      "          min_lr: 1.0e-05\n",
      "          last_epoch: -1\n",
      "          warmup_steps: 10\n",
      "    target: bionemo.model.protein.esm1nv.ESM1nvModel\n",
      "    infer_target: bionemo.model.protein.esm1nv.infer.ESM1nvInference\n",
      "    \n",
      "[NeMo I 2023-12-02 04:28:49 downstream_flip:38] ************** Starting Training ***********\n",
      "[NeMo W 2023-12-02 04:28:49 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/plugins/precision/native_amp.py:131: LightningDeprecationWarning: The `NativeMixedPrecisionPlugin` class has been renamed in v1.9.0 and will be removed in v2.0.0. Please use `pytorch_lightning.plugins.MixedPrecisionPlugin` instead.\n",
      "      rank_zero_deprecation(\n",
      "    \n",
      "[NeMo I 2023-12-02 04:28:49 utils:178] Selected Callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]\n",
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "[NeMo E 2023-12-02 04:28:49 exp_manager:646] exp_manager received explicit_log_dir: /opt/nvidia/bionemo/results/nemo_experiments/esm1nv_flip/esm1nv_flip_scl_finetuning_encoder_frozen_True and at least one of exp_dir: /opt/nvidia/bionemo/results/nemo_experiments/esm1nv_flip/esm1nv_flip_scl_finetuning_encoder_frozen_True, or version: None. Please note that exp_dir, name, and version will be ignored.\n",
      "[NeMo W 2023-12-02 04:28:49 exp_manager:651] Exp_manager is logging to /opt/nvidia/bionemo/results/nemo_experiments/esm1nv_flip/esm1nv_flip_scl_finetuning_encoder_frozen_True, but it already exists.\n",
      "[NeMo W 2023-12-02 04:28:49 exp_manager:568] There was no checkpoint folder at checkpoint_dir :/opt/nvidia/bionemo/results/nemo_experiments/esm1nv_flip/esm1nv_flip_scl_finetuning_encoder_frozen_True/checkpoints. Training from scratch.\n",
      "[NeMo I 2023-12-02 04:28:49 exp_manager:374] Experiments will be logged at /opt/nvidia/bionemo/results/nemo_experiments/esm1nv_flip/esm1nv_flip_scl_finetuning_encoder_frozen_True\n",
      "[NeMo I 2023-12-02 04:28:49 exp_manager:797] TensorboardLogger has been set up\n",
      "[NeMo W 2023-12-02 04:28:49 exp_manager:893] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to 1000000. Please ensure that max_steps will run for at least 1 epochs to ensure that checkpointing will not error out.\n",
      "[NeMo I 2023-12-02 04:28:49 utils:198] Resuming training from checkpoint: None\n",
      "[NeMo I 2023-12-02 04:28:49 utils:241] \n",
      "    \n",
      "    ************** Trainer configuration ***********\n",
      "[NeMo I 2023-12-02 04:28:49 utils:242] \n",
      "    name: esm1nv_flip\n",
      "    do_training: true\n",
      "    do_testing: true\n",
      "    restore_from_path: ${oc.env:BIONEMO_HOME}/models/protein/esm1nv/esm1nv.nemo\n",
      "    trainer:\n",
      "      devices: 2\n",
      "      num_nodes: 1\n",
      "      accelerator: gpu\n",
      "      precision: 16\n",
      "      logger: false\n",
      "      enable_checkpointing: false\n",
      "      replace_sampler_ddp: false\n",
      "      max_epochs: 20\n",
      "      max_steps: 1000000\n",
      "      log_every_n_steps: 10\n",
      "      val_check_interval: 20\n",
      "      limit_val_batches: 1000\n",
      "      limit_test_batches: 1000\n",
      "      accumulate_grad_batches: 1\n",
      "      gradient_clip_val: 1.0\n",
      "      benchmark: false\n",
      "    exp_manager:\n",
      "      name: ${name}\n",
      "      exp_dir: ${oc.env:BIONEMO_HOME}/results/nemo_experiments/${.name}/${.wandb_logger_kwargs.name}\n",
      "      explicit_log_dir: ${.exp_dir}\n",
      "      create_wandb_logger: false\n",
      "      create_tensorboard_logger: true\n",
      "      wandb_logger_kwargs:\n",
      "        project: ${name}_${model.data.task_name}_finetuning\n",
      "        name: ${name}_${model.data.task_name}_finetuning_encoder_frozen_${model.encoder_frozen}\n",
      "        group: ${name}\n",
      "        job_type: Localhost_nodes_${trainer.num_nodes}_gpus_${trainer.devices}\n",
      "        notes: 'date: ${now:%y%m%d-%H%M%S}'\n",
      "        tags:\n",
      "        - ${name}\n",
      "        offline: false\n",
      "      resume_if_exists: true\n",
      "      resume_ignore_no_checkpoint: true\n",
      "      create_checkpoint_callback: true\n",
      "      checkpoint_callback_params:\n",
      "        monitor: val_loss\n",
      "        save_top_k: 10\n",
      "        mode: min\n",
      "        always_save_nemo: false\n",
      "        filename: megatron_bert--{val_loss:.2f}-{step}-{consumed_samples}\n",
      "        model_parallel_size: ${multiply:${model.tensor_model_parallel_size}, ${model.pipeline_model_parallel_size}}\n",
      "    model:\n",
      "      micro_batch_size: 64\n",
      "      tensor_model_parallel_size: 1\n",
      "      pipeline_model_parallel_size: 1\n",
      "      seq_length: 512\n",
      "      max_position_embeddings: ${.seq_length}\n",
      "      encoder_seq_length: ${.seq_length}\n",
      "      num_layers: 6\n",
      "      hidden_size: 768\n",
      "      ffn_hidden_size: 3072\n",
      "      num_attention_heads: 12\n",
      "      init_method_std: 0.02\n",
      "      hidden_dropout: 0.1\n",
      "      kv_channels: null\n",
      "      apply_query_key_layer_scaling: true\n",
      "      layernorm_epsilon: 1.0e-05\n",
      "      make_vocab_size_divisible_by: 128\n",
      "      pre_process: true\n",
      "      post_process: false\n",
      "      bert_binary_head: false\n",
      "      resume_from_checkpoint: null\n",
      "      masked_softmax_fusion: true\n",
      "      tokenizer:\n",
      "        library: sentencepiece\n",
      "        type: null\n",
      "        model: ${oc.env:BIONEMO_HOME}/tokenizers/protein/esm1nv/vocab/protein_sequence_sentencepiece.model\n",
      "        vocab_file: ${oc.env:BIONEMO_HOME}/tokenizers/vocab/protein_sequence_sentencepiece.vocab\n",
      "        merge_file: null\n",
      "      native_amp_init_scale: 4294967296\n",
      "      native_amp_growth_interval: 1000\n",
      "      fp32_residual_connection: false\n",
      "      fp16_lm_cross_entropy: false\n",
      "      seed: 1234\n",
      "      use_cpu_initialization: false\n",
      "      onnx_safe: false\n",
      "      activations_checkpoint_method: null\n",
      "      activations_checkpoint_num_layers: 1\n",
      "      data:\n",
      "        ngc_registry_target: uniref50_2022_05\n",
      "        ngc_registry_version: v23.06\n",
      "        data_prefix: ''\n",
      "        num_workers: 8\n",
      "        dataloader_type: single\n",
      "        reset_position_ids: false\n",
      "        reset_attention_mask: false\n",
      "        eod_mask_loss: false\n",
      "        masked_lm_prob: 0.15\n",
      "        short_seq_prob: 0.1\n",
      "        skip_lines: 0\n",
      "        drop_last: false\n",
      "        pin_memory: false\n",
      "        index_mapping_dir: null\n",
      "        data_impl: csv_mmap\n",
      "        data_impl_kwargs:\n",
      "          csv_mmap:\n",
      "            header_lines: 1\n",
      "            newline_int: 10\n",
      "            workers: ${model.data.num_workers}\n",
      "            sort_dataset_paths: true\n",
      "            data_sep: ','\n",
      "            data_col: 3\n",
      "        use_upsampling: true\n",
      "        seed: ${model.seed}\n",
      "        max_seq_length: ${model.seq_length}\n",
      "        dynamic_padding: false\n",
      "        dataset_path: ${model.data.preprocessed_data_path}/${model.data.task_name}\n",
      "        dataset:\n",
      "          train: x000\n",
      "          test: x000\n",
      "          val: x000\n",
      "        micro_batch_size: ${model.micro_batch_size}\n",
      "        modify_percent: 0.1\n",
      "        perturb_percent: 0.5\n",
      "        task_name: scl\n",
      "        task_type: classification\n",
      "        preprocessed_data_path: ${oc.env:BIONEMO_HOME}/data/FLIP\n",
      "        sequence_column: sequence\n",
      "        target_column:\n",
      "        - scl_label\n",
      "        target_sizes:\n",
      "        - 10\n",
      "        num_classes: 10\n",
      "        shuffle: true\n",
      "        emb_batch_size: ${model.micro_batch_size}\n",
      "      optim:\n",
      "        name: fused_adam\n",
      "        lr: 0.0002\n",
      "        weight_decay: 0.01\n",
      "        betas:\n",
      "        - 0.9\n",
      "        - 0.98\n",
      "        sched:\n",
      "          name: CosineAnnealing\n",
      "          warmup_steps: 500\n",
      "          constant_steps: 50000\n",
      "          min_lr: 2.0e-05\n",
      "      dwnstr_task_validation:\n",
      "        enabled: true\n",
      "        dataset:\n",
      "          class: bionemo.model.core.dwnstr_task_callbacks.PerTokenPredictionCallback\n",
      "          task_type: token-level-classification\n",
      "          infer_target: bionemo.model.protein.esm1nv.infer.ESM1nvInference\n",
      "          max_seq_length: ${model.seq_length}\n",
      "          emb_batch_size: 128\n",
      "          batch_size: 128\n",
      "          num_epochs: 10\n",
      "          shuffle: true\n",
      "          num_workers: 8\n",
      "          task_name: secondary_structure\n",
      "          dataset_path: ${oc.env:BIONEMO_HOME}/data/FLIP/${model.dwnstr_task_validation.dataset.task_name}\n",
      "          dataset:\n",
      "            train: x000\n",
      "            test: x000\n",
      "          sequence_column: sequence\n",
      "          target_column:\n",
      "          - 3state\n",
      "          - resolved\n",
      "          target_sizes:\n",
      "          - 3\n",
      "          - 2\n",
      "          mask_column:\n",
      "          - resolved\n",
      "          - null\n",
      "          random_seed: 1234\n",
      "          optim:\n",
      "            name: adam\n",
      "            lr: 0.0001\n",
      "            betas:\n",
      "            - 0.9\n",
      "            - 0.999\n",
      "            eps: 1.0e-08\n",
      "            weight_decay: 0.01\n",
      "            sched:\n",
      "              name: WarmupAnnealing\n",
      "              min_lr: 1.0e-05\n",
      "              last_epoch: -1\n",
      "              warmup_ratio: 0.01\n",
      "              max_steps: 1000\n",
      "      encoder_frozen: true\n",
      "      global_batch_size: 128\n",
      "      loss_func: CrossEntropyLoss\n",
      "      hidden_layer_size: 256\n",
      "      finetuning_optim:\n",
      "        name: adam\n",
      "        lr: 0.0005\n",
      "        betas:\n",
      "        - 0.9\n",
      "        - 0.999\n",
      "        eps: 1.0e-08\n",
      "        weight_decay: 0.01\n",
      "        sched:\n",
      "          name: WarmupAnnealing\n",
      "          min_lr: 1.0e-05\n",
      "          last_epoch: -1\n",
      "          warmup_steps: 10\n",
      "      precision: 16\n",
      "    target: bionemo.model.protein.esm1nv.ESM1nvModel\n",
      "    infer_target: bionemo.model.protein.esm1nv.infer.ESM1nvInference\n",
      "    \n",
      "[NeMo I 2023-12-02 04:28:49 utils:257] Restoring model from /opt/nvidia/bionemo/models/protein/esm1nv/esm1nv.nemo\n",
      "[NeMo I 2023-12-02 04:28:49 utils:261] Loading model class: bionemo.model.protein.esm1nv.ESM1nvModel\n",
      "[NeMo W 2023-12-02 04:28:49 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/plugins/precision/native_amp.py:131: LightningDeprecationWarning: The `NativeMixedPrecisionPlugin` class has been renamed in v1.9.0 and will be removed in v2.0.0. Please use `pytorch_lightning.plugins.MixedPrecisionPlugin` instead.\n",
      "      rank_zero_deprecation(\n",
      "    \n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "[NeMo E 2023-12-02 04:28:49 exp_manager:646] exp_manager received explicit_log_dir: /opt/nvidia/bionemo/results/nemo_experiments/esm1nv_flip/esm1nv_flip_scl_finetuning_encoder_frozen_True and at least one of exp_dir: /opt/nvidia/bionemo/results/nemo_experiments/esm1nv_flip/esm1nv_flip_scl_finetuning_encoder_frozen_True, or version: None. Please note that exp_dir, name, and version will be ignored.\n",
      "[NeMo W 2023-12-02 04:28:49 exp_manager:651] Exp_manager is logging to /opt/nvidia/bionemo/results/nemo_experiments/esm1nv_flip/esm1nv_flip_scl_finetuning_encoder_frozen_True, but it already exists.\n",
      "[NeMo W 2023-12-02 04:28:49 exp_manager:568] There was no checkpoint folder at checkpoint_dir :/opt/nvidia/bionemo/results/nemo_experiments/esm1nv_flip/esm1nv_flip_scl_finetuning_encoder_frozen_True/checkpoints. Training from scratch.\n",
      "[NeMo I 2023-12-02 04:28:49 exp_manager:374] Experiments will be logged at /opt/nvidia/bionemo/results/nemo_experiments/esm1nv_flip/esm1nv_flip_scl_finetuning_encoder_frozen_True\n",
      "[NeMo I 2023-12-02 04:28:49 exp_manager:797] TensorboardLogger has been set up\n",
      "[NeMo W 2023-12-02 04:28:49 exp_manager:893] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to 1000000. Please ensure that max_steps will run for at least 1 epochs to ensure that checkpointing will not error out.\n",
      "[NeMo I 2023-12-02 04:28:49 utils:241] \n",
      "    \n",
      "    ************** Trainer configuration ***********\n",
      "[NeMo I 2023-12-02 04:28:49 utils:242] \n",
      "    name: esm1nv_flip\n",
      "    do_training: true\n",
      "    do_testing: true\n",
      "    restore_from_path: ${oc.env:BIONEMO_HOME}/models/protein/esm1nv/esm1nv.nemo\n",
      "    trainer:\n",
      "      devices: 2\n",
      "      num_nodes: 1\n",
      "      accelerator: gpu\n",
      "      precision: 16\n",
      "      logger: false\n",
      "      enable_checkpointing: false\n",
      "      replace_sampler_ddp: false\n",
      "      max_epochs: 20\n",
      "      max_steps: 1000000\n",
      "      log_every_n_steps: 10\n",
      "      val_check_interval: 20\n",
      "      limit_val_batches: 1000\n",
      "      limit_test_batches: 1000\n",
      "      accumulate_grad_batches: 1\n",
      "      gradient_clip_val: 1.0\n",
      "      benchmark: false\n",
      "    exp_manager:\n",
      "      name: ${name}\n",
      "      exp_dir: ${oc.env:BIONEMO_HOME}/results/nemo_experiments/${.name}/${.wandb_logger_kwargs.name}\n",
      "      explicit_log_dir: ${.exp_dir}\n",
      "      create_wandb_logger: false\n",
      "      create_tensorboard_logger: true\n",
      "      wandb_logger_kwargs:\n",
      "        project: ${name}_${model.data.task_name}_finetuning\n",
      "        name: ${name}_${model.data.task_name}_finetuning_encoder_frozen_${model.encoder_frozen}\n",
      "        group: ${name}\n",
      "        job_type: Localhost_nodes_${trainer.num_nodes}_gpus_${trainer.devices}\n",
      "        notes: 'date: ${now:%y%m%d-%H%M%S}'\n",
      "        tags:\n",
      "        - ${name}\n",
      "        offline: false\n",
      "      resume_if_exists: true\n",
      "      resume_ignore_no_checkpoint: true\n",
      "      create_checkpoint_callback: true\n",
      "      checkpoint_callback_params:\n",
      "        monitor: val_loss\n",
      "        save_top_k: 10\n",
      "        mode: min\n",
      "        always_save_nemo: false\n",
      "        filename: megatron_bert--{val_loss:.2f}-{step}-{consumed_samples}\n",
      "        model_parallel_size: ${multiply:${model.tensor_model_parallel_size}, ${model.pipeline_model_parallel_size}}\n",
      "    model:\n",
      "      micro_batch_size: 64\n",
      "      tensor_model_parallel_size: 1\n",
      "      pipeline_model_parallel_size: 1\n",
      "      seq_length: 512\n",
      "      max_position_embeddings: ${.seq_length}\n",
      "      encoder_seq_length: ${.seq_length}\n",
      "      num_layers: 6\n",
      "      hidden_size: 768\n",
      "      ffn_hidden_size: 3072\n",
      "      num_attention_heads: 12\n",
      "      init_method_std: 0.02\n",
      "      hidden_dropout: 0.1\n",
      "      kv_channels: null\n",
      "      apply_query_key_layer_scaling: true\n",
      "      layernorm_epsilon: 1.0e-05\n",
      "      make_vocab_size_divisible_by: 128\n",
      "      pre_process: true\n",
      "      post_process: false\n",
      "      bert_binary_head: false\n",
      "      resume_from_checkpoint: null\n",
      "      masked_softmax_fusion: true\n",
      "      tokenizer:\n",
      "        library: sentencepiece\n",
      "        type: null\n",
      "        model: ${oc.env:BIONEMO_HOME}/tokenizers/protein/esm1nv/vocab/protein_sequence_sentencepiece.model\n",
      "        vocab_file: ${oc.env:BIONEMO_HOME}/tokenizers/vocab/protein_sequence_sentencepiece.vocab\n",
      "        merge_file: null\n",
      "      native_amp_init_scale: 4294967296\n",
      "      native_amp_growth_interval: 1000\n",
      "      fp32_residual_connection: false\n",
      "      fp16_lm_cross_entropy: false\n",
      "      seed: 1234\n",
      "      use_cpu_initialization: false\n",
      "      onnx_safe: false\n",
      "      activations_checkpoint_method: null\n",
      "      activations_checkpoint_num_layers: 1\n",
      "      data:\n",
      "        ngc_registry_target: uniref50_2022_05\n",
      "        ngc_registry_version: v23.06\n",
      "        data_prefix: ''\n",
      "        num_workers: 8\n",
      "        dataloader_type: single\n",
      "        reset_position_ids: false\n",
      "        reset_attention_mask: false\n",
      "        eod_mask_loss: false\n",
      "        masked_lm_prob: 0.15\n",
      "        short_seq_prob: 0.1\n",
      "        skip_lines: 0\n",
      "        drop_last: false\n",
      "        pin_memory: false\n",
      "        data_impl: csv_mmap\n",
      "        data_impl_kwargs:\n",
      "          csv_mmap:\n",
      "            header_lines: 1\n",
      "            newline_int: 10\n",
      "            workers: ${model.data.num_workers}\n",
      "            sort_dataset_paths: true\n",
      "            data_sep: ','\n",
      "            data_col: 3\n",
      "        use_upsampling: true\n",
      "        seed: ${model.seed}\n",
      "        max_seq_length: ${model.seq_length}\n",
      "        dataset_path: ${model.data.preprocessed_data_path}/${model.data.task_name}\n",
      "        dataset:\n",
      "          train: x000\n",
      "          test: x000\n",
      "          val: x000\n",
      "        micro_batch_size: ${model.micro_batch_size}\n",
      "        modify_percent: 0.1\n",
      "        perturb_percent: 0.5\n",
      "        index_mapping_dir: null\n",
      "        dynamic_padding: false\n",
      "        task_name: scl\n",
      "        task_type: classification\n",
      "        preprocessed_data_path: ${oc.env:BIONEMO_HOME}/data/FLIP\n",
      "        sequence_column: sequence\n",
      "        target_column:\n",
      "        - scl_label\n",
      "        target_sizes:\n",
      "        - 10\n",
      "        num_classes: 10\n",
      "        shuffle: true\n",
      "        emb_batch_size: ${model.micro_batch_size}\n",
      "      optim:\n",
      "        name: fused_adam\n",
      "        lr: 0.0002\n",
      "        weight_decay: 0.01\n",
      "        betas:\n",
      "        - 0.9\n",
      "        - 0.98\n",
      "        sched:\n",
      "          name: CosineAnnealing\n",
      "          warmup_steps: 500\n",
      "          constant_steps: 50000\n",
      "          min_lr: 2.0e-05\n",
      "      dwnstr_task_validation:\n",
      "        enabled: true\n",
      "        dataset:\n",
      "          class: bionemo.model.core.dwnstr_task_callbacks.PerTokenPredictionCallback\n",
      "          task_type: token-level-classification\n",
      "          infer_target: bionemo.model.protein.esm1nv.infer.ESM1nvInference\n",
      "          max_seq_length: ${model.seq_length}\n",
      "          emb_batch_size: 128\n",
      "          batch_size: 128\n",
      "          num_epochs: 10\n",
      "          task_name: secondary_structure\n",
      "          dataset_path: ${oc.env:BIONEMO_HOME}/data/FLIP/${model.dwnstr_task_validation.dataset.task_name}\n",
      "          dataset:\n",
      "            train: x000\n",
      "            test: x000\n",
      "          sequence_col: sequence\n",
      "          labels_col:\n",
      "          - 3state\n",
      "          - resolved\n",
      "          labels_size:\n",
      "          - 3\n",
      "          - 2\n",
      "          mask_col:\n",
      "          - resolved\n",
      "          - null\n",
      "          random_seed: 1234\n",
      "          optim:\n",
      "            name: adam\n",
      "            lr: 0.0001\n",
      "            betas:\n",
      "            - 0.9\n",
      "            - 0.999\n",
      "            eps: 1.0e-08\n",
      "            weight_decay: 0.01\n",
      "            sched:\n",
      "              name: WarmupAnnealing\n",
      "              min_lr: 1.0e-05\n",
      "              last_epoch: -1\n",
      "              warmup_ratio: 0.01\n",
      "              max_steps: 1000\n",
      "          shuffle: true\n",
      "          num_workers: 8\n",
      "          sequence_column: sequence\n",
      "          target_column:\n",
      "          - 3state\n",
      "          - resolved\n",
      "          target_sizes:\n",
      "          - 3\n",
      "          - 2\n",
      "          mask_column:\n",
      "          - resolved\n",
      "          - null\n",
      "      global_batch_size: 128\n",
      "      precision: 16\n",
      "      target: bionemo.model.protein.esm1nv.esm1nv_model.ESM1nvModel\n",
      "      nemo_version: 1.18.1\n",
      "      encoder_frozen: true\n",
      "      loss_func: CrossEntropyLoss\n",
      "      hidden_layer_size: 256\n",
      "      finetuning_optim:\n",
      "        name: adam\n",
      "        lr: 0.0005\n",
      "        betas:\n",
      "        - 0.9\n",
      "        - 0.999\n",
      "        eps: 1.0e-08\n",
      "        weight_decay: 0.01\n",
      "        sched:\n",
      "          name: WarmupAnnealing\n",
      "          min_lr: 1.0e-05\n",
      "          last_epoch: -1\n",
      "          warmup_steps: 10\n",
      "    target: bionemo.model.protein.esm1nv.ESM1nvModel\n",
      "    infer_target: bionemo.model.protein.esm1nv.infer.ESM1nvInference\n",
      "    \n",
      "[NeMo I 2023-12-02 04:28:50 megatron_init:234] Rank 0 has data parallel group: [0, 1]\n",
      "[NeMo I 2023-12-02 04:28:50 megatron_init:237] All data parallel group ranks: [[0, 1]]\n",
      "[NeMo I 2023-12-02 04:28:50 megatron_init:238] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2023-12-02 04:28:50 megatron_init:246] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2023-12-02 04:28:50 megatron_init:247] All model parallel group ranks: [[0], [1]]\n",
      "[NeMo I 2023-12-02 04:28:50 megatron_init:257] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2023-12-02 04:28:50 megatron_init:261] All tensor model parallel group ranks: [[0], [1]]\n",
      "[NeMo I 2023-12-02 04:28:50 megatron_init:262] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2023-12-02 04:28:50 megatron_init:276] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2023-12-02 04:28:50 megatron_init:288] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2023-12-02 04:28:50 megatron_init:294] All pipeline model parallel group ranks: [[0], [1]]\n",
      "[NeMo I 2023-12-02 04:28:50 megatron_init:295] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2023-12-02 04:28:50 megatron_init:296] All embedding group ranks: [[0], [1]]\n",
      "[NeMo I 2023-12-02 04:28:50 megatron_init:297] Rank 0 has embedding rank: 0\n",
      "23-12-02 04:28:50 - PID:14623 - rank:(0, 0, 0, 0) - microbatches.py:39 - INFO - setting number of micro-batches to constant 1\n",
      "[NeMo I 2023-12-02 04:28:50 megatron_init:234] Rank 0 has data parallel group: [0, 1]\n",
      "[NeMo I 2023-12-02 04:28:50 megatron_init:237] All data parallel group ranks: [[0, 1]]\n",
      "[NeMo I 2023-12-02 04:28:50 megatron_init:238] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2023-12-02 04:28:50 megatron_init:246] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2023-12-02 04:28:50 megatron_init:247] All model parallel group ranks: [[0], [1]]\n",
      "[NeMo I 2023-12-02 04:28:50 megatron_init:257] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2023-12-02 04:28:50 megatron_init:261] All tensor model parallel group ranks: [[0], [1]]\n",
      "[NeMo I 2023-12-02 04:28:50 megatron_init:262] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2023-12-02 04:28:50 megatron_init:276] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2023-12-02 04:28:50 megatron_init:288] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2023-12-02 04:28:50 megatron_init:294] All pipeline model parallel group ranks: [[0], [1]]\n",
      "[NeMo I 2023-12-02 04:28:50 megatron_init:295] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2023-12-02 04:28:50 megatron_init:296] All embedding group ranks: [[0], [1]]\n",
      "[NeMo I 2023-12-02 04:28:50 megatron_init:297] Rank 0 has embedding rank: 0\n",
      "[NeMo E 2023-12-02 04:28:50 common:506] Model instantiation failed!\n",
      "    Target class:\tbionemo.model.protein.esm1nv.esm1nv_model.ESM1nvModel\n",
      "    Error(s):\tNo Tokenizer path provided or file does not exist!\n",
      "    Traceback (most recent call last):\n",
      "      File \"/usr/local/lib/python3.10/dist-packages/nemo/core/classes/common.py\", line 485, in from_config_dict\n",
      "        instance = imported_cls(cfg=config, trainer=trainer)\n",
      "      File \"/opt/nvidia/bionemo/bionemo/model/protein/esm1nv/esm1nv_model.py\", line 54, in __init__\n",
      "        super().__init__(cfg, trainer=trainer)\n",
      "      File \"/usr/local/lib/python3.10/dist-packages/nemo/collections/nlp/models/language_modeling/megatron_bert_model.py\", line 89, in __init__\n",
      "        super().__init__(cfg, trainer=trainer, no_lm_init=False)\n",
      "      File \"/usr/local/lib/python3.10/dist-packages/nemo/collections/nlp/models/language_modeling/megatron_base_model.py\", line 149, in __init__\n",
      "        self._build_tokenizer()\n",
      "      File \"/opt/nvidia/bionemo/bionemo/model/protein/esm1nv/esm1nv_model.py\", line 63, in _build_tokenizer\n",
      "        self.tokenizer = get_nmt_tokenizer(\n",
      "      File \"/usr/local/lib/python3.10/dist-packages/nemo/collections/nlp/modules/common/tokenizer_utils.py\", line 176, in get_nmt_tokenizer\n",
      "        raise ValueError(\"No Tokenizer path provided or file does not exist!\")\n",
      "    ValueError: No Tokenizer path provided or file does not exist!\n",
      "    \n",
      "Error executing job with overrides: []\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/nvflare/bionemo/fedavg_finetune_esm1nv_alpha1.0/simulate_job/app_site-1/custom/downstream_flip.py\", line 40, in main\n",
      "    model = FineTuneProteinModel(cfg, trainer)\n",
      "  File \"/opt/nvidia/bionemo/bionemo/model/protein/downstream/protein_model_finetuning.py\", line 37, in __init__\n",
      "    super().__init__(cfg.model, trainer=trainer)\n",
      "  File \"/opt/nvidia/bionemo/bionemo/model/core/encoder_finetuning.py\", line 58, in __init__\n",
      "    enc_model = self.setup_encoder_model(cfg, trainer)\n",
      "  File \"/opt/nvidia/bionemo/bionemo/model/protein/downstream/protein_model_finetuning.py\", line 73, in setup_encoder_model\n",
      "    pretrained_model = infer_class(\n",
      "  File \"/opt/nvidia/bionemo/bionemo/model/protein/esm1nv/infer.py\", line 30, in __init__\n",
      "    super().__init__(\n",
      "  File \"/opt/nvidia/bionemo/bionemo/model/core/infer.py\", line 51, in __init__\n",
      "    self.model = self.load_model(cfg, model=model, restore_path=restore_path)\n",
      "  File \"/opt/nvidia/bionemo/bionemo/model/protein/esm1nv/infer.py\", line 95, in load_model\n",
      "    model = super().load_model(cfg, model=model, restore_path=restore_path)\n",
      "  File \"/opt/nvidia/bionemo/bionemo/model/core/infer.py\", line 68, in load_model\n",
      "    model = restore_model(restore_path=restore_path, cfg=cfg, adjust_config=self.adjust_config)\n",
      "  File \"/opt/nvidia/bionemo/bionemo/model/utils.py\", line 282, in restore_model\n",
      "    model = model_cls.restore_from(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/nemo/core/classes/modelPT.py\", line 435, in restore_from\n",
      "    instance = cls._save_restore_connector.restore_from(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/nemo/collections/nlp/parts/nlp_overrides.py\", line 393, in restore_from\n",
      "    loaded_params = super().load_config_and_state_dict(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/nemo/core/connectors/save_restore_connector.py\", line 164, in load_config_and_state_dict\n",
      "    instance = calling_cls.from_config_dict(config=conf, trainer=trainer)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/nemo/core/classes/common.py\", line 507, in from_config_dict\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/nemo/core/classes/common.py\", line 499, in from_config_dict\n",
      "    instance = cls(cfg=config, trainer=trainer)\n",
      "  File \"/opt/nvidia/bionemo/bionemo/model/protein/esm1nv/esm1nv_model.py\", line 54, in __init__\n",
      "    super().__init__(cfg, trainer=trainer)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/nemo/collections/nlp/models/language_modeling/megatron_bert_model.py\", line 89, in __init__\n",
      "    super().__init__(cfg, trainer=trainer, no_lm_init=False)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/nemo/collections/nlp/models/language_modeling/megatron_base_model.py\", line 149, in __init__\n",
      "    self._build_tokenizer()\n",
      "  File \"/opt/nvidia/bionemo/bionemo/model/protein/esm1nv/esm1nv_model.py\", line 63, in _build_tokenizer\n",
      "    self.tokenizer = get_nmt_tokenizer(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/nemo/collections/nlp/modules/common/tokenizer_utils.py\", line 176, in get_nmt_tokenizer\n",
      "    raise ValueError(\"No Tokenizer path provided or file does not exist!\")\n",
      "ValueError: No Tokenizer path provided or file does not exist!\n",
      "\n",
      "Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\n",
      "2023-12-02 04:28:51,616 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-1, peer_run=simulate_job]: got result from client site-1 for task: name=train, id=f8432354-8374-4118-a2aa-dddd8fde2cdf\n",
      "2023-12-02 04:28:51,619 - ServerRunner - ERROR - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-1, peer_run=simulate_job, peer_rc=TASK_ABORTED, task_name=train, task_id=f8432354-8374-4118-a2aa-dddd8fde2cdf]: Aborting current RUN due to FATAL_SYSTEM_ERROR received: Result from site-1 is bad, error code: TASK_ABORTED. ScatterAndGather exiting at round 0.\n",
      "2023-12-02 04:28:51,621 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-1, peer_run=simulate_job, peer_rc=TASK_ABORTED, task_name=train, task_id=f8432354-8374-4118-a2aa-dddd8fde2cdf]: asked to abort - triggered abort_signal to stop the RUN\n",
      "2023-12-02 04:28:51,622 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-1, peer_run=simulate_job, peer_rc=TASK_ABORTED, task_name=train, task_id=f8432354-8374-4118-a2aa-dddd8fde2cdf]: finished processing client result by scatter_and_gather\n",
      "2023-12-02 04:28:51,623 - SubmitUpdateCommand - INFO - submit_update process. client_name:site-1   task_id:f8432354-8374-4118-a2aa-dddd8fde2cdf\n",
      "2023-12-02 04:28:51,541 - PTClientAPILauncherExecutor - INFO - [identity=site-1, run=simulate_job]: launcher completed train with status failed at time 1701491331.54184\n",
      "2023-12-02 04:28:51,605 - PTClientAPILauncherExecutor - ERROR - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=f8432354-8374-4118-a2aa-dddd8fde2cdf]: External execution set up failed.\n",
      "2023-12-02 04:28:51,606 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=f8432354-8374-4118-a2aa-dddd8fde2cdf]: try #1: sending task result to server\n",
      "2023-12-02 04:28:51,606 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=f8432354-8374-4118-a2aa-dddd8fde2cdf]: checking task ...\n",
      "2023-12-02 04:28:51,606 - Cell - INFO - broadcast: channel='aux_communication', topic='__task_check__', targets=['server.simulate_job'], timeout=5.0\n",
      "2023-12-02 04:28:51,614 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=f8432354-8374-4118-a2aa-dddd8fde2cdf]: start to send task result to server\n",
      "2023-12-02 04:28:51,614 - FederatedClient - INFO - Starting to push execute result.\n",
      "2023-12-02 04:28:51,625 - Communicator - INFO -  SubmitUpdate size: 560B (560 Bytes). time: 0.011341 seconds\n",
      "2023-12-02 04:28:51,625 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=f8432354-8374-4118-a2aa-dddd8fde2cdf]: task result sent to server\n",
      "2023-12-02 04:28:51,625 - ClientTaskWorker - INFO - Finished one task run for client: site-1 interval: 2 task_processed: True\n",
      "2023-12-02 04:28:51,976 - ScatterAndGather - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: Abort signal received. Exiting at round 0.\n",
      "2023-12-02 04:28:51,978 - ScatterAndGather - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: task train exit with status TaskCompletionStatus.ABORTED\n",
      "2023-12-02 04:28:51,980 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: Workflow: scatter_and_gather finalizing ...\n",
      "2023-12-02 04:28:52,483 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: ABOUT_TO_END_RUN fired\n",
      "2023-12-02 04:28:52,486 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: Firing CHECK_END_RUN_READINESS ...\n",
      "2023-12-02 04:28:52,489 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: END_RUN fired\n",
      "2023-12-02 04:28:52,491 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: Server runner finished.\n",
      "2023-12-02 04:28:53,456 - SimulatorServer - INFO - Server app stopped.\n",
      "\n",
      "\n",
      "2023-12-02 04:28:53,626 - nvflare.fuel.hci.server.hci - INFO - Admin Server localhost on Port 37861 shutdown!\n",
      "2023-12-02 04:28:53,629 - SimulatorServer - INFO - shutting down server\n",
      "2023-12-02 04:28:53,632 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-1, peer_run=simulate_job]: server runner is finalizing - asked client to end the run\n",
      "2023-12-02 04:28:53,634 - SimulatorServer - INFO - canceling sync locks\n",
      "2023-12-02 04:28:53,636 - GetTaskCommand - INFO - return task to client.  client_name: site-1  task_name: __end_run__   task_id:   sharable_header_task_id: \n",
      "2023-12-02 04:28:53,638 - SimulatorServer - INFO - server off\n",
      "2023-12-02 04:28:53,644 - FederatedClient - INFO - Shutting down client run: site-1\n",
      "2023-12-02 04:28:53,646 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: asked to abort - triggered abort_signal to stop the RUN\n",
      "2023-12-02 04:28:53,648 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connection [CN00004 Not Connected] is closed PID: 14412\n",
      "2023-12-02 04:28:53,643 - FederatedClient - INFO - pull_task completed. Task name:__end_run__ Status:True \n",
      "2023-12-02 04:28:53,643 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job]: server asked to end the run\n",
      "2023-12-02 04:28:53,643 - ClientTaskWorker - INFO - End the Simulator run.\n",
      "2023-12-02 04:28:53,644 - ClientTaskWorker - INFO - Clean up ClientRunner for : site-1 \n",
      "2023-12-02 04:28:53,647 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connection [CN00002 Not Connected] is closed PID: 14530\n",
      "2023-12-02 04:28:57,117 - MPM - INFO - MPM: Good Bye!\n",
      "Simulator finished with run_status 0\n"
     ]
    }
   ],
   "source": [
    "# DEBUG\n",
    "import os\n",
    "os.environ[\"SIM_LOCAL\"] = \"False\"\n",
    "from nvflare import SimulatorRunner    \n",
    "n_clients = 1\n",
    "split_alpha = 1.0\n",
    "\n",
    "simulator = SimulatorRunner(\n",
    "    job_folder=\"jobs/fedavg_finetune_esm1nv\",\n",
    "    workspace=f\"/tmp/nvflare/bionemo/fedavg_finetune_esm1nv_alpha{split_alpha}\",\n",
    "    n_clients=n_clients,\n",
    "    threads=n_clients\n",
    ")\n",
    "run_status = simulator.run()\n",
    "print(\"Simulator finished with run_status\", run_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163b39f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
