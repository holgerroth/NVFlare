{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34ac3a00",
   "metadata": {},
   "source": [
    "# Federated Protein Embeddings and Task Model Fitting with BioNeMo\n",
    "\n",
    "This example notebook shows how to obtain protein learned representations in the form of embeddings using the ESM-1nv pre-trained model. The model is trained with NVIDIA's BioNeMo framework for Large Language Model training and inference. For more details, please visit NVIDIA BioNeMo Service at https://www.nvidia.com/en-us/gpu-cloud/bionemo.\n",
    "\n",
    "This notebook will walk you through the task fitting workflow in the following sections:\n",
    "\n",
    "* \n",
    "*\n",
    "*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42013524",
   "metadata": {},
   "source": [
    "### Install requirements\n",
    "Let's start by installing and importing library dependencies. We'll use requests to interact with the BioNeMo service, BioPython to parse FASTA sequences into SeqRecord objects, scikit-learn for classification tasks, and matplotlib for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e2fb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt\n",
    "#!pip install -e /home/hroth/Code/nvflare/bionemo_nvflare\n",
    "#!pip install biopython scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3de5184",
   "metadata": {},
   "source": [
    "### Obtaining the protein embeddings using the BioNeMo ESM-1nv model\n",
    "Using BioNeMo, users can obtain numerical vector representations of protein sequences called embeddings. Protein embeddings can then be used for visualization or making downstream predictions.\n",
    "\n",
    "Here we are interested in training a neural network to predict subcellular location from an embedding.\n",
    "\n",
    "The data we will be using comes from the paper [Light attention predicts protein location from the language of life](https://academic.oup.com/bioinformaticsadvances/article/1/1/vbab035/6432029) by StÃ¤rk et al. In this paper, the authors developed a machine learning algorithm to predict the subcellular location of proteins from sequence through protein langage models that are similar to those hosted by BioNeMo. Protein subcellular location refers to where the protein localizes in the cell, for example a protein my be expressed in the Nucleus or in the Cytoplasm. Knowing where proteins localize can provide insights into the underlying mechanisms of cellular processes and help identify potential targets for drug development. The following image includes a few examples of subcellular locations in an animal cell:\n",
    "\n",
    "\n",
    "(Image freely available at https://pixabay.com/images/id-48542)\n",
    "\n",
    "### Dataset sourcing\n",
    "For our target input sequences, we will point to FASTA sequences in a benchmark dataset called Fitness Landscape Inference for Proteins (FLIP). FLIP encompasses experimental data across adeno-associated virus stability for gene therapy, protein domain B1 stability and immunoglobulin binding, and thermostability from multiple protein families."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "096ddf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example protein dataset location\n",
    "fasta_url= \"http://data.bioembeddings.com/public/FLIP/fasta/scl/mixed_soft.fasta\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084fd472",
   "metadata": {},
   "source": [
    "First, we define the source of example protein dataset with the FASTA sequences. This data follows the [biotrainer](https://github.com/sacdallago/biotrainer/blob/main/docs/data_standardization.md) standard, so it includes information about the class in the FASTA header, and the protein sequence. Here are two example sequences in this file:\n",
    "\n",
    "```\n",
    ">Sequence1 TARGET=Cell_membrane SET=train VALIDATION=False\n",
    "MMKTLSSGNCTLNVPAKNSYRMVVLGASRVGKSSIVSRFLNGRFEDQYTPTIEDFHRKVYNIHGDMYQLDILDTSGNHPFPAM\n",
    "RRLSILTGDVFILVFSLDSRESFDEVKRLQKQILEVKSCLKNKTKEAAELPMVICGNKNDHSELCRQVPAMEAELLVSGDENC\n",
    "AYFEVSAKKNTNVNEMFYVLFSMAKLPHEMSPALHHKISVQYGDAFHPRPFCMRRTKVAGAYGMVSPFARRPSVNSDLKYIKA\n",
    "KVLREGQARERDKCSIQ\n",
    ">Sequence4833 TARGET=Nucleus SET=train VALIDATION=False\n",
    "MARTKQTARKSTGGKAPRKQLATKAARKSAPATGGVKKPHRFRPGTVALREIRKYQKSTELLIRKLPFQRLVREIAQDFKTDL\n",
    "RFQSSAVAALQEAAEAYLVGLFEDTNLCAIHAKRVTIMPKDIQLARRIRGERA\n",
    "Note the following attributes in the FASTA header:\n",
    "```\n",
    "\n",
    "* `TARGET` attribute holds the subcellular location classification for the sequence, for instance Cell_membrane and Nucleus. This dataset includes a total of ten subcellelular location classes -- more on that below.\n",
    "* `SET` attribute defines whether the sequence should be used for training (train) or testing (test)\n",
    "* `VALIDATION` attribute defines whether the sequence should be used for validation (all sequences where this is True are also in set=train)\n",
    "\n",
    "### Downloading the protein sequences and subcellular location annotations\n",
    "In this step we download the FASTA file defined above and parse the sequences into a list of BioPython SeqRecord objects.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b29decb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 13949 sequences\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import requests\n",
    "from Bio import SeqIO\n",
    "\n",
    "# Download the FASTA file from FLIP: https://github.com/J-SNACKKB/FLIP/tree/main/splits/scl\n",
    "fasta_content = requests.get(fasta_url, headers={\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x86)'\n",
    "}).content.decode('utf-8')\n",
    "fasta_stream = io.StringIO(fasta_content)\n",
    "\n",
    "# Obtain a list of SeqRecords/proteins which contain sequence and attributes\n",
    "# from the FASTA header\n",
    "proteins = list(SeqIO.parse(fasta_stream, \"fasta\"))\n",
    "print(f\"Downloaded {len(proteins)} sequences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89e551d",
   "metadata": {},
   "source": [
    "### Data splitting\n",
    "Next, we prepare the data for simulating federated learning using `n_clients`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c15629e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 8619 valid sequences.\n",
      "Partition protein dataset with 10 classes into 1 sites with Dirichlet sampling under alpha 100.0\n",
      "{'site-1': {'Cell_membrane': 514,\n",
      "            'Cytoplasm': 1163,\n",
      "            'Endoplasmic_reticulum': 462,\n",
      "            'Extracellular': 1332,\n",
      "            'Golgi_apparatus': 164,\n",
      "            'Lysosome': 149,\n",
      "            'Mitochondrion': 938,\n",
      "            'Nucleus': 1696,\n",
      "            'Peroxisome': 89,\n",
      "            'Plastid': 412}}\n",
      "Saved 6918 training and 1700 testing proteins for site-1, (10/10) unique train/test classes.\n"
     ]
    }
   ],
   "source": [
    "n_clients = 1\n",
    "# limiting to the proteins with sequence length<512 for embedding queries\n",
    "MAX_SEQUENCE_LEN = 512\n",
    "seed=0\n",
    "out_dir = \"/tmp/fasta/mixed_soft\"\n",
    "split_alpha = 100.0  # moderate label heterogeneity of alpha=1.0\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import uuid\n",
    "\n",
    "from importlib import reload\n",
    "import split_data\n",
    "reload(split_data)\n",
    "from split_data import split, list_to_dataframe\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Extract meta data and split\n",
    "data = []\n",
    "for i, x in enumerate(proteins):\n",
    "        if len(str(x.seq)) > MAX_SEQUENCE_LEN:\n",
    "            continue\n",
    "            \n",
    "        entry = {key: value for key, value in re.findall(r\"([A-Z_]+)=(-?[A-z0-9]+[.0-9]*)\", x.description)}\n",
    "        entry[\"sequence\"] = str(x.seq)\n",
    "        entry[\"id\"] = str(i)\n",
    "       \n",
    "        data.append(entry)\n",
    "print(f\"Read {len(data)} valid sequences.\")\n",
    "               \n",
    "# Split the data and save for each client\n",
    "# Note, test_data is kept the same on each client and is not split\n",
    "split(proteins=data, num_sites=n_clients, split_dir=out_dir, alpha=split_alpha)  # use `concat=True` if using separate inference + MLP classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e5ee68",
   "metadata": {},
   "source": [
    "### Federated embedding extraction\n",
    "Running inference of the ESM-1nv model to extract embeddings requires a GPU with at least 12 GB memory. Here we run inference on each client sequentially using one thread to preserve GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c03589",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nvflare import SimulatorRunner    \n",
    "\n",
    "simulator = SimulatorRunner(\n",
    "    job_folder=\"jobs/embeddings\",\n",
    "    workspace=\"/tmp/nvflare/bionemo/embeddings\",\n",
    "    n_clients=n_clients,\n",
    "    threads=1  # due to memory constraints, we run the client execution sequentially in one thread\n",
    ")\n",
    "run_status = simulator.run()\n",
    "print(\"Simulator finished with run_status\", run_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22eb642",
   "metadata": {},
   "source": [
    "### Inspecting the embeddings and labels\n",
    "Embeddings returned from the BioNeMo server are vectors of fixed size for each input sequence. In other words, if we input 10 sequences, we will obtain a matrix `10xD`, where `D` is the size of the embedding (in the case of ESM-1nv, `D=768`). At a glance, these real-valued vector embeddings don't show any obvious features (see the printout in the next cell). But these vectors do contain information that can be used in downstream models to reveal properties of the protein, for example the subcellular location as we'll explore below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbb4805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embeddings from site-1\n",
    "import pickle\n",
    "protein_embeddings = pickle.load(open(os.path.join(out_dir, \"data_site-1.pkl\"), \"rb\"))\n",
    "print(f\"Loaded {len(protein_embeddings)} embeddings from site-1.\")\n",
    "\n",
    "for i in range(4):\n",
    "    protein_embedding = protein_embeddings[i]\n",
    "    print(f\"Inference result contains {list(protein_embedding.keys())}\")\n",
    "    x = protein_embedding[\"embeddings\"]\n",
    "    print(f\"{protein_embedding['id']}: range {np.min(x)}-{np.max(x)}, mean={np.mean(x)}, shape={x.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c159c17",
   "metadata": {},
   "source": [
    "Let's enumerate the labels corresponding to potential subcellular locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32823537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also print all the labels\n",
    "\n",
    "labels = set([entry['TARGET'] for entry in data])\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    print(f\"{i+1}. {label.replace('_', ' ')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1c8415",
   "metadata": {},
   "source": [
    "### Training a MLP to predict subcellular location\n",
    "To be able to classify proteins for their subcellular location, we train a simple scikit-learn Multi-layer Perceptron (MPL) classifier. The MLP model uses a network of hidden layers to fit the input embedding vectors to the model classes (the cellular locations above). In the call below, we define the MLP to use the Adam optimizer with a network of 32 hidden layers, defining a random state (or seed) for reproducibility, and trained for a maximum of 500 iterations.\n",
    "\n",
    "### Local training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2144ef0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"SIM_LOCAL\"] = \"True\"\n",
    "\n",
    "simulator = SimulatorRunner(\n",
    "    job_folder=\"jobs/fedavg\",\n",
    "    workspace=f\"/tmp/nvflare/bionemo/local_alpha{split_alpha}\",\n",
    "    n_clients=n_clients,\n",
    "    threads=n_clients\n",
    ")\n",
    "run_status = simulator.run()\n",
    "print(\"Simulator finished with run_status\", run_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0257944",
   "metadata": {},
   "source": [
    "### Federated learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37b1c06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"SIM_LOCAL\"] = \"False\"\n",
    "\n",
    "simulator = SimulatorRunner(\n",
    "    job_folder=\"jobs/fedavg\",\n",
    "    workspace=f\"/tmp/nvflare/bionemo/fedavg_alpha{split_alpha}\",\n",
    "    n_clients=n_clients,\n",
    "    threads=n_clients\n",
    ")\n",
    "run_status = simulator.run()\n",
    "print(\"Simulator finished with run_status\", run_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68dfe9a",
   "metadata": {},
   "source": [
    "## Finetuning ESM1nv\n",
    "#### Federated Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fedaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing\n",
    "#!python jobs/fedavg_finetune_esm1nv/app/custom/downstream_flip.py do_training=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85816617",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-06 18:40:40,753 - SimulatorRunner - INFO - Create the Simulator Server.\n",
      "2023-12-06 18:40:40,758 - CoreCell - INFO - server: creating listener on tcp://0:38571\n",
      "2023-12-06 18:40:40,781 - CoreCell - INFO - server: created backbone external listener for tcp://0:38571\n",
      "2023-12-06 18:40:40,782 - ConnectorManager - INFO - 261821: Try start_listener Listener resources: {'secure': False, 'host': 'localhost'}\n",
      "2023-12-06 18:40:40,784 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connector [CH00002 PASSIVE tcp://0:11179] is starting\n",
      "2023-12-06 18:40:41,287 - CoreCell - INFO - server: created backbone internal listener for tcp://localhost:11179\n",
      "2023-12-06 18:40:41,288 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connector [CH00001 PASSIVE tcp://0:38571] is starting\n",
      "2023-12-06 18:40:41,384 - nvflare.fuel.hci.server.hci - INFO - Starting Admin Server localhost on Port 39331\n",
      "2023-12-06 18:40:41,385 - SimulatorRunner - INFO - Deploy the Apps.\n",
      "2023-12-06 18:40:41,392 - SimulatorRunner - INFO - Create the simulate clients.\n",
      "2023-12-06 18:40:41,397 - ClientManager - INFO - Client: New client site-1@10.117.19.216 joined. Sent token: 137531df-ffe5-4a34-8d2f-2c1b6dd95cbb.  Total clients: 1\n",
      "2023-12-06 18:40:41,398 - FederatedClient - INFO - Successfully registered client:site-1 for project simulator_server. Token:137531df-ffe5-4a34-8d2f-2c1b6dd95cbb SSID:\n",
      "2023-12-06 18:40:41,399 - SimulatorRunner - INFO - Set the client status ready.\n",
      "2023-12-06 18:40:41,400 - SimulatorRunner - INFO - Deploy and start the Server App.\n",
      "2023-12-06 18:40:41,404 - Cell - INFO - Register blob CB for channel='server_command', topic='*'\n",
      "2023-12-06 18:40:41,406 - Cell - INFO - Register blob CB for channel='aux_communication', topic='*'\n",
      "2023-12-06 18:40:41,408 - ServerCommandAgent - INFO - ServerCommandAgent cell register_request_cb: server.simulate_job\n",
      "2023-12-06 18:40:46,169 - torch.distributed.nn.jit.instantiator - INFO - Created a temporary directory at /tmp/tmpx9ye35ao\n",
      "2023-12-06 18:40:46,171 - torch.distributed.nn.jit.instantiator - INFO - Writing /tmp/tmpx9ye35ao/_remote_module_non_scriptable.py\n",
      "2023-12-06 18:40:46,856 - IntimeModelSelector - INFO - model selection weights control: None\n",
      "2023-12-06 18:40:46,860 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job]: Server runner starting ...\n",
      "2023-12-06 18:40:46,863 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job]: starting workflow scatter_and_gather (<class 'nvflare.app_common.workflows.scatter_and_gather.ScatterAndGather'>) ...\n",
      "2023-12-06 18:40:46,864 - ScatterAndGather - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: Initializing ScatterAndGather workflow.\n",
      "2023-12-06 18:40:46,866 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: Workflow scatter_and_gather (<class 'nvflare.app_common.workflows.scatter_and_gather.ScatterAndGather'>) started\n",
      "2023-12-06 18:40:46,868 - ScatterAndGather - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: Beginning ScatterAndGather training phase.\n",
      "2023-12-06 18:40:46,870 - ScatterAndGather - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: Round 0 started.\n",
      "2023-12-06 18:40:46,871 - ScatterAndGather - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: scheduled task train\n",
      "2023-12-06 18:40:47,413 - SimulatorClientRunner - INFO - Start the clients run simulation.\n",
      "2023-12-06 18:40:48,419 - SimulatorClientRunner - INFO - Simulate Run client: site-1 on GPU group: None\n",
      "2023-12-06 18:40:49,503 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connection [CN00004 127.0.0.1:38571 <= 127.0.0.1:58866] is created: PID: 261821\n",
      "2023-12-06 18:40:49,477 - ClientTaskWorker - INFO - ClientTaskWorker started to run\n",
      "2023-12-06 18:40:49,501 - CoreCell - INFO - site-1.simulate_job: created backbone external connector to tcp://localhost:38571\n",
      "2023-12-06 18:40:49,501 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connector [CH00001 ACTIVE tcp://localhost:38571] is starting\n",
      "2023-12-06 18:40:49,502 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connection [CN00002 127.0.0.1:58866 => 127.0.0.1:38571] is created: PID: 261963\n",
      "2023-12-06 18:40:54,492 - torch.distributed.nn.jit.instantiator - INFO - Created a temporary directory at /tmp/tmps6358n9h\n",
      "2023-12-06 18:40:54,492 - torch.distributed.nn.jit.instantiator - INFO - Writing /tmp/tmps6358n9h/_remote_module_non_scriptable.py\n",
      "2023-12-06 18:40:55,711 - Cell - INFO - Register blob CB for channel='aux_communication', topic='*'\n",
      "2023-12-06 18:40:56,237 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-1, peer_run=simulate_job, task_name=train, task_id=a3ab6143-85a0-4156-827d-79bb1a07a8a6]: assigned task to client site-1: name=train, id=a3ab6143-85a0-4156-827d-79bb1a07a8a6\n",
      "2023-12-06 18:40:56,239 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-1, peer_run=simulate_job, task_name=train, task_id=a3ab6143-85a0-4156-827d-79bb1a07a8a6]: sent task assignment to client. client_name:site-1 task_id:a3ab6143-85a0-4156-827d-79bb1a07a8a6\n",
      "2023-12-06 18:40:56,241 - GetTaskCommand - INFO - return task to client.  client_name: site-1  task_name: train   task_id: a3ab6143-85a0-4156-827d-79bb1a07a8a6  sharable_header_task_id: a3ab6143-85a0-4156-827d-79bb1a07a8a6\n",
      "2023-12-06 18:40:56,217 - Cell - INFO - broadcast: channel='aux_communication', topic='__sync_runner__', targets=['server.simulate_job'], timeout=2.0\n",
      "2023-12-06 18:40:56,226 - ClientRunner - INFO - [identity=site-1, run=simulate_job]: synced to Server Runner in 0.5098187923431396 seconds\n",
      "2023-12-06 18:40:56,231 - ClientRunner - INFO - [identity=site-1, run=simulate_job]: client runner started\n",
      "2023-12-06 18:40:56,231 - ClientTaskWorker - INFO - Initialize ClientRunner for client: site-1\n",
      "2023-12-06 18:40:56,245 - Communicator - INFO - Received from simulator_server server. getTask: train size: 644B (644 Bytes) time: 0.013311 seconds\n",
      "2023-12-06 18:40:56,245 - FederatedClient - INFO - pull_task completed. Task name:train Status:True \n",
      "2023-12-06 18:40:56,245 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job]: got task assignment: name=train, id=a3ab6143-85a0-4156-827d-79bb1a07a8a6\n",
      "2023-12-06 18:40:56,246 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=a3ab6143-85a0-4156-827d-79bb1a07a8a6]: invoking task executor PTClientAPILauncherExecutor\n",
      "2023-12-06 18:40:56,246 - PTClientAPILauncherExecutor - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=a3ab6143-85a0-4156-827d-79bb1a07a8a6]: execute for task (train)\n",
      "2023-12-06 18:40:56,246 - PTClientAPILauncherExecutor - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=a3ab6143-85a0-4156-827d-79bb1a07a8a6]: External execution for task (train) is launched.\n",
      "[NeMo W 2023-12-06 18:41:10 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'downstream_flip_scl': Defaults list is missing `_self_`. See https://hydra.cc/docs/upgrades/1.0_to_1.1/default_composition_order for more information\n",
      "      warnings.warn(msg, UserWarning)\n",
      "    \n",
      "[NeMo W 2023-12-06 18:41:10 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "    See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "      ret = run_job(\n",
      "    \n",
      "[NeMo I 2023-12-06 18:41:10 downstream_flip:34] \n",
      "    \n",
      "    ************* Finetune config ****************\n",
      "[NeMo I 2023-12-06 18:41:10 downstream_flip:35] \n",
      "    name: esm2nv_flip\n",
      "    do_training: true\n",
      "    do_testing: false\n",
      "    restore_from_path: ${oc.env:BIONEMO_HOME}/models/protein/esm2nv/esm2nv_650M_converted.nemo\n",
      "    trainer:\n",
      "      devices: 2\n",
      "      num_nodes: 1\n",
      "      accelerator: gpu\n",
      "      precision: 16\n",
      "      logger: false\n",
      "      enable_checkpointing: false\n",
      "      replace_sampler_ddp: false\n",
      "      max_epochs: 500\n",
      "      log_every_n_steps: 10\n",
      "      val_check_interval: 20\n",
      "      limit_val_batches: 1000\n",
      "      limit_test_batches: 1000\n",
      "      accumulate_grad_batches: 1\n",
      "      gradient_clip_val: 1.0\n",
      "      benchmark: false\n",
      "      max_steps: 5000000\n",
      "    exp_manager:\n",
      "      name: ${name}\n",
      "      exp_dir: ${.name}/${.wandb_logger_kwargs.name}\n",
      "      explicit_log_dir: ${.exp_dir}\n",
      "      create_wandb_logger: true\n",
      "      create_tensorboard_logger: true\n",
      "      wandb_logger_kwargs:\n",
      "        project: ${name}_${model.data.task_name}_finetuning\n",
      "        name: ${name}_${model.data.task_name}_finetuning_encoder_frozen_${model.encoder_frozen}\n",
      "        group: ${name}\n",
      "        job_type: Localhost_nodes_${trainer.num_nodes}_gpus_${trainer.devices}\n",
      "        notes: 'date: ${now:%y%m%d-%H%M%S}'\n",
      "        tags:\n",
      "        - ${name}\n",
      "        offline: true\n",
      "      resume_if_exists: false\n",
      "      resume_ignore_no_checkpoint: true\n",
      "      create_checkpoint_callback: true\n",
      "      checkpoint_callback_params:\n",
      "        monitor: val_TARGET_accuracy\n",
      "        save_top_k: 1\n",
      "        mode: max\n",
      "        always_save_nemo: false\n",
      "        filename: esm2nv--{val_loss:.2f}-{step}-{consumed_samples}\n",
      "        model_parallel_size: ${multiply:${model.tensor_model_parallel_size}, ${model.pipeline_model_parallel_size}}\n",
      "    model:\n",
      "      precision: 16\n",
      "      micro_batch_size: 8\n",
      "      seq_length: 1024\n",
      "      num_layers: 33\n",
      "      hidden_size: 1280\n",
      "      ffn_hidden_size: ${multiply:${model.hidden_size}, 4}\n",
      "      num_attention_heads: 20\n",
      "      megatron_legacy: false\n",
      "      position_embedding_type: rope\n",
      "      hidden_dropout: 0\n",
      "      embedding_use_attention_mask: true\n",
      "      embedding_token_dropout: true\n",
      "      mask_token_id: ${.tokenizer.mask_id}\n",
      "      attention_dropout: 0.0\n",
      "      normalize_attention_scores: false\n",
      "      tensor_model_parallel_size: 1\n",
      "      pipeline_model_parallel_size: 1\n",
      "      bias_gelu_fusion: false\n",
      "      use_esm_attention: true\n",
      "      esm_gelu: true\n",
      "      use_pt_layernorm: false\n",
      "      use_pt_mlp_out: false\n",
      "      max_position_embeddings: ${.seq_length}\n",
      "      encoder_seq_length: ${.seq_length}\n",
      "      optim:\n",
      "        name: fused_adam\n",
      "        lr: 0.0004\n",
      "        weight_decay: 0.01\n",
      "        betas:\n",
      "        - 0.9\n",
      "        - 0.98\n",
      "        sched:\n",
      "          name: CosineAnnealing\n",
      "          warmup_steps: 2000\n",
      "          constant_steps: 50000\n",
      "          min_lr: 4.0e-05\n",
      "      init_method_std: 0.02\n",
      "      kv_channels: null\n",
      "      apply_query_key_layer_scaling: true\n",
      "      layernorm_epsilon: 1.0e-05\n",
      "      make_vocab_size_divisible_by: 128\n",
      "      pre_process: true\n",
      "      post_process: false\n",
      "      bert_binary_head: false\n",
      "      resume_from_checkpoint: null\n",
      "      masked_softmax_fusion: true\n",
      "      tokenizer:\n",
      "        library: huggingface\n",
      "        type: BertWordPieceLowerCase\n",
      "        model_name: facebook/esm2_t33_650M_UR50D\n",
      "        mask_id: 32\n",
      "        model: null\n",
      "        vocab_file: null\n",
      "        merge_file: null\n",
      "      native_amp_init_scale: 4294967296\n",
      "      native_amp_growth_interval: 1000\n",
      "      fp32_residual_connection: false\n",
      "      fp16_lm_cross_entropy: false\n",
      "      seed: 1234\n",
      "      use_cpu_initialization: false\n",
      "      onnx_safe: false\n",
      "      activations_checkpoint_method: null\n",
      "      activations_checkpoint_num_layers: 1\n",
      "      data:\n",
      "        ngc_registry_target: uniref50_2022_05\n",
      "        ngc_registry_version: v23.06\n",
      "        data_prefix: ''\n",
      "        num_workers: 8\n",
      "        dataloader_type: single\n",
      "        reset_position_ids: false\n",
      "        reset_attention_mask: false\n",
      "        eod_mask_loss: false\n",
      "        masked_lm_prob: 0.15\n",
      "        short_seq_prob: 0.1\n",
      "        skip_lines: 0\n",
      "        drop_last: false\n",
      "        pin_memory: false\n",
      "        dynamic_padding: false\n",
      "        force_regen_sample_mapping: false\n",
      "        preprocessing:\n",
      "          num_preprocess_workers: 16\n",
      "        train:\n",
      "          data_impl: csv_mmap\n",
      "          data_impl_kwargs:\n",
      "            csv_mmap:\n",
      "              data_col: 1\n",
      "              header_lines: 1\n",
      "          use_upsampling: true\n",
      "          range: x[000..049]\n",
      "          sample_from_map: true\n",
      "          uf50_datapath: ${oc.env:BIONEMO_HOME}/data/uniref202104_esm2/uniref50_train_filt.fasta\n",
      "          uf90_datapath: ${oc.env:BIONEMO_HOME}/data/uniref202104_esm2/uniref90membersandreps_ur50trainfiltreps.fasta\n",
      "          cluster_mapping_tsv: ${oc.env:BIONEMO_HOME}/data/uniref202104_esm2/uniref50_mapper_to_uniref90clustermembers_andselfuniref50.tsv\n",
      "          dataset_path: ${oc.env:BIONEMO_HOME}/data/uniref202104_esm2/uf50\n",
      "          sort_fastas: true\n",
      "          num_workers: ${model.data.num_workers}\n",
      "          uf90:\n",
      "            uniref90_path: ${oc.env:BIONEMO_HOME}/data/uniref202104_esm2/uf90/\n",
      "            dataset:\n",
      "              uf90_csvs: x[000..049]\n",
      "            data_impl: csv_fields_mmap\n",
      "            data_impl_kwargs:\n",
      "              csv_fields_mmap:\n",
      "                header_lines: 1\n",
      "                newline_int: 10\n",
      "                workers: ${model.data.train.num_workers}\n",
      "                sort_dataset_paths: true\n",
      "                data_sep: ','\n",
      "                data_fields:\n",
      "                  sequence: 3\n",
      "                  sequence_id: 1\n",
      "            index_mapping_dir: ${model.data.index_mapping_dir}\n",
      "          max_seq_length: ${model.seq_length}\n",
      "          seed: ${model.seed}\n",
      "        val:\n",
      "          use_upsampling: false\n",
      "          range: x[000..049]\n",
      "          uf50_datapath: ${oc.env:BIONEMO_HOME}/data/dataqc/uniref50_train_filt.fasta\n",
      "          dataset_path: ${oc.env:BIONEMO_HOME}/data/dataqc/uf50\n",
      "          data_impl: csv_fields_mmap\n",
      "          num_workers: ${model.data.num_workers}\n",
      "          data_impl_kwargs:\n",
      "            csv_fields_mmap:\n",
      "              header_lines: 1\n",
      "              newline_int: 10\n",
      "              workers: ${model.data.val.num_workers}\n",
      "              sort_dataset_paths: true\n",
      "              data_sep: ','\n",
      "              data_fields:\n",
      "                sequence: 3\n",
      "                sequence_id: 1\n",
      "            index_mapping_dir: ${model.data.index_mapping_dir}\n",
      "          max_seq_length: ${model.seq_length}\n",
      "          seed: ${model.seed}\n",
      "        test:\n",
      "          use_upsampling: false\n",
      "          range: x[000..049]\n",
      "          uf50_datapath: ${oc.env:BIONEMO_HOME}/data/uniref202104_esm2/uniref50_train_filt.fasta\n",
      "          dataset_path: ${oc.env:BIONEMO_HOME}/data/dataqc/uf50\n",
      "          data_impl: csv_fields_mmap\n",
      "          num_workers: ${model.data.num_workers}\n",
      "          data_impl_kwargs:\n",
      "            csv_fields_mmap:\n",
      "              header_lines: 1\n",
      "              newline_int: 10\n",
      "              workers: ${model.data.test.num_workers}\n",
      "              sort_dataset_paths: true\n",
      "              data_sep: ','\n",
      "              data_fields:\n",
      "                sequence: 3\n",
      "                sequence_id: 1\n",
      "            index_mapping_dir: ${model.data.index_mapping_dir}\n",
      "          max_seq_length: ${model.seq_length}\n",
      "          seed: ${model.seed}\n",
      "        modify_percent: 0.1\n",
      "        perturb_percent: 0.5\n",
      "        index_mapping_dir: ${oc.env:BIONEMO_HOME}/data/uniref202104_esm2/\n",
      "        task_name: scl\n",
      "        task_type: classification\n",
      "        preprocessed_data_path: /tmp/fasta\n",
      "        dataset_path: ${model.data.preprocessed_data_path}/mixed_soft\n",
      "        dataset:\n",
      "          train: data_train_site-1\n",
      "          val: data_val_site-1\n",
      "          test: data_test_site-1\n",
      "        sequence_column: sequence\n",
      "        target_column:\n",
      "        - TARGET\n",
      "        target_sizes:\n",
      "        - 10\n",
      "        num_classes: 10\n",
      "        shuffle: true\n",
      "        max_seq_length: ${model.seq_length}\n",
      "        emb_batch_size: ${model.micro_batch_size}\n",
      "      dwnstr_task_validation:\n",
      "        enabled: false\n",
      "        dataset:\n",
      "          class: bionemo.model.core.dwnstr_task_callbacks.PerTokenPredictionCallback\n",
      "          task_type: token-level-classification\n",
      "          infer_target: bionemo.model.protein.esm1nv.infer.ESM1nvInference\n",
      "          max_seq_length: ${model.seq_length}\n",
      "          emb_batch_size: 128\n",
      "          batch_size: 128\n",
      "          num_epochs: 10\n",
      "          shuffle: true\n",
      "          num_workers: 8\n",
      "          task_name: secondary_structure\n",
      "          dataset_path: ${oc.env:BIONEMO_HOME}/data/FLIP/${model.dwnstr_task_validation.dataset.task_name}\n",
      "          dataset:\n",
      "            train: x000\n",
      "            test: x000\n",
      "          sequence_column: sequence\n",
      "          target_column:\n",
      "          - 3state\n",
      "          - resolved\n",
      "          target_sizes:\n",
      "          - 3\n",
      "          - 2\n",
      "          mask_column:\n",
      "          - resolved\n",
      "          - null\n",
      "          random_seed: 1234\n",
      "          optim:\n",
      "            name: adam\n",
      "            lr: 0.0001\n",
      "            betas:\n",
      "            - 0.9\n",
      "            - 0.999\n",
      "            eps: 1.0e-08\n",
      "            weight_decay: 0.01\n",
      "            sched:\n",
      "              name: WarmupAnnealing\n",
      "              min_lr: 1.0e-05\n",
      "              last_epoch: -1\n",
      "              warmup_ratio: 0.01\n",
      "              max_steps: 1000\n",
      "      encoder_frozen: false\n",
      "      global_batch_size: null\n",
      "      loss_func: CrossEntropyLoss\n",
      "      hidden_layer_size: 256\n",
      "      finetuning_optim:\n",
      "        name: adam\n",
      "        lr: 0.0005\n",
      "        betas:\n",
      "        - 0.9\n",
      "        - 0.999\n",
      "        eps: 1.0e-08\n",
      "        weight_decay: 0.0005\n",
      "        sched:\n",
      "          name: WarmupAnnealing\n",
      "          min_lr: 1.0e-05\n",
      "          last_epoch: -1\n",
      "          warmup_steps: 10\n",
      "    target: bionemo.model.protein.esm1nv.ESM2nvModel\n",
      "    infer_target: bionemo.model.protein.esm1nv.infer.ESM1nvInference\n",
      "    \n",
      "[NeMo I 2023-12-06 18:41:10 downstream_flip:38] ************** Starting Training ***********\n",
      "[NeMo W 2023-12-06 18:41:10 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/plugins/precision/native_amp.py:131: LightningDeprecationWarning: The `NativeMixedPrecisionPlugin` class has been renamed in v1.9.0 and will be removed in v2.0.0. Please use `pytorch_lightning.plugins.MixedPrecisionPlugin` instead.\n",
      "      rank_zero_deprecation(\n",
      "    \n",
      "[NeMo I 2023-12-06 18:41:10 utils:178] Selected Callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]\n",
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "[NeMo E 2023-12-06 18:41:10 exp_manager:646] exp_manager received explicit_log_dir: esm2nv_flip/esm2nv_flip_scl_finetuning_encoder_frozen_False and at least one of exp_dir: esm2nv_flip/esm2nv_flip_scl_finetuning_encoder_frozen_False, or version: None. Please note that exp_dir, name, and version will be ignored.\n",
      "[NeMo I 2023-12-06 18:41:10 exp_manager:374] Experiments will be logged at esm2nv_flip/esm2nv_flip_scl_finetuning_encoder_frozen_False\n",
      "[NeMo I 2023-12-06 18:41:10 exp_manager:797] TensorboardLogger has been set up\n",
      "wandb: WARNING `resume` will be ignored since W&B syncing is set to `offline`. Starting a new run with run id zz3paqub.\n",
      "wandb: Tracking run with wandb version 0.15.6\n",
      "wandb: W&B syncing is set to `offline` in this directory.  \n",
      "wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.\n",
      "[NeMo I 2023-12-06 18:41:11 exp_manager:812] WandBLogger has been set up\n",
      "[NeMo W 2023-12-06 18:41:11 exp_manager:893] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to 5000000. Please ensure that max_steps will run for at least 1 epochs to ensure that checkpointing will not error out.\n",
      "[NeMo I 2023-12-06 18:41:11 utils:198] Resuming training from checkpoint: None\n",
      "[NeMo I 2023-12-06 18:41:11 utils:241] \n",
      "    \n",
      "    ************** Trainer configuration ***********\n",
      "[NeMo I 2023-12-06 18:41:11 utils:242] \n",
      "    name: esm2nv_flip\n",
      "    do_training: true\n",
      "    do_testing: false\n",
      "    restore_from_path: ${oc.env:BIONEMO_HOME}/models/protein/esm2nv/esm2nv_650M_converted.nemo\n",
      "    trainer:\n",
      "      devices: 2\n",
      "      num_nodes: 1\n",
      "      accelerator: gpu\n",
      "      precision: 16\n",
      "      logger: false\n",
      "      enable_checkpointing: false\n",
      "      replace_sampler_ddp: false\n",
      "      max_epochs: 500\n",
      "      log_every_n_steps: 10\n",
      "      val_check_interval: 20\n",
      "      limit_val_batches: 1000\n",
      "      limit_test_batches: 1000\n",
      "      accumulate_grad_batches: 1\n",
      "      gradient_clip_val: 1.0\n",
      "      benchmark: false\n",
      "      max_steps: 5000000\n",
      "    exp_manager:\n",
      "      name: ${name}\n",
      "      exp_dir: ${.name}/${.wandb_logger_kwargs.name}\n",
      "      explicit_log_dir: ${.exp_dir}\n",
      "      create_wandb_logger: true\n",
      "      create_tensorboard_logger: true\n",
      "      wandb_logger_kwargs:\n",
      "        project: ${name}_${model.data.task_name}_finetuning\n",
      "        name: ${name}_${model.data.task_name}_finetuning_encoder_frozen_${model.encoder_frozen}\n",
      "        group: ${name}\n",
      "        job_type: Localhost_nodes_${trainer.num_nodes}_gpus_${trainer.devices}\n",
      "        notes: 'date: ${now:%y%m%d-%H%M%S}'\n",
      "        tags:\n",
      "        - ${name}\n",
      "        offline: true\n",
      "      resume_if_exists: false\n",
      "      resume_ignore_no_checkpoint: true\n",
      "      create_checkpoint_callback: true\n",
      "      checkpoint_callback_params:\n",
      "        monitor: val_TARGET_accuracy\n",
      "        save_top_k: 1\n",
      "        mode: max\n",
      "        always_save_nemo: false\n",
      "        filename: esm2nv--{val_loss:.2f}-{step}-{consumed_samples}\n",
      "        model_parallel_size: ${multiply:${model.tensor_model_parallel_size}, ${model.pipeline_model_parallel_size}}\n",
      "    model:\n",
      "      precision: 16\n",
      "      micro_batch_size: 8\n",
      "      seq_length: 1024\n",
      "      num_layers: 33\n",
      "      hidden_size: 1280\n",
      "      ffn_hidden_size: ${multiply:${model.hidden_size}, 4}\n",
      "      num_attention_heads: 20\n",
      "      megatron_legacy: false\n",
      "      position_embedding_type: rope\n",
      "      hidden_dropout: 0\n",
      "      embedding_use_attention_mask: true\n",
      "      embedding_token_dropout: true\n",
      "      mask_token_id: ${.tokenizer.mask_id}\n",
      "      attention_dropout: 0.0\n",
      "      normalize_attention_scores: false\n",
      "      tensor_model_parallel_size: 1\n",
      "      pipeline_model_parallel_size: 1\n",
      "      bias_gelu_fusion: false\n",
      "      use_esm_attention: true\n",
      "      esm_gelu: true\n",
      "      use_pt_layernorm: false\n",
      "      use_pt_mlp_out: false\n",
      "      max_position_embeddings: ${.seq_length}\n",
      "      encoder_seq_length: ${.seq_length}\n",
      "      optim:\n",
      "        name: fused_adam\n",
      "        lr: 0.0004\n",
      "        weight_decay: 0.01\n",
      "        betas:\n",
      "        - 0.9\n",
      "        - 0.98\n",
      "        sched:\n",
      "          name: CosineAnnealing\n",
      "          warmup_steps: 2000\n",
      "          constant_steps: 50000\n",
      "          min_lr: 4.0e-05\n",
      "      init_method_std: 0.02\n",
      "      kv_channels: null\n",
      "      apply_query_key_layer_scaling: true\n",
      "      layernorm_epsilon: 1.0e-05\n",
      "      make_vocab_size_divisible_by: 128\n",
      "      pre_process: true\n",
      "      post_process: false\n",
      "      bert_binary_head: false\n",
      "      resume_from_checkpoint: null\n",
      "      masked_softmax_fusion: true\n",
      "      tokenizer:\n",
      "        library: huggingface\n",
      "        type: BertWordPieceLowerCase\n",
      "        model_name: facebook/esm2_t33_650M_UR50D\n",
      "        mask_id: 32\n",
      "        model: null\n",
      "        vocab_file: null\n",
      "        merge_file: null\n",
      "      native_amp_init_scale: 4294967296\n",
      "      native_amp_growth_interval: 1000\n",
      "      fp32_residual_connection: false\n",
      "      fp16_lm_cross_entropy: false\n",
      "      seed: 1234\n",
      "      use_cpu_initialization: false\n",
      "      onnx_safe: false\n",
      "      activations_checkpoint_method: null\n",
      "      activations_checkpoint_num_layers: 1\n",
      "      data:\n",
      "        ngc_registry_target: uniref50_2022_05\n",
      "        ngc_registry_version: v23.06\n",
      "        data_prefix: ''\n",
      "        num_workers: 8\n",
      "        dataloader_type: single\n",
      "        reset_position_ids: false\n",
      "        reset_attention_mask: false\n",
      "        eod_mask_loss: false\n",
      "        masked_lm_prob: 0.15\n",
      "        short_seq_prob: 0.1\n",
      "        skip_lines: 0\n",
      "        drop_last: false\n",
      "        pin_memory: false\n",
      "        dynamic_padding: false\n",
      "        force_regen_sample_mapping: false\n",
      "        preprocessing:\n",
      "          num_preprocess_workers: 16\n",
      "        train:\n",
      "          data_impl: csv_mmap\n",
      "          data_impl_kwargs:\n",
      "            csv_mmap:\n",
      "              data_col: 1\n",
      "              header_lines: 1\n",
      "          use_upsampling: true\n",
      "          range: x[000..049]\n",
      "          sample_from_map: true\n",
      "          uf50_datapath: ${oc.env:BIONEMO_HOME}/data/uniref202104_esm2/uniref50_train_filt.fasta\n",
      "          uf90_datapath: ${oc.env:BIONEMO_HOME}/data/uniref202104_esm2/uniref90membersandreps_ur50trainfiltreps.fasta\n",
      "          cluster_mapping_tsv: ${oc.env:BIONEMO_HOME}/data/uniref202104_esm2/uniref50_mapper_to_uniref90clustermembers_andselfuniref50.tsv\n",
      "          dataset_path: ${oc.env:BIONEMO_HOME}/data/uniref202104_esm2/uf50\n",
      "          sort_fastas: true\n",
      "          num_workers: ${model.data.num_workers}\n",
      "          uf90:\n",
      "            uniref90_path: ${oc.env:BIONEMO_HOME}/data/uniref202104_esm2/uf90/\n",
      "            dataset:\n",
      "              uf90_csvs: x[000..049]\n",
      "            data_impl: csv_fields_mmap\n",
      "            data_impl_kwargs:\n",
      "              csv_fields_mmap:\n",
      "                header_lines: 1\n",
      "                newline_int: 10\n",
      "                workers: ${model.data.train.num_workers}\n",
      "                sort_dataset_paths: true\n",
      "                data_sep: ','\n",
      "                data_fields:\n",
      "                  sequence: 3\n",
      "                  sequence_id: 1\n",
      "            index_mapping_dir: ${model.data.index_mapping_dir}\n",
      "          max_seq_length: ${model.seq_length}\n",
      "          seed: ${model.seed}\n",
      "        val:\n",
      "          use_upsampling: false\n",
      "          range: x[000..049]\n",
      "          uf50_datapath: ${oc.env:BIONEMO_HOME}/data/dataqc/uniref50_train_filt.fasta\n",
      "          dataset_path: ${oc.env:BIONEMO_HOME}/data/dataqc/uf50\n",
      "          data_impl: csv_fields_mmap\n",
      "          num_workers: ${model.data.num_workers}\n",
      "          data_impl_kwargs:\n",
      "            csv_fields_mmap:\n",
      "              header_lines: 1\n",
      "              newline_int: 10\n",
      "              workers: ${model.data.val.num_workers}\n",
      "              sort_dataset_paths: true\n",
      "              data_sep: ','\n",
      "              data_fields:\n",
      "                sequence: 3\n",
      "                sequence_id: 1\n",
      "            index_mapping_dir: ${model.data.index_mapping_dir}\n",
      "          max_seq_length: ${model.seq_length}\n",
      "          seed: ${model.seed}\n",
      "        test:\n",
      "          use_upsampling: false\n",
      "          range: x[000..049]\n",
      "          uf50_datapath: ${oc.env:BIONEMO_HOME}/data/uniref202104_esm2/uniref50_train_filt.fasta\n",
      "          dataset_path: ${oc.env:BIONEMO_HOME}/data/dataqc/uf50\n",
      "          data_impl: csv_fields_mmap\n",
      "          num_workers: ${model.data.num_workers}\n",
      "          data_impl_kwargs:\n",
      "            csv_fields_mmap:\n",
      "              header_lines: 1\n",
      "              newline_int: 10\n",
      "              workers: ${model.data.test.num_workers}\n",
      "              sort_dataset_paths: true\n",
      "              data_sep: ','\n",
      "              data_fields:\n",
      "                sequence: 3\n",
      "                sequence_id: 1\n",
      "            index_mapping_dir: ${model.data.index_mapping_dir}\n",
      "          max_seq_length: ${model.seq_length}\n",
      "          seed: ${model.seed}\n",
      "        modify_percent: 0.1\n",
      "        perturb_percent: 0.5\n",
      "        index_mapping_dir: ${oc.env:BIONEMO_HOME}/data/uniref202104_esm2/\n",
      "        task_name: scl\n",
      "        task_type: classification\n",
      "        preprocessed_data_path: /tmp/fasta\n",
      "        dataset_path: ${model.data.preprocessed_data_path}/mixed_soft\n",
      "        dataset:\n",
      "          train: data_train_site-1\n",
      "          val: data_val_site-1\n",
      "          test: data_test_site-1\n",
      "        sequence_column: sequence\n",
      "        target_column:\n",
      "        - TARGET\n",
      "        target_sizes:\n",
      "        - 10\n",
      "        num_classes: 10\n",
      "        shuffle: true\n",
      "        max_seq_length: ${model.seq_length}\n",
      "        emb_batch_size: ${model.micro_batch_size}\n",
      "      dwnstr_task_validation:\n",
      "        enabled: false\n",
      "        dataset:\n",
      "          class: bionemo.model.core.dwnstr_task_callbacks.PerTokenPredictionCallback\n",
      "          task_type: token-level-classification\n",
      "          infer_target: bionemo.model.protein.esm1nv.infer.ESM1nvInference\n",
      "          max_seq_length: ${model.seq_length}\n",
      "          emb_batch_size: 128\n",
      "          batch_size: 128\n",
      "          num_epochs: 10\n",
      "          shuffle: true\n",
      "          num_workers: 8\n",
      "          task_name: secondary_structure\n",
      "          dataset_path: ${oc.env:BIONEMO_HOME}/data/FLIP/${model.dwnstr_task_validation.dataset.task_name}\n",
      "          dataset:\n",
      "            train: x000\n",
      "            test: x000\n",
      "          sequence_column: sequence\n",
      "          target_column:\n",
      "          - 3state\n",
      "          - resolved\n",
      "          target_sizes:\n",
      "          - 3\n",
      "          - 2\n",
      "          mask_column:\n",
      "          - resolved\n",
      "          - null\n",
      "          random_seed: 1234\n",
      "          optim:\n",
      "            name: adam\n",
      "            lr: 0.0001\n",
      "            betas:\n",
      "            - 0.9\n",
      "            - 0.999\n",
      "            eps: 1.0e-08\n",
      "            weight_decay: 0.01\n",
      "            sched:\n",
      "              name: WarmupAnnealing\n",
      "              min_lr: 1.0e-05\n",
      "              last_epoch: -1\n",
      "              warmup_ratio: 0.01\n",
      "              max_steps: 1000\n",
      "      encoder_frozen: false\n",
      "      global_batch_size: 16\n",
      "      loss_func: CrossEntropyLoss\n",
      "      hidden_layer_size: 256\n",
      "      finetuning_optim:\n",
      "        name: adam\n",
      "        lr: 0.0005\n",
      "        betas:\n",
      "        - 0.9\n",
      "        - 0.999\n",
      "        eps: 1.0e-08\n",
      "        weight_decay: 0.0005\n",
      "        sched:\n",
      "          name: WarmupAnnealing\n",
      "          min_lr: 1.0e-05\n",
      "          last_epoch: -1\n",
      "          warmup_steps: 10\n",
      "    target: bionemo.model.protein.esm1nv.ESM2nvModel\n",
      "    infer_target: bionemo.model.protein.esm1nv.infer.ESM1nvInference\n",
      "    \n",
      "[NeMo I 2023-12-06 18:41:14 utils:257] Restoring model from /opt/nvidia/bionemo/models/protein/esm2nv/esm2nv_650M_converted.nemo\n",
      "[NeMo I 2023-12-06 18:41:14 utils:261] Loading model class: bionemo.model.protein.esm1nv.ESM2nvModel\n",
      "[NeMo W 2023-12-06 18:41:14 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/plugins/precision/native_amp.py:131: LightningDeprecationWarning: The `NativeMixedPrecisionPlugin` class has been renamed in v1.9.0 and will be removed in v2.0.0. Please use `pytorch_lightning.plugins.MixedPrecisionPlugin` instead.\n",
      "      rank_zero_deprecation(\n",
      "    \n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "[NeMo E 2023-12-06 18:41:14 exp_manager:646] exp_manager received explicit_log_dir: esm2nv_flip/esm2nv_flip_scl_finetuning_encoder_frozen_False and at least one of exp_dir: esm2nv_flip/esm2nv_flip_scl_finetuning_encoder_frozen_False, or version: None. Please note that exp_dir, name, and version will be ignored.\n",
      "[NeMo W 2023-12-06 18:41:14 exp_manager:651] Exp_manager is logging to esm2nv_flip/esm2nv_flip_scl_finetuning_encoder_frozen_False, but it already exists.\n",
      "[NeMo I 2023-12-06 18:41:14 exp_manager:374] Experiments will be logged at esm2nv_flip/esm2nv_flip_scl_finetuning_encoder_frozen_False\n",
      "[NeMo I 2023-12-06 18:41:14 exp_manager:797] TensorboardLogger has been set up\n",
      "[NeMo W 2023-12-06 18:41:14 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loggers/wandb.py:395: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "[NeMo I 2023-12-06 18:41:14 exp_manager:812] WandBLogger has been set up\n",
      "[NeMo W 2023-12-06 18:41:14 exp_manager:893] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to 5000000. Please ensure that max_steps will run for at least 1 epochs to ensure that checkpointing will not error out.\n",
      "[NeMo I 2023-12-06 18:41:14 utils:241] \n",
      "    \n",
      "    ************** Trainer configuration ***********\n",
      "[NeMo I 2023-12-06 18:41:14 utils:242] \n",
      "    name: esm2nv_flip\n",
      "    do_training: true\n",
      "    do_testing: false\n",
      "    restore_from_path: ${oc.env:BIONEMO_HOME}/models/protein/esm2nv/esm2nv_650M_converted.nemo\n",
      "    trainer:\n",
      "      devices: 2\n",
      "      num_nodes: 1\n",
      "      accelerator: gpu\n",
      "      precision: 16\n",
      "      logger: false\n",
      "      enable_checkpointing: false\n",
      "      replace_sampler_ddp: false\n",
      "      max_epochs: 500\n",
      "      log_every_n_steps: 10\n",
      "      val_check_interval: 20\n",
      "      limit_val_batches: 1000\n",
      "      limit_test_batches: 1000\n",
      "      accumulate_grad_batches: 1\n",
      "      gradient_clip_val: 1.0\n",
      "      benchmark: false\n",
      "      max_steps: 5000000\n",
      "    exp_manager:\n",
      "      name: ${name}\n",
      "      exp_dir: ${.name}/${.wandb_logger_kwargs.name}\n",
      "      explicit_log_dir: ${.exp_dir}\n",
      "      create_wandb_logger: true\n",
      "      create_tensorboard_logger: true\n",
      "      wandb_logger_kwargs:\n",
      "        project: ${name}_${model.data.task_name}_finetuning\n",
      "        name: ${name}_${model.data.task_name}_finetuning_encoder_frozen_${model.encoder_frozen}\n",
      "        group: ${name}\n",
      "        job_type: Localhost_nodes_${trainer.num_nodes}_gpus_${trainer.devices}\n",
      "        notes: 'date: ${now:%y%m%d-%H%M%S}'\n",
      "        tags:\n",
      "        - ${name}\n",
      "        offline: true\n",
      "      resume_if_exists: false\n",
      "      resume_ignore_no_checkpoint: true\n",
      "      create_checkpoint_callback: true\n",
      "      checkpoint_callback_params:\n",
      "        monitor: val_TARGET_accuracy\n",
      "        save_top_k: 1\n",
      "        mode: max\n",
      "        always_save_nemo: false\n",
      "        filename: esm2nv--{val_loss:.2f}-{step}-{consumed_samples}\n",
      "        model_parallel_size: ${multiply:${model.tensor_model_parallel_size}, ${model.pipeline_model_parallel_size}}\n",
      "    model:\n",
      "      micro_batch_size: 8\n",
      "      tensor_model_parallel_size: 1\n",
      "      pipeline_model_parallel_size: 1\n",
      "      seq_length: 1024\n",
      "      max_position_embeddings: ${.seq_length}\n",
      "      encoder_seq_length: ${.seq_length}\n",
      "      num_layers: 33\n",
      "      hidden_size: 1280\n",
      "      ffn_hidden_size: ${multiply:${model.hidden_size}, 4}\n",
      "      num_attention_heads: 20\n",
      "      init_method_std: 0.02\n",
      "      hidden_dropout: 0\n",
      "      kv_channels: null\n",
      "      apply_query_key_layer_scaling: true\n",
      "      layernorm_epsilon: 1.0e-05\n",
      "      make_vocab_size_divisible_by: 128\n",
      "      pre_process: true\n",
      "      post_process: false\n",
      "      bert_binary_head: false\n",
      "      resume_from_checkpoint: null\n",
      "      masked_softmax_fusion: true\n",
      "      tokenizer:\n",
      "        library: huggingface\n",
      "        type: BertWordPieceLowerCase\n",
      "        model: null\n",
      "        vocab_file: null\n",
      "        merge_file: null\n",
      "        mask_id: 32\n",
      "        model_name: facebook/esm2_t33_650M_UR50D\n",
      "      native_amp_init_scale: 4294967296\n",
      "      native_amp_growth_interval: 1000\n",
      "      fp32_residual_connection: false\n",
      "      fp16_lm_cross_entropy: false\n",
      "      seed: 1234\n",
      "      use_cpu_initialization: false\n",
      "      onnx_safe: false\n",
      "      activations_checkpoint_method: null\n",
      "      activations_checkpoint_num_layers: 1\n",
      "      data:\n",
      "        ngc_registry_target: uniref50_2022_05\n",
      "        ngc_registry_version: v23.06\n",
      "        data_prefix: ''\n",
      "        num_workers: 8\n",
      "        dataloader_type: single\n",
      "        reset_position_ids: false\n",
      "        reset_attention_mask: false\n",
      "        eod_mask_loss: false\n",
      "        masked_lm_prob: 0.15\n",
      "        short_seq_prob: 0.1\n",
      "        skip_lines: 0\n",
      "        drop_last: false\n",
      "        pin_memory: false\n",
      "        index_mapping_dir: ${oc.env:BIONEMO_HOME}/data/uniref202104_esm2/\n",
      "        data_impl: csv_mmap\n",
      "        data_impl_kwargs:\n",
      "          csv_mmap:\n",
      "            header_lines: 1\n",
      "            newline_int: 10\n",
      "            workers: 10\n",
      "            sort_dataset_paths: true\n",
      "            data_sep: ','\n",
      "            data_col: 3\n",
      "        use_upsampling: true\n",
      "        seed: 1234\n",
      "        max_seq_length: ${model.seq_length}\n",
      "        dataset_path: ${model.data.preprocessed_data_path}/mixed_soft\n",
      "        dataset:\n",
      "          train: data_train_site-1\n",
      "          test: data_test_site-1\n",
      "          val: data_val_site-1\n",
      "        micro_batch_size: 32\n",
      "        global_batch_size: null\n",
      "        modify_percent: 0.1\n",
      "        perturb_percent: 0.5\n",
      "        dynamic_padding: false\n",
      "        force_regen_sample_mapping: false\n",
      "        preprocessing:\n",
      "          num_preprocess_workers: 16\n",
      "        train:\n",
      "          data_impl: csv_mmap\n",
      "          data_impl_kwargs:\n",
      "            csv_mmap:\n",
      "              data_col: 1\n",
      "              header_lines: 1\n",
      "          use_upsampling: true\n",
      "          range: x[000..049]\n",
      "          sample_from_map: true\n",
      "          uf50_datapath: ${oc.env:BIONEMO_HOME}/data/uniref202104_esm2/uniref50_train_filt.fasta\n",
      "          uf90_datapath: ${oc.env:BIONEMO_HOME}/data/uniref202104_esm2/uniref90membersandreps_ur50trainfiltreps.fasta\n",
      "          cluster_mapping_tsv: ${oc.env:BIONEMO_HOME}/data/uniref202104_esm2/uniref50_mapper_to_uniref90clustermembers_andselfuniref50.tsv\n",
      "          dataset_path: ${oc.env:BIONEMO_HOME}/data/uniref202104_esm2/uf50\n",
      "          sort_fastas: true\n",
      "          num_workers: ${model.data.num_workers}\n",
      "          uf90:\n",
      "            uniref90_path: ${oc.env:BIONEMO_HOME}/data/uniref202104_esm2/uf90/\n",
      "            dataset:\n",
      "              uf90_csvs: x[000..049]\n",
      "            data_impl: csv_fields_mmap\n",
      "            data_impl_kwargs:\n",
      "              csv_fields_mmap:\n",
      "                header_lines: 1\n",
      "                newline_int: 10\n",
      "                workers: ${model.data.train.num_workers}\n",
      "                sort_dataset_paths: true\n",
      "                data_sep: ','\n",
      "                data_fields:\n",
      "                  sequence: 3\n",
      "                  sequence_id: 1\n",
      "            index_mapping_dir: ${model.data.index_mapping_dir}\n",
      "          max_seq_length: ${model.seq_length}\n",
      "          seed: ${model.seed}\n",
      "        val:\n",
      "          use_upsampling: false\n",
      "          range: x[000..049]\n",
      "          uf50_datapath: ${oc.env:BIONEMO_HOME}/data/dataqc/uniref50_train_filt.fasta\n",
      "          dataset_path: ${oc.env:BIONEMO_HOME}/data/dataqc/uf50\n",
      "          data_impl: csv_fields_mmap\n",
      "          num_workers: ${model.data.num_workers}\n",
      "          data_impl_kwargs:\n",
      "            csv_fields_mmap:\n",
      "              header_lines: 1\n",
      "              newline_int: 10\n",
      "              workers: ${model.data.val.num_workers}\n",
      "              sort_dataset_paths: true\n",
      "              data_sep: ','\n",
      "              data_fields:\n",
      "                sequence: 3\n",
      "                sequence_id: 1\n",
      "            index_mapping_dir: ${model.data.index_mapping_dir}\n",
      "          max_seq_length: ${model.seq_length}\n",
      "          seed: ${model.seed}\n",
      "        test:\n",
      "          use_upsampling: false\n",
      "          range: x[000..049]\n",
      "          uf50_datapath: ${oc.env:BIONEMO_HOME}/data/uniref202104_esm2/uniref50_train_filt.fasta\n",
      "          dataset_path: ${oc.env:BIONEMO_HOME}/data/dataqc/uf50\n",
      "          data_impl: csv_fields_mmap\n",
      "          num_workers: ${model.data.num_workers}\n",
      "          data_impl_kwargs:\n",
      "            csv_fields_mmap:\n",
      "              header_lines: 1\n",
      "              newline_int: 10\n",
      "              workers: ${model.data.test.num_workers}\n",
      "              sort_dataset_paths: true\n",
      "              data_sep: ','\n",
      "              data_fields:\n",
      "                sequence: 3\n",
      "                sequence_id: 1\n",
      "            index_mapping_dir: ${model.data.index_mapping_dir}\n",
      "          max_seq_length: ${model.seq_length}\n",
      "          seed: ${model.seed}\n",
      "        task_name: scl\n",
      "        task_type: classification\n",
      "        preprocessed_data_path: /tmp/fasta\n",
      "        sequence_column: sequence\n",
      "        target_column:\n",
      "        - TARGET\n",
      "        target_sizes:\n",
      "        - 10\n",
      "        num_classes: 10\n",
      "        shuffle: true\n",
      "        emb_batch_size: ${model.micro_batch_size}\n",
      "      optim:\n",
      "        name: fused_adam\n",
      "        lr: 0.0004\n",
      "        weight_decay: 0.01\n",
      "        betas:\n",
      "        - 0.9\n",
      "        - 0.98\n",
      "        sched:\n",
      "          name: CosineAnnealing\n",
      "          warmup_steps: 2000\n",
      "          constant_steps: 50000\n",
      "          min_lr: 4.0e-05\n",
      "      dwnstr_task_validation:\n",
      "        enabled: false\n",
      "        dataset:\n",
      "          class: bionemo.model.core.dwnstr_task_callbacks.PerTokenPredictionCallback\n",
      "          task_type: token-level-classification\n",
      "          infer_target: bionemo.model.protein.esm1nv.infer.ESM1nvInference\n",
      "          max_seq_length: ${model.seq_length}\n",
      "          emb_batch_size: 128\n",
      "          batch_size: 128\n",
      "          num_epochs: 10\n",
      "          shuffle: true\n",
      "          num_workers: 8\n",
      "          task_name: secondary_structure\n",
      "          dataset_path: ${oc.env:BIONEMO_HOME}/data/FLIP/${model.dwnstr_task_validation.dataset.task_name}\n",
      "          dataset:\n",
      "            train: x000\n",
      "            test: x000\n",
      "          sequence_column: sequence\n",
      "          target_column:\n",
      "          - 3state\n",
      "          - resolved\n",
      "          target_sizes:\n",
      "          - 3\n",
      "          - 2\n",
      "          mask_column:\n",
      "          - resolved\n",
      "          - null\n",
      "          random_seed: 1234\n",
      "          optim:\n",
      "            name: adam\n",
      "            lr: 0.0001\n",
      "            betas:\n",
      "            - 0.9\n",
      "            - 0.999\n",
      "            eps: 1.0e-08\n",
      "            weight_decay: 0.01\n",
      "            sched:\n",
      "              name: WarmupAnnealing\n",
      "              min_lr: 1.0e-05\n",
      "              last_epoch: -1\n",
      "              warmup_ratio: 0.01\n",
      "              max_steps: 1000\n",
      "      precision: 16\n",
      "      megatron_legacy: false\n",
      "      position_embedding_type: rope\n",
      "      embedding_use_attention_mask: true\n",
      "      embedding_token_dropout: true\n",
      "      mask_token_id: ${.tokenizer.mask_id}\n",
      "      attention_dropout: 0.0\n",
      "      use_esm_attention: true\n",
      "      normalize_attention_scores: false\n",
      "      esm_gelu: true\n",
      "      bias_gelu_fusion: false\n",
      "      use_pt_layernorm: false\n",
      "      use_pt_mlp_out: false\n",
      "      target: bionemo.model.protein.esm1nv.esm1nv_model.ESM2nvModel\n",
      "      nemo_version: 1.20.0\n",
      "      encoder_frozen: false\n",
      "      global_batch_size: 16\n",
      "      loss_func: CrossEntropyLoss\n",
      "      hidden_layer_size: 256\n",
      "      finetuning_optim:\n",
      "        name: adam\n",
      "        lr: 0.0005\n",
      "        betas:\n",
      "        - 0.9\n",
      "        - 0.999\n",
      "        eps: 1.0e-08\n",
      "        weight_decay: 0.0005\n",
      "        sched:\n",
      "          name: WarmupAnnealing\n",
      "          min_lr: 1.0e-05\n",
      "          last_epoch: -1\n",
      "          warmup_steps: 10\n",
      "    target: bionemo.model.protein.esm1nv.ESM2nvModel\n",
      "    infer_target: bionemo.model.protein.esm1nv.infer.ESM1nvInference\n",
      "    \n",
      "[NeMo I 2023-12-06 18:41:15 megatron_init:234] Rank 0 has data parallel group: [0, 1]\n",
      "[NeMo I 2023-12-06 18:41:15 megatron_init:237] All data parallel group ranks: [[0, 1]]\n",
      "[NeMo I 2023-12-06 18:41:15 megatron_init:238] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2023-12-06 18:41:15 megatron_init:246] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2023-12-06 18:41:15 megatron_init:247] All model parallel group ranks: [[0], [1]]\n",
      "[NeMo I 2023-12-06 18:41:15 megatron_init:257] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2023-12-06 18:41:15 megatron_init:261] All tensor model parallel group ranks: [[0], [1]]\n",
      "[NeMo I 2023-12-06 18:41:15 megatron_init:262] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2023-12-06 18:41:15 megatron_init:276] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2023-12-06 18:41:15 megatron_init:288] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2023-12-06 18:41:15 megatron_init:294] All pipeline model parallel group ranks: [[0], [1]]\n",
      "[NeMo I 2023-12-06 18:41:15 megatron_init:295] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2023-12-06 18:41:15 megatron_init:296] All embedding group ranks: [[0], [1]]\n",
      "[NeMo I 2023-12-06 18:41:15 megatron_init:297] Rank 0 has embedding rank: 0\n",
      "23-12-06 18:41:15 - PID:262112 - rank:(0, 0, 0, 0) - microbatches.py:39 - INFO - setting number of micro-batches to constant 1\n",
      "[NeMo I 2023-12-06 18:41:15 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: facebook/esm2_t33_650M_UR50D\n",
      "Using sep_token, but it is not set yet.\n",
      "Using bos_token, but it is not set yet.\n",
      "[NeMo I 2023-12-06 18:41:15 megatron_base_model:264] Padded vocab_size: 128, original vocab_size: 33, dummy tokens: 95.\n",
      "[NeMo I 2023-12-06 18:41:17 nlp_overrides:401] Model ESM2nvModel was successfully restored from /opt/nvidia/bionemo/models/protein/esm2nv/esm2nv_650M_converted.nemo.\n",
      "[NeMo I 2023-12-06 18:41:17 utils:340] DDP is not initialized. Initializing...\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2\n",
      "Using sep_token, but it is not set yet.\n",
      "Using bos_token, but it is not set yet.\n",
      "Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2\n",
      "Added key: store_based_barrier_key:0 to store for rank: 1\n",
      "Added key: store_based_barrier_key:0 to store for rank: 0\n",
      "Rank 0: Completed store-based barrier for key:store_based_barrier_key:0 with 2 nodes.\n",
      "Rank 1: Completed store-based barrier for key:store_based_barrier_key:0 with 2 nodes.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 2 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Added key: store_based_barrier_key:1 to store for rank: 0\n",
      "Added key: store_based_barrier_key:1 to store for rank: 1\n",
      "Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.\n",
      "Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.\n",
      "Added key: store_based_barrier_key:2 to store for rank: 0\n",
      "Added key: store_based_barrier_key:2 to store for rank: 1\n",
      "Rank 1: Completed store-based barrier for key:store_based_barrier_key:2 with 2 nodes.\n",
      "Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 2 nodes.\n",
      "Added key: store_based_barrier_key:3 to store for rank: 1\n",
      "Added key: store_based_barrier_key:3 to store for rank: 0\n",
      "Rank 1: Completed store-based barrier for key:store_based_barrier_key:3 with 2 nodes.\n",
      "Rank 0: Completed store-based barrier for key:store_based_barrier_key:3 with 2 nodes.\n",
      "Added key: store_based_barrier_key:4 to store for rank: 0\n",
      "Added key: store_based_barrier_key:4 to store for rank: 1\n",
      "Rank 0: Completed store-based barrier for key:store_based_barrier_key:4 with 2 nodes.\n",
      "Rank 1: Completed store-based barrier for key:store_based_barrier_key:4 with 2 nodes.\n",
      "Added key: store_based_barrier_key:5 to store for rank: 1\n",
      "Added key: store_based_barrier_key:5 to store for rank: 0\n",
      "Rank 1: Completed store-based barrier for key:store_based_barrier_key:5 with 2 nodes.\n",
      "Rank 0: Completed store-based barrier for key:store_based_barrier_key:5 with 2 nodes.\n",
      "Added key: store_based_barrier_key:6 to store for rank: 0\n",
      "Added key: store_based_barrier_key:6 to store for rank: 1\n",
      "Rank 1: Completed store-based barrier for key:store_based_barrier_key:6 with 2 nodes.\n",
      "Rank 0: Completed store-based barrier for key:store_based_barrier_key:6 with 2 nodes.\n",
      "Added key: store_based_barrier_key:7 to store for rank: 1\n",
      "Added key: store_based_barrier_key:7 to store for rank: 0\n",
      "Rank 1: Completed store-based barrier for key:store_based_barrier_key:7 with 2 nodes.\n",
      "Rank 0: Completed store-based barrier for key:store_based_barrier_key:7 with 2 nodes.\n",
      "Added key: store_based_barrier_key:8 to store for rank: 1\n",
      "Added key: store_based_barrier_key:8 to store for rank: 0\n",
      "Rank 1: Completed store-based barrier for key:store_based_barrier_key:8 with 2 nodes.\n",
      "Rank 0: Completed store-based barrier for key:store_based_barrier_key:8 with 2 nodes.\n",
      "Added key: store_based_barrier_key:9 to store for rank: 1\n",
      "Added key: store_based_barrier_key:9 to store for rank: 0\n",
      "Rank 1: Completed store-based barrier for key:store_based_barrier_key:9 with 2 nodes.\n",
      "Rank 0: Completed store-based barrier for key:store_based_barrier_key:9 with 2 nodes.\n",
      "Added key: store_based_barrier_key:10 to store for rank: 0\n",
      "Added key: store_based_barrier_key:10 to store for rank: 1\n",
      "Rank 0: Completed store-based barrier for key:store_based_barrier_key:10 with 2 nodes.\n",
      "Rank 1: Completed store-based barrier for key:store_based_barrier_key:10 with 2 nodes.\n",
      "Added key: store_based_barrier_key:11 to store for rank: 0\n",
      "Added key: store_based_barrier_key:11 to store for rank: 1\n",
      "Rank 1: Completed store-based barrier for key:store_based_barrier_key:11 with 2 nodes.\n",
      "Rank 0: Completed store-based barrier for key:store_based_barrier_key:11 with 2 nodes.\n",
      "Added key: store_based_barrier_key:12 to store for rank: 0\n",
      "Added key: store_based_barrier_key:12 to store for rank: 1\n",
      "Rank 1: Completed store-based barrier for key:store_based_barrier_key:12 with 2 nodes.\n",
      "Rank 0: Completed store-based barrier for key:store_based_barrier_key:12 with 2 nodes.\n",
      "[NeMo W 2023-12-06 18:41:37 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/apex/transformer/pipeline_parallel/utils.py:81: UserWarning: This function is only for unittest\n",
      "      warnings.warn(\"This function is only for unittest\")\n",
      "    \n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "[NeMo W 2023-12-06 18:41:39 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:366: UserWarning: One of given dataloaders is None and it will be skipped.\n",
      "      rank_zero_warn(\"One of given dataloaders is None and it will be skipped.\")\n",
      "    \n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "--- fl_sys_info ---\n",
      "{'site_name': 'site-1', 'job_id': 'simulate_job'}\n",
      "--- validate global model ---\n",
      "--- train new model ---\n",
      "[NeMo I 2023-12-06 18:41:42 nlp_overrides:124] Configuring DDP for model parallelism.\n",
      "[NeMo I 2023-12-06 18:41:42 modelPT:721] Optimizer config = Adam (\n",
      "    Parameter Group 0\n",
      "        amsgrad: False\n",
      "        betas: [0.9, 0.999]\n",
      "        capturable: False\n",
      "        differentiable: False\n",
      "        eps: 1e-08\n",
      "        foreach: None\n",
      "        fused: None\n",
      "        lr: 0.0005\n",
      "        maximize: False\n",
      "        weight_decay: 0.0005\n",
      "    )\n",
      "[NeMo I 2023-12-06 18:41:42 lr_scheduler:910] Scheduler \"<nemo.core.optim.lr_scheduler.WarmupAnnealing object at 0x7fece1164730>\" \n",
      "    will be used during training (effective maximum steps = 5000000) - \n",
      "    Parameters : \n",
      "    (min_lr: 1.0e-05\n",
      "    last_epoch: -1\n",
      "    warmup_steps: 10\n",
      "    max_steps: 5000000\n",
      "    )\n",
      "\n",
      "   | Name                      | Type             | Params\n",
      "----------------------------------------------------------------\n",
      "0  | encoder_model             | ESM1nvInference  | 651 M \n",
      "1  | encoder_model.model       | ESM2nvModel      | 651 M \n",
      "2  | encoder_model.model.model | ESMnvBertModel   | 651 M \n",
      "3  | loss_fn                   | CrossEntropyLoss | 0     \n",
      "4  | task_head                 | MLPModel         | 333 K \n",
      "5  | task_head.linear_layers   | ModuleList       | 330 K \n",
      "6  | task_head.linear_layers.0 | Linear           | 327 K \n",
      "7  | task_head.linear_layers.1 | Linear           | 2.6 K \n",
      "8  | task_head.layer_norm      | LayerNorm        | 2.6 K \n",
      "9  | task_head.act             | ReLU             | 0     \n",
      "10 | task_head.dropout         | Dropout          | 0     \n",
      "----------------------------------------------------------------\n",
      "651 M     Trainable params\n",
      "0         Non-trainable params\n",
      "651 M     Total params\n",
      "1,302.995 Total estimated model params size (MB)\n",
      "[NeMo I 2023-12-06 18:41:42 single_value_dataset:49] Reading file /tmp/fasta/mixed_soft/train/data_train_site-1.csv...\n",
      "[NeMo I 2023-12-06 18:41:42 single_value_dataset:49] Reading file /tmp/fasta/mixed_soft/val/data_val_site-1.csv...\n",
      "[NeMo I 2023-12-06 18:41:42 single_value_dataset:49] Reading file /tmp/fasta/mixed_soft/test/data_test_site-1.csv...\n",
      "[NeMo I 2023-12-06 18:41:42 encoder_finetuning:274] Setting up train dataloader with len(len(self._train_ds)): 6918 and consumed samples: 0\n",
      "[NeMo I 2023-12-06 18:41:42 data_samplers:77] Instantiating MegatronPretrainingSampler with total_samples: 6918 and consumed_samples: 0\n",
      "[NeMo I 2023-12-06 18:41:42 encoder_finetuning:283] Setting up validation dataloader with len(len(self._validation_ds)): 1699 and consumed samples: 0\n",
      "[NeMo I 2023-12-06 18:41:42 data_samplers:77] Instantiating MegatronPretrainingSampler with total_samples: 1699 and consumed_samples: 0\n",
      "[NeMo I 2023-12-06 18:41:42 encoder_finetuning:292] Setting up test dataloader with len(len(self._test_ds)): 1699 and consumed samples: 0\n",
      "[NeMo I 2023-12-06 18:41:42 data_samplers:77] Instantiating MegatronPretrainingSampler with total_samples: 1699 and consumed_samples: 0\n",
      "Sanity Checking: 0it [00:00, ?it/s]--- fl_sys_info ---\n",
      "{'site_name': 'site-1', 'job_id': 'simulate_job'}\n",
      "--- validate global model ---\n",
      "--- train new model ---\n",
      "Sanity Checking DataLoader 0: 100%|ââââââââââ| 2/2 [00:01<00:00,  1.25it/s][NeMo W 2023-12-06 18:41:44 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:536: PossibleUserWarning: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "      warning_cache.warn(\n",
      "    \n",
      "[NeMo W 2023-12-06 18:41:44 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:536: PossibleUserWarning: It is recommended to use `self.log('val_TARGET_accuracy', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "      warning_cache.warn(\n",
      "    \n",
      "Epoch 0:   0%|          | 0/2680 [00:00<?, ?it/s]                          [NeMo W 2023-12-06 18:41:46 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:232: UserWarning: You called `self.log('global_step', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.\n",
      "      warning_cache.warn(\n",
      "    \n",
      "[NeMo W 2023-12-06 18:41:46 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:232: UserWarning: You called `self.log('consumed_samples', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.\n",
      "      warning_cache.warn(\n",
      "    \n",
      "[NeMo W 2023-12-06 18:41:47 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nemo/collections/nlp/parts/nlp_overrides.py:568: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:82.)\n",
      "      found_inf = torch.cuda.FloatTensor([sum(v.item() for v in optimizer_state[\"found_inf_per_device\"].values())])\n",
      "    \n",
      "[NeMo W 2023-12-06 18:41:47 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "      warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "    \n",
      "Epoch 0:   1%|          | 20/2680 [00:12<28:07,  1.58it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/107 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/107 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   1%|          | 21/2680 [00:13<28:29,  1.56it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   1%|          | 22/2680 [00:13<27:28,  1.61it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   1%|          | 23/2680 [00:13<26:33,  1.67it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   1%|          | 24/2680 [00:13<25:40,  1.72it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   1%|          | 25/2680 [00:14<24:52,  1.78it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   1%|          | 26/2680 [00:14<24:07,  1.83it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   1%|          | 27/2680 [00:14<23:27,  1.88it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   1%|          | 28/2680 [00:14<22:50,  1.94it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   1%|          | 29/2680 [00:14<22:15,  1.99it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   1%|          | 30/2680 [00:14<21:42,  2.04it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   1%|          | 31/2680 [00:14<21:11,  2.08it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   1%|          | 32/2680 [00:15<20:42,  2.13it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   1%|          | 33/2680 [00:15<20:16,  2.18it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   1%|â         | 34/2680 [00:15<19:51,  2.22it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   1%|â         | 35/2680 [00:15<19:28,  2.26it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   1%|â         | 36/2680 [00:15<19:04,  2.31it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   1%|â         | 37/2680 [00:15<18:43,  2.35it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   1%|â         | 38/2680 [00:15<18:24,  2.39it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   1%|â         | 39/2680 [00:16<18:04,  2.43it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   1%|â         | 40/2680 [00:16<17:45,  2.48it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   2%|â         | 41/2680 [00:16<17:28,  2.52it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   2%|â         | 42/2680 [00:16<17:11,  2.56it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   2%|â         | 43/2680 [00:16<16:55,  2.60it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   2%|â         | 44/2680 [00:16<16:40,  2.64it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   2%|â         | 45/2680 [00:16<16:25,  2.67it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   2%|â         | 46/2680 [00:16<16:11,  2.71it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   2%|â         | 47/2680 [00:17<15:57,  2.75it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   2%|â         | 48/2680 [00:17<15:45,  2.78it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   2%|â         | 49/2680 [00:17<15:32,  2.82it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   2%|â         | 50/2680 [00:17<15:21,  2.86it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   2%|â         | 51/2680 [00:17<15:09,  2.89it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   2%|â         | 52/2680 [00:17<14:59,  2.92it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   2%|â         | 53/2680 [00:17<14:49,  2.95it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   2%|â         | 54/2680 [00:18<14:38,  2.99it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   2%|â         | 55/2680 [00:18<14:29,  3.02it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   2%|â         | 56/2680 [00:18<14:19,  3.05it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   2%|â         | 57/2680 [00:18<14:10,  3.09it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   2%|â         | 58/2680 [00:18<14:00,  3.12it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   2%|â         | 59/2680 [00:18<13:51,  3.15it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   2%|â         | 60/2680 [00:18<13:43,  3.18it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   2%|â         | 61/2680 [00:18<13:34,  3.21it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   2%|â         | 62/2680 [00:19<13:26,  3.24it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   2%|â         | 63/2680 [00:19<13:19,  3.27it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   2%|â         | 64/2680 [00:19<13:12,  3.30it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   2%|â         | 65/2680 [00:19<13:05,  3.33it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   2%|â         | 66/2680 [00:19<12:58,  3.36it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   2%|â         | 67/2680 [00:19<12:51,  3.39it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   3%|â         | 68/2680 [00:19<12:45,  3.41it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   3%|â         | 69/2680 [00:20<12:38,  3.44it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   3%|â         | 70/2680 [00:20<12:32,  3.47it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   3%|â         | 71/2680 [00:20<12:26,  3.49it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   3%|â         | 72/2680 [00:20<12:21,  3.52it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   3%|â         | 73/2680 [00:20<12:16,  3.54it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   3%|â         | 74/2680 [00:20<12:10,  3.57it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   3%|â         | 75/2680 [00:20<12:05,  3.59it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   3%|â         | 76/2680 [00:21<11:59,  3.62it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   3%|â         | 77/2680 [00:21<11:54,  3.64it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   3%|â         | 78/2680 [00:21<11:49,  3.66it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   3%|â         | 79/2680 [00:21<11:45,  3.69it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   3%|â         | 80/2680 [00:21<11:41,  3.71it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   3%|â         | 81/2680 [00:21<11:36,  3.73it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   3%|â         | 82/2680 [00:21<11:32,  3.75it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   3%|â         | 83/2680 [00:21<11:28,  3.77it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   3%|â         | 84/2680 [00:22<11:24,  3.79it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   3%|â         | 85/2680 [00:22<11:19,  3.82it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   3%|â         | 86/2680 [00:22<11:16,  3.84it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   3%|â         | 87/2680 [00:22<11:12,  3.86it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   3%|â         | 88/2680 [00:22<11:08,  3.88it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   3%|â         | 89/2680 [00:22<11:05,  3.90it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   3%|â         | 90/2680 [00:22<11:01,  3.91it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   3%|â         | 91/2680 [00:23<10:57,  3.94it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   3%|â         | 92/2680 [00:23<10:54,  3.95it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   3%|â         | 93/2680 [00:23<10:51,  3.97it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   4%|â         | 94/2680 [00:23<10:47,  3.99it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   4%|â         | 95/2680 [00:23<10:44,  4.01it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   4%|â         | 96/2680 [00:23<10:40,  4.03it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   4%|â         | 97/2680 [00:23<10:37,  4.05it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   4%|â         | 98/2680 [00:24<10:34,  4.07it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   4%|â         | 99/2680 [00:24<10:31,  4.09it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   4%|â         | 100/2680 [00:24<10:28,  4.10it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   4%|â         | 101/2680 [00:24<10:25,  4.12it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   4%|â         | 102/2680 [00:24<10:22,  4.14it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   4%|â         | 103/2680 [00:24<10:19,  4.16it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   4%|â         | 104/2680 [00:24<10:16,  4.18it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   4%|â         | 105/2680 [00:25<10:13,  4.20it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   4%|â         | 106/2680 [00:25<10:10,  4.21it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   4%|â         | 107/2680 [00:25<10:08,  4.23it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   4%|â         | 108/2680 [00:25<10:05,  4.25it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   4%|â         | 109/2680 [00:25<10:02,  4.27it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   4%|â         | 110/2680 [00:25<10:00,  4.28it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   4%|â         | 111/2680 [00:25<09:57,  4.30it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   4%|â         | 112/2680 [00:25<09:55,  4.31it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   4%|â         | 113/2680 [00:26<09:52,  4.33it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   4%|â         | 114/2680 [00:26<09:50,  4.35it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   4%|â         | 115/2680 [00:26<09:47,  4.36it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   4%|â         | 116/2680 [00:26<09:45,  4.38it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   4%|â         | 117/2680 [00:26<09:43,  4.39it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   4%|â         | 118/2680 [00:26<09:40,  4.41it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   4%|â         | 119/2680 [00:26<09:38,  4.43it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   4%|â         | 120/2680 [00:27<09:36,  4.44it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   5%|â         | 121/2680 [00:27<09:34,  4.46it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   5%|â         | 122/2680 [00:27<09:32,  4.47it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   5%|â         | 123/2680 [00:27<09:30,  4.48it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   5%|â         | 124/2680 [00:27<09:27,  4.50it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   5%|â         | 125/2680 [00:27<09:25,  4.52it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "Epoch 0:   5%|â         | 126/2680 [00:27<09:24,  4.52it/s, loss=2.29, v_num=aqub, reduced_train_loss=2.200, global_step=19.00, consumed_samples=320.0]\n",
      "                                                                          \u001b[AEpoch 0, global step 20: 'val_TARGET_accuracy' reached 0.28833 (best 0.28833), saving model to '/tmp/nvflare/bionemo/local_site1_finetune_esm2nv_alpha100.0_unfreeze_encoder1_large_ds/simulate_job/app_site-1/esm2nv_flip/esm2nv_flip_scl_finetuning_encoder_frozen_False/checkpoints/esm2nv--val_loss=2.18-step=20-consumed_samples=320.0.ckpt' as top 1\n",
      "Epoch 0:   5%|â         | 146/2680 [00:56<16:12,  2.61it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/107 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/107 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   5%|â         | 147/2680 [00:56<16:16,  2.59it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   6%|â         | 148/2680 [00:56<16:12,  2.60it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   6%|â         | 149/2680 [00:56<16:07,  2.62it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   6%|â         | 150/2680 [00:57<16:02,  2.63it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   6%|â         | 151/2680 [00:57<15:58,  2.64it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   6%|â         | 152/2680 [00:57<15:53,  2.65it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   6%|â         | 153/2680 [00:57<15:49,  2.66it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   6%|â         | 154/2680 [00:57<15:45,  2.67it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   6%|â         | 155/2680 [00:57<15:41,  2.68it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   6%|â         | 156/2680 [00:57<15:36,  2.69it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   6%|â         | 157/2680 [00:58<15:32,  2.70it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   6%|â         | 158/2680 [00:58<15:28,  2.72it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   6%|â         | 159/2680 [00:58<15:24,  2.73it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   6%|â         | 160/2680 [00:58<15:21,  2.74it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   6%|â         | 161/2680 [00:58<15:17,  2.75it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   6%|â         | 162/2680 [00:58<15:13,  2.76it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   6%|â         | 163/2680 [00:58<15:09,  2.77it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   6%|â         | 164/2680 [00:59<15:05,  2.78it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   6%|â         | 165/2680 [00:59<15:02,  2.79it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   6%|â         | 166/2680 [00:59<14:58,  2.80it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   6%|â         | 167/2680 [00:59<14:54,  2.81it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   6%|â         | 168/2680 [00:59<14:50,  2.82it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   6%|â         | 169/2680 [00:59<14:47,  2.83it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   6%|â         | 170/2680 [00:59<14:43,  2.84it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   6%|â         | 171/2680 [00:59<14:40,  2.85it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   6%|â         | 172/2680 [01:00<14:36,  2.86it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   6%|â         | 173/2680 [01:00<14:32,  2.87it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   6%|â         | 174/2680 [01:00<14:29,  2.88it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   7%|â         | 175/2680 [01:00<14:26,  2.89it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   7%|â         | 176/2680 [01:00<14:22,  2.90it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   7%|â         | 177/2680 [01:00<14:19,  2.91it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   7%|â         | 178/2680 [01:00<14:16,  2.92it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   7%|â         | 179/2680 [01:01<14:13,  2.93it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   7%|â         | 180/2680 [01:01<14:10,  2.94it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   7%|â         | 181/2680 [01:01<14:07,  2.95it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   7%|â         | 182/2680 [01:01<14:03,  2.96it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   7%|â         | 183/2680 [01:01<14:00,  2.97it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   7%|â         | 184/2680 [01:01<13:57,  2.98it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   7%|â         | 185/2680 [01:01<13:54,  2.99it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   7%|â         | 186/2680 [01:01<13:51,  3.00it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   7%|â         | 187/2680 [01:02<13:48,  3.01it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   7%|â         | 188/2680 [01:02<13:45,  3.02it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   7%|â         | 189/2680 [01:02<13:42,  3.03it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   7%|â         | 190/2680 [01:02<13:39,  3.04it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   7%|â         | 191/2680 [01:02<13:36,  3.05it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   7%|â         | 192/2680 [01:02<13:33,  3.06it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   7%|â         | 193/2680 [01:02<13:30,  3.07it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   7%|â         | 194/2680 [01:03<13:28,  3.08it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   7%|â         | 195/2680 [01:03<13:25,  3.09it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   7%|â         | 196/2680 [01:03<13:22,  3.10it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   7%|â         | 197/2680 [01:03<13:19,  3.10it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   7%|â         | 198/2680 [01:03<13:17,  3.11it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   7%|â         | 199/2680 [01:03<13:14,  3.12it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   7%|â         | 200/2680 [01:03<13:12,  3.13it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   8%|â         | 201/2680 [01:04<13:09,  3.14it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   8%|â         | 202/2680 [01:04<13:06,  3.15it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   8%|â         | 203/2680 [01:04<13:04,  3.16it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   8%|â         | 204/2680 [01:04<13:01,  3.17it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   8%|â         | 205/2680 [01:04<12:59,  3.17it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   8%|â         | 206/2680 [01:04<12:57,  3.18it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   8%|â         | 207/2680 [01:04<12:54,  3.19it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   8%|â         | 208/2680 [01:04<12:52,  3.20it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   8%|â         | 209/2680 [01:05<12:50,  3.21it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   8%|â         | 210/2680 [01:05<12:47,  3.22it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   8%|â         | 211/2680 [01:05<12:45,  3.23it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   8%|â         | 212/2680 [01:05<12:43,  3.23it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   8%|â         | 213/2680 [01:05<12:40,  3.24it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   8%|â         | 214/2680 [01:05<12:38,  3.25it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   8%|â         | 215/2680 [01:05<12:36,  3.26it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   8%|â         | 216/2680 [01:06<12:34,  3.27it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   8%|â         | 217/2680 [01:06<12:31,  3.28it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   8%|â         | 218/2680 [01:06<12:29,  3.28it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   8%|â         | 219/2680 [01:06<12:27,  3.29it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   8%|â         | 220/2680 [01:06<12:25,  3.30it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   8%|â         | 221/2680 [01:06<12:23,  3.31it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   8%|â         | 222/2680 [01:06<12:21,  3.32it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   8%|â         | 223/2680 [01:07<12:18,  3.32it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   8%|â         | 224/2680 [01:07<12:16,  3.33it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   8%|â         | 225/2680 [01:07<12:14,  3.34it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   8%|â         | 226/2680 [01:07<12:12,  3.35it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   8%|â         | 227/2680 [01:07<12:10,  3.36it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   9%|â         | 228/2680 [01:07<12:08,  3.36it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   9%|â         | 229/2680 [01:07<12:06,  3.37it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   9%|â         | 230/2680 [01:08<12:04,  3.38it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   9%|â         | 231/2680 [01:08<12:02,  3.39it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   9%|â         | 232/2680 [01:08<12:00,  3.40it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   9%|â         | 233/2680 [01:08<11:58,  3.41it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   9%|â         | 234/2680 [01:08<11:56,  3.41it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   9%|â         | 235/2680 [01:08<11:54,  3.42it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   9%|â         | 236/2680 [01:08<11:52,  3.43it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   9%|â         | 237/2680 [01:08<11:50,  3.44it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   9%|â         | 238/2680 [01:09<11:48,  3.45it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   9%|â         | 239/2680 [01:09<11:46,  3.45it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   9%|â         | 240/2680 [01:09<11:44,  3.46it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   9%|â         | 241/2680 [01:09<11:43,  3.47it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   9%|â         | 242/2680 [01:09<11:41,  3.48it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   9%|â         | 243/2680 [01:09<11:39,  3.48it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   9%|â         | 244/2680 [01:09<11:37,  3.49it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   9%|â         | 245/2680 [01:09<11:35,  3.50it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   9%|â         | 246/2680 [01:10<11:33,  3.51it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   9%|â         | 247/2680 [01:10<11:32,  3.51it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   9%|â         | 248/2680 [01:10<11:30,  3.52it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   9%|â         | 249/2680 [01:10<11:28,  3.53it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   9%|â         | 250/2680 [01:10<11:26,  3.54it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   9%|â         | 251/2680 [01:10<11:24,  3.55it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "Epoch 0:   9%|â         | 252/2680 [01:10<11:23,  3.55it/s, loss=2.04, v_num=aqub, reduced_train_loss=1.760, global_step=39.00, consumed_samples=640.0]\n",
      "                                                                          \u001b[AEpoch 0, global step 40: 'val_TARGET_accuracy' reached 0.40507 (best 0.40507), saving model to '/tmp/nvflare/bionemo/local_site1_finetune_esm2nv_alpha100.0_unfreeze_encoder1_large_ds/simulate_job/app_site-1/esm2nv_flip/esm2nv_flip_scl_finetuning_encoder_frozen_False/checkpoints/esm2nv--val_loss=1.78-step=40-consumed_samples=640.0.ckpt' as top 1\n",
      "[NeMo I 2023-12-06 18:43:05 nlp_overrides:231] Removing checkpoint: /tmp/nvflare/bionemo/local_site1_finetune_esm2nv_alpha100.0_unfreeze_encoder1_large_ds/simulate_job/app_site-1/esm2nv_flip/esm2nv_flip_scl_finetuning_encoder_frozen_False/checkpoints/esm2nv--val_loss=2.18-step=20-consumed_samples=320.0.ckpt\n",
      "[NeMo I 2023-12-06 18:43:15 nlp_overrides:231] Removing checkpoint: /tmp/nvflare/bionemo/local_site1_finetune_esm2nv_alpha100.0_unfreeze_encoder1_large_ds/simulate_job/app_site-1/esm2nv_flip/esm2nv_flip_scl_finetuning_encoder_frozen_False/checkpoints/esm2nv--val_loss=2.18-step=20-consumed_samples=320.0-last.ckpt\n",
      "Epoch 0:  10%|â         | 272/2680 [01:39<14:36,  2.75it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/107 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/107 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  10%|â         | 273/2680 [01:39<14:38,  2.74it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  10%|â         | 274/2680 [01:39<14:35,  2.75it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  10%|â         | 275/2680 [01:39<14:33,  2.75it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  10%|â         | 276/2680 [01:39<14:30,  2.76it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  10%|â         | 277/2680 [01:40<14:28,  2.77it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  10%|â         | 278/2680 [01:40<14:26,  2.77it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  10%|â         | 279/2680 [01:40<14:24,  2.78it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  10%|â         | 280/2680 [01:40<14:21,  2.78it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  10%|â         | 281/2680 [01:40<14:19,  2.79it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  11%|â         | 282/2680 [01:40<14:17,  2.80it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  11%|â         | 283/2680 [01:40<14:15,  2.80it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  11%|â         | 284/2680 [01:41<14:12,  2.81it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  11%|â         | 285/2680 [01:41<14:10,  2.82it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  11%|â         | 286/2680 [01:41<14:08,  2.82it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  11%|â         | 287/2680 [01:41<14:06,  2.83it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  11%|â         | 288/2680 [01:41<14:04,  2.83it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  11%|â         | 289/2680 [01:41<14:02,  2.84it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  11%|â         | 290/2680 [01:41<14:00,  2.84it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  11%|â         | 291/2680 [01:42<13:58,  2.85it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  11%|â         | 292/2680 [01:42<13:55,  2.86it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  11%|â         | 293/2680 [01:42<13:53,  2.86it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  11%|â         | 294/2680 [01:42<13:51,  2.87it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  11%|â         | 295/2680 [01:42<13:49,  2.87it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  11%|â         | 296/2680 [01:42<13:47,  2.88it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  11%|â         | 297/2680 [01:42<13:45,  2.89it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  11%|â         | 298/2680 [01:43<13:43,  2.89it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  11%|â         | 299/2680 [01:43<13:41,  2.90it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  11%|â         | 300/2680 [01:43<13:39,  2.90it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  11%|â         | 301/2680 [01:43<13:37,  2.91it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  11%|ââ        | 302/2680 [01:43<13:35,  2.92it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  11%|ââ        | 303/2680 [01:43<13:33,  2.92it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  11%|ââ        | 304/2680 [01:43<13:31,  2.93it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  11%|ââ        | 305/2680 [01:43<13:29,  2.93it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  11%|ââ        | 306/2680 [01:44<13:27,  2.94it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  11%|ââ        | 307/2680 [01:44<13:25,  2.94it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  11%|ââ        | 308/2680 [01:44<13:23,  2.95it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  12%|ââ        | 309/2680 [01:44<13:22,  2.96it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  12%|ââ        | 310/2680 [01:44<13:20,  2.96it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  12%|ââ        | 311/2680 [01:44<13:18,  2.97it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  12%|ââ        | 312/2680 [01:44<13:16,  2.97it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  12%|ââ        | 313/2680 [01:45<13:14,  2.98it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  12%|ââ        | 314/2680 [01:45<13:12,  2.99it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  12%|ââ        | 315/2680 [01:45<13:10,  2.99it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  12%|ââ        | 316/2680 [01:45<13:08,  3.00it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  12%|ââ        | 317/2680 [01:45<13:06,  3.00it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  12%|ââ        | 318/2680 [01:45<13:05,  3.01it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  12%|ââ        | 319/2680 [01:45<13:03,  3.01it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  12%|ââ        | 320/2680 [01:45<13:01,  3.02it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  12%|ââ        | 321/2680 [01:46<12:59,  3.03it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  12%|ââ        | 322/2680 [01:46<12:58,  3.03it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  12%|ââ        | 323/2680 [01:46<12:56,  3.04it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  12%|ââ        | 324/2680 [01:46<12:54,  3.04it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  12%|ââ        | 325/2680 [01:46<12:52,  3.05it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  12%|ââ        | 326/2680 [01:46<12:51,  3.05it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  12%|ââ        | 327/2680 [01:46<12:49,  3.06it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  12%|ââ        | 328/2680 [01:47<12:47,  3.06it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  12%|ââ        | 329/2680 [01:47<12:46,  3.07it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  12%|ââ        | 330/2680 [01:47<12:44,  3.07it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  12%|ââ        | 331/2680 [01:47<12:42,  3.08it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  12%|ââ        | 332/2680 [01:47<12:41,  3.08it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  12%|ââ        | 333/2680 [01:47<12:39,  3.09it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  12%|ââ        | 334/2680 [01:47<12:37,  3.10it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  12%|ââ        | 335/2680 [01:48<12:36,  3.10it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  13%|ââ        | 336/2680 [01:48<12:34,  3.11it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  13%|ââ        | 337/2680 [01:48<12:33,  3.11it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  13%|ââ        | 338/2680 [01:48<12:31,  3.12it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  13%|ââ        | 339/2680 [01:48<12:30,  3.12it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  13%|ââ        | 340/2680 [01:48<12:28,  3.13it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  13%|ââ        | 341/2680 [01:48<12:26,  3.13it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  13%|ââ        | 342/2680 [01:49<12:25,  3.14it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  13%|ââ        | 343/2680 [01:49<12:23,  3.14it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  13%|ââ        | 344/2680 [01:49<12:22,  3.15it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  13%|ââ        | 345/2680 [01:49<12:20,  3.15it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  13%|ââ        | 346/2680 [01:49<12:19,  3.16it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  13%|ââ        | 347/2680 [01:49<12:17,  3.16it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  13%|ââ        | 348/2680 [01:49<12:16,  3.17it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  13%|ââ        | 349/2680 [01:49<12:14,  3.17it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  13%|ââ        | 350/2680 [01:50<12:13,  3.18it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  13%|ââ        | 351/2680 [01:50<12:11,  3.18it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  13%|ââ        | 352/2680 [01:50<12:10,  3.19it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  13%|ââ        | 353/2680 [01:50<12:08,  3.19it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  13%|ââ        | 354/2680 [01:50<12:07,  3.20it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  13%|ââ        | 355/2680 [01:50<12:05,  3.20it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  13%|ââ        | 356/2680 [01:50<12:04,  3.21it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  13%|ââ        | 357/2680 [01:51<12:02,  3.21it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  13%|ââ        | 358/2680 [01:51<12:01,  3.22it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  13%|ââ        | 359/2680 [01:51<11:59,  3.22it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  13%|ââ        | 360/2680 [01:51<11:58,  3.23it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  13%|ââ        | 361/2680 [01:51<11:56,  3.24it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  14%|ââ        | 362/2680 [01:51<11:55,  3.24it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  14%|ââ        | 363/2680 [01:51<11:54,  3.24it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  14%|ââ        | 364/2680 [01:52<11:52,  3.25it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  14%|ââ        | 365/2680 [01:52<11:51,  3.25it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  14%|ââ        | 366/2680 [01:52<11:49,  3.26it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  14%|ââ        | 367/2680 [01:52<11:48,  3.26it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  14%|ââ        | 368/2680 [01:52<11:47,  3.27it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  14%|ââ        | 369/2680 [01:52<11:45,  3.27it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  14%|ââ        | 370/2680 [01:52<11:44,  3.28it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  14%|ââ        | 371/2680 [01:52<11:42,  3.29it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  14%|ââ        | 372/2680 [01:53<11:41,  3.29it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  14%|ââ        | 373/2680 [01:53<11:40,  3.29it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  14%|ââ        | 374/2680 [01:53<11:38,  3.30it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  14%|ââ        | 375/2680 [01:53<11:37,  3.30it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  14%|ââ        | 376/2680 [01:53<11:35,  3.31it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  14%|ââ        | 377/2680 [01:53<11:34,  3.32it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "Epoch 0:  14%|ââ        | 378/2680 [01:53<11:33,  3.32it/s, loss=2.11, v_num=aqub, reduced_train_loss=1.940, global_step=59.00, consumed_samples=960.0]\n",
      "                                                                          \u001b[AEpoch 0, global step 60: 'val_TARGET_accuracy' was not in top 1\n",
      "[NeMo I 2023-12-06 18:43:48 nlp_overrides:231] Removing checkpoint: /tmp/nvflare/bionemo/local_site1_finetune_esm2nv_alpha100.0_unfreeze_encoder1_large_ds/simulate_job/app_site-1/esm2nv_flip/esm2nv_flip_scl_finetuning_encoder_frozen_False/checkpoints/esm2nv--val_loss=1.78-step=40-consumed_samples=640.0-last.ckpt\n",
      "Epoch 0:  15%|ââ        | 398/2680 [02:13<12:43,  2.99it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/107 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/107 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  15%|ââ        | 399/2680 [02:13<12:44,  2.98it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  15%|ââ        | 400/2680 [02:13<12:43,  2.99it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  15%|ââ        | 401/2680 [02:14<12:42,  2.99it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  15%|ââ        | 402/2680 [02:14<12:40,  3.00it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  15%|ââ        | 403/2680 [02:14<12:39,  3.00it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  15%|ââ        | 404/2680 [02:14<12:37,  3.00it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  15%|ââ        | 405/2680 [02:14<12:36,  3.01it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  15%|ââ        | 406/2680 [02:14<12:34,  3.01it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  15%|ââ        | 407/2680 [02:14<12:33,  3.02it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  15%|ââ        | 408/2680 [02:15<12:31,  3.02it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  15%|ââ        | 409/2680 [02:15<12:30,  3.03it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  15%|ââ        | 410/2680 [02:15<12:29,  3.03it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  15%|ââ        | 411/2680 [02:15<12:27,  3.03it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  15%|ââ        | 412/2680 [02:15<12:26,  3.04it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  15%|ââ        | 413/2680 [02:15<12:25,  3.04it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  15%|ââ        | 414/2680 [02:15<12:23,  3.05it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  15%|ââ        | 415/2680 [02:16<12:22,  3.05it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  16%|ââ        | 416/2680 [02:16<12:20,  3.06it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  16%|ââ        | 417/2680 [02:16<12:19,  3.06it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  16%|ââ        | 418/2680 [02:16<12:18,  3.06it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  16%|ââ        | 419/2680 [02:16<12:16,  3.07it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  16%|ââ        | 420/2680 [02:16<12:15,  3.07it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  16%|ââ        | 421/2680 [02:16<12:14,  3.08it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  16%|ââ        | 422/2680 [02:16<12:12,  3.08it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  16%|ââ        | 423/2680 [02:17<12:11,  3.09it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  16%|ââ        | 424/2680 [02:17<12:10,  3.09it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  16%|ââ        | 425/2680 [02:17<12:08,  3.09it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  16%|ââ        | 426/2680 [02:17<12:07,  3.10it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  16%|ââ        | 427/2680 [02:17<12:06,  3.10it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  16%|ââ        | 428/2680 [02:17<12:04,  3.11it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  16%|ââ        | 429/2680 [02:17<12:03,  3.11it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  16%|ââ        | 430/2680 [02:18<12:02,  3.11it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  16%|ââ        | 431/2680 [02:18<12:01,  3.12it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  16%|ââ        | 432/2680 [02:18<11:59,  3.12it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  16%|ââ        | 433/2680 [02:18<11:58,  3.13it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  16%|ââ        | 434/2680 [02:18<11:57,  3.13it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  16%|ââ        | 435/2680 [02:18<11:55,  3.14it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  16%|ââ        | 436/2680 [02:18<11:54,  3.14it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  16%|ââ        | 437/2680 [02:18<11:53,  3.14it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  16%|ââ        | 438/2680 [02:19<11:52,  3.15it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  16%|ââ        | 439/2680 [02:19<11:50,  3.15it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  16%|ââ        | 440/2680 [02:19<11:49,  3.16it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  16%|ââ        | 441/2680 [02:19<11:48,  3.16it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  16%|ââ        | 442/2680 [02:19<11:46,  3.17it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  17%|ââ        | 443/2680 [02:19<11:45,  3.17it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  17%|ââ        | 444/2680 [02:19<11:44,  3.17it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  17%|ââ        | 445/2680 [02:20<11:43,  3.18it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  17%|ââ        | 446/2680 [02:20<11:42,  3.18it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  17%|ââ        | 447/2680 [02:20<11:40,  3.19it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  17%|ââ        | 448/2680 [02:20<11:39,  3.19it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  17%|ââ        | 449/2680 [02:20<11:38,  3.19it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  17%|ââ        | 450/2680 [02:20<11:37,  3.20it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  17%|ââ        | 451/2680 [02:20<11:36,  3.20it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  17%|ââ        | 452/2680 [02:21<11:35,  3.21it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  17%|ââ        | 453/2680 [02:21<11:33,  3.21it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  17%|ââ        | 454/2680 [02:21<11:32,  3.21it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  17%|ââ        | 455/2680 [02:21<11:31,  3.22it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  17%|ââ        | 456/2680 [02:21<11:30,  3.22it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  17%|ââ        | 457/2680 [02:21<11:29,  3.23it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  17%|ââ        | 458/2680 [02:21<11:28,  3.23it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  17%|ââ        | 459/2680 [02:21<11:26,  3.23it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  17%|ââ        | 460/2680 [02:22<11:25,  3.24it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  17%|ââ        | 461/2680 [02:22<11:24,  3.24it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  17%|ââ        | 462/2680 [02:22<11:23,  3.24it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  17%|ââ        | 463/2680 [02:22<11:22,  3.25it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  17%|ââ        | 464/2680 [02:22<11:21,  3.25it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  17%|ââ        | 465/2680 [02:22<11:20,  3.26it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  17%|ââ        | 466/2680 [02:22<11:19,  3.26it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  17%|ââ        | 467/2680 [02:23<11:18,  3.26it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  17%|ââ        | 468/2680 [02:23<11:16,  3.27it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  18%|ââ        | 469/2680 [02:23<11:15,  3.27it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  18%|ââ        | 470/2680 [02:23<11:14,  3.28it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  18%|ââ        | 471/2680 [02:23<11:13,  3.28it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  18%|ââ        | 472/2680 [02:23<11:12,  3.28it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  18%|ââ        | 473/2680 [02:23<11:11,  3.29it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  18%|ââ        | 474/2680 [02:24<11:10,  3.29it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  18%|ââ        | 475/2680 [02:24<11:09,  3.29it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  18%|ââ        | 476/2680 [02:24<11:08,  3.30it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  18%|ââ        | 477/2680 [02:24<11:07,  3.30it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  18%|ââ        | 478/2680 [02:24<11:06,  3.31it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  18%|ââ        | 479/2680 [02:24<11:05,  3.31it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  18%|ââ        | 480/2680 [02:24<11:03,  3.31it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  18%|ââ        | 481/2680 [02:24<11:02,  3.32it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  18%|ââ        | 482/2680 [02:25<11:01,  3.32it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  18%|ââ        | 483/2680 [02:25<11:00,  3.33it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  18%|ââ        | 484/2680 [02:25<10:59,  3.33it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  18%|ââ        | 485/2680 [02:25<10:58,  3.33it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  18%|ââ        | 486/2680 [02:25<10:57,  3.34it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  18%|ââ        | 487/2680 [02:25<10:56,  3.34it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  18%|ââ        | 488/2680 [02:25<10:55,  3.34it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  18%|ââ        | 489/2680 [02:26<10:54,  3.35it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  18%|ââ        | 490/2680 [02:26<10:53,  3.35it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  18%|ââ        | 491/2680 [02:26<10:52,  3.36it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  18%|ââ        | 492/2680 [02:26<10:51,  3.36it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  18%|ââ        | 493/2680 [02:26<10:50,  3.36it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  18%|ââ        | 494/2680 [02:26<10:49,  3.37it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  18%|ââ        | 495/2680 [02:26<10:48,  3.37it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  19%|ââ        | 496/2680 [02:26<10:47,  3.37it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  19%|ââ        | 497/2680 [02:27<10:46,  3.38it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  19%|ââ        | 498/2680 [02:27<10:45,  3.38it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  19%|ââ        | 499/2680 [02:27<10:44,  3.39it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  19%|ââ        | 500/2680 [02:27<10:43,  3.39it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  19%|ââ        | 501/2680 [02:27<10:42,  3.39it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  19%|ââ        | 502/2680 [02:27<10:41,  3.40it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  19%|ââ        | 503/2680 [02:27<10:40,  3.40it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "Epoch 0:  19%|ââ        | 504/2680 [02:28<10:39,  3.40it/s, loss=2.07, v_num=aqub, reduced_train_loss=1.930, global_step=79.00, consumed_samples=1280.0]\n",
      "                                                                          \u001b[AEpoch 0, global step 80: 'val_TARGET_accuracy' was not in top 1\n",
      "[NeMo I 2023-12-06 18:44:22 nlp_overrides:231] Removing checkpoint: /tmp/nvflare/bionemo/local_site1_finetune_esm2nv_alpha100.0_unfreeze_encoder1_large_ds/simulate_job/app_site-1/esm2nv_flip/esm2nv_flip_scl_finetuning_encoder_frozen_False/checkpoints/esm2nv--val_loss=1.86-step=60-consumed_samples=960.0-last.ckpt\n",
      "Epoch 0:  20%|ââ        | 524/2680 [02:46<11:23,  3.15it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/107 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/107 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  20%|ââ        | 525/2680 [02:46<11:24,  3.15it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  20%|ââ        | 526/2680 [02:46<11:23,  3.15it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  20%|ââ        | 527/2680 [02:46<11:22,  3.16it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  20%|ââ        | 528/2680 [02:47<11:21,  3.16it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  20%|ââ        | 529/2680 [02:47<11:20,  3.16it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  20%|ââ        | 530/2680 [02:47<11:19,  3.17it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  20%|ââ        | 531/2680 [02:47<11:17,  3.17it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  20%|ââ        | 532/2680 [02:47<11:16,  3.17it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  20%|ââ        | 533/2680 [02:47<11:15,  3.18it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  20%|ââ        | 534/2680 [02:47<11:14,  3.18it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  20%|ââ        | 535/2680 [02:48<11:13,  3.18it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  20%|ââ        | 536/2680 [02:48<11:12,  3.19it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  20%|ââ        | 537/2680 [02:48<11:11,  3.19it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  20%|ââ        | 538/2680 [02:48<11:10,  3.19it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  20%|ââ        | 539/2680 [02:48<11:09,  3.20it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  20%|ââ        | 540/2680 [02:48<11:08,  3.20it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  20%|ââ        | 541/2680 [02:48<11:07,  3.20it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  20%|ââ        | 542/2680 [02:49<11:06,  3.21it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  20%|ââ        | 543/2680 [02:49<11:05,  3.21it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  20%|ââ        | 544/2680 [02:49<11:04,  3.21it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  20%|ââ        | 545/2680 [02:49<11:03,  3.22it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  20%|ââ        | 546/2680 [02:49<11:02,  3.22it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  20%|ââ        | 547/2680 [02:49<11:01,  3.22it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  20%|ââ        | 548/2680 [02:49<11:00,  3.23it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  20%|ââ        | 549/2680 [02:50<10:59,  3.23it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  21%|ââ        | 550/2680 [02:50<10:58,  3.23it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  21%|ââ        | 551/2680 [02:50<10:57,  3.24it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  21%|ââ        | 552/2680 [02:50<10:56,  3.24it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  21%|ââ        | 553/2680 [02:50<10:55,  3.24it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  21%|ââ        | 554/2680 [02:50<10:54,  3.25it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  21%|ââ        | 555/2680 [02:50<10:54,  3.25it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  21%|ââ        | 556/2680 [02:50<10:53,  3.25it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  21%|ââ        | 557/2680 [02:51<10:52,  3.26it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  21%|ââ        | 558/2680 [02:51<10:51,  3.26it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  21%|ââ        | 559/2680 [02:51<10:50,  3.26it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  21%|ââ        | 560/2680 [02:51<10:49,  3.27it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  21%|ââ        | 561/2680 [02:51<10:48,  3.27it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  21%|ââ        | 562/2680 [02:51<10:47,  3.27it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  21%|ââ        | 563/2680 [02:51<10:46,  3.28it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  21%|ââ        | 564/2680 [02:52<10:45,  3.28it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  21%|ââ        | 565/2680 [02:52<10:44,  3.28it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  21%|ââ        | 566/2680 [02:52<10:43,  3.29it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  21%|ââ        | 567/2680 [02:52<10:42,  3.29it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  21%|ââ        | 568/2680 [02:52<10:41,  3.29it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  21%|ââ        | 569/2680 [02:52<10:40,  3.30it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  21%|âââ       | 570/2680 [02:52<10:39,  3.30it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  21%|âââ       | 571/2680 [02:52<10:38,  3.30it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  21%|âââ       | 572/2680 [02:53<10:37,  3.30it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  21%|âââ       | 573/2680 [02:53<10:36,  3.31it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  21%|âââ       | 574/2680 [02:53<10:35,  3.31it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  21%|âââ       | 575/2680 [02:53<10:35,  3.31it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  21%|âââ       | 576/2680 [02:53<10:34,  3.32it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  22%|âââ       | 577/2680 [02:53<10:33,  3.32it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  22%|âââ       | 578/2680 [02:53<10:32,  3.32it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  22%|âââ       | 579/2680 [02:54<10:31,  3.33it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  22%|âââ       | 580/2680 [02:54<10:30,  3.33it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  22%|âââ       | 581/2680 [02:54<10:29,  3.33it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  22%|âââ       | 582/2680 [02:54<10:28,  3.34it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  22%|âââ       | 583/2680 [02:54<10:27,  3.34it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  22%|âââ       | 584/2680 [02:54<10:27,  3.34it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  22%|âââ       | 585/2680 [02:54<10:26,  3.35it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  22%|âââ       | 586/2680 [02:55<10:25,  3.35it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  22%|âââ       | 587/2680 [02:55<10:24,  3.35it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  22%|âââ       | 588/2680 [02:55<10:23,  3.35it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  22%|âââ       | 589/2680 [02:55<10:22,  3.36it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  22%|âââ       | 590/2680 [02:55<10:21,  3.36it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  22%|âââ       | 591/2680 [02:55<10:21,  3.36it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  22%|âââ       | 592/2680 [02:55<10:20,  3.37it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  22%|âââ       | 593/2680 [02:55<10:19,  3.37it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  22%|âââ       | 594/2680 [02:56<10:18,  3.37it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  22%|âââ       | 595/2680 [02:56<10:17,  3.38it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  22%|âââ       | 596/2680 [02:56<10:16,  3.38it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  22%|âââ       | 597/2680 [02:56<10:15,  3.38it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  22%|âââ       | 598/2680 [02:56<10:15,  3.38it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  22%|âââ       | 599/2680 [02:56<10:14,  3.39it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  22%|âââ       | 600/2680 [02:56<10:13,  3.39it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  22%|âââ       | 601/2680 [02:57<10:12,  3.39it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  22%|âââ       | 602/2680 [02:57<10:11,  3.40it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  22%|âââ       | 603/2680 [02:57<10:10,  3.40it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  23%|âââ       | 604/2680 [02:57<10:10,  3.40it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  23%|âââ       | 605/2680 [02:57<10:09,  3.41it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  23%|âââ       | 606/2680 [02:57<10:08,  3.41it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  23%|âââ       | 607/2680 [02:57<10:07,  3.41it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  23%|âââ       | 608/2680 [02:58<10:06,  3.42it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  23%|âââ       | 609/2680 [02:58<10:05,  3.42it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  23%|âââ       | 610/2680 [02:58<10:04,  3.42it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  23%|âââ       | 611/2680 [02:58<10:04,  3.42it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  23%|âââ       | 612/2680 [02:58<10:03,  3.43it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  23%|âââ       | 613/2680 [02:58<10:02,  3.43it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  23%|âââ       | 614/2680 [02:58<10:01,  3.43it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  23%|âââ       | 615/2680 [02:58<10:00,  3.44it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  23%|âââ       | 616/2680 [02:59<10:00,  3.44it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  23%|âââ       | 617/2680 [02:59<09:59,  3.44it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  23%|âââ       | 618/2680 [02:59<09:58,  3.45it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  23%|âââ       | 619/2680 [02:59<09:57,  3.45it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  23%|âââ       | 620/2680 [02:59<09:56,  3.45it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  23%|âââ       | 621/2680 [02:59<09:55,  3.45it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  23%|âââ       | 622/2680 [02:59<09:55,  3.46it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  23%|âââ       | 623/2680 [02:59<09:54,  3.46it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  23%|âââ       | 624/2680 [03:00<09:53,  3.46it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  23%|âââ       | 625/2680 [03:00<09:52,  3.47it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  23%|âââ       | 626/2680 [03:00<09:51,  3.47it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  23%|âââ       | 627/2680 [03:00<09:51,  3.47it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  23%|âââ       | 628/2680 [03:00<09:50,  3.48it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  23%|âââ       | 629/2680 [03:00<09:49,  3.48it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "Epoch 0:  24%|âââ       | 630/2680 [03:00<09:48,  3.48it/s, loss=2.14, v_num=aqub, reduced_train_loss=2.090, global_step=99.00, consumed_samples=1600.0]\n",
      "                                                                          \u001b[AEpoch 0, global step 100: 'val_TARGET_accuracy' was not in top 1\n",
      "[NeMo I 2023-12-06 18:44:55 nlp_overrides:231] Removing checkpoint: /tmp/nvflare/bionemo/local_site1_finetune_esm2nv_alpha100.0_unfreeze_encoder1_large_ds/simulate_job/app_site-1/esm2nv_flip/esm2nv_flip_scl_finetuning_encoder_frozen_False/checkpoints/esm2nv--val_loss=2.03-step=80-consumed_samples=1280.0-last.ckpt\n",
      "Epoch 0:  24%|âââ       | 650/2680 [03:19<10:21,  3.26it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/107 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/107 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  24%|âââ       | 651/2680 [03:19<10:22,  3.26it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  24%|âââ       | 652/2680 [03:19<10:21,  3.26it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  24%|âââ       | 653/2680 [03:19<10:20,  3.27it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  24%|âââ       | 654/2680 [03:20<10:19,  3.27it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  24%|âââ       | 655/2680 [03:20<10:19,  3.27it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  24%|âââ       | 656/2680 [03:20<10:18,  3.27it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  25%|âââ       | 657/2680 [03:20<10:17,  3.28it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  25%|âââ       | 658/2680 [03:20<10:16,  3.28it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  25%|âââ       | 659/2680 [03:20<10:15,  3.28it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  25%|âââ       | 660/2680 [03:20<10:14,  3.29it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  25%|âââ       | 661/2680 [03:21<10:14,  3.29it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  25%|âââ       | 662/2680 [03:21<10:13,  3.29it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  25%|âââ       | 663/2680 [03:21<10:12,  3.29it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  25%|âââ       | 664/2680 [03:21<10:11,  3.30it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  25%|âââ       | 665/2680 [03:21<10:10,  3.30it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  25%|âââ       | 666/2680 [03:21<10:10,  3.30it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  25%|âââ       | 667/2680 [03:21<10:09,  3.30it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  25%|âââ       | 668/2680 [03:22<10:08,  3.31it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  25%|âââ       | 669/2680 [03:22<10:07,  3.31it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  25%|âââ       | 670/2680 [03:22<10:06,  3.31it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  25%|âââ       | 671/2680 [03:22<10:06,  3.31it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  25%|âââ       | 672/2680 [03:22<10:05,  3.32it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  25%|âââ       | 673/2680 [03:22<10:04,  3.32it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  25%|âââ       | 674/2680 [03:22<10:03,  3.32it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  25%|âââ       | 675/2680 [03:22<10:02,  3.33it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  25%|âââ       | 676/2680 [03:23<10:02,  3.33it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  25%|âââ       | 677/2680 [03:23<10:01,  3.33it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  25%|âââ       | 678/2680 [03:23<10:00,  3.33it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  25%|âââ       | 679/2680 [03:23<09:59,  3.34it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  25%|âââ       | 680/2680 [03:23<09:58,  3.34it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  25%|âââ       | 681/2680 [03:23<09:58,  3.34it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  25%|âââ       | 682/2680 [03:23<09:57,  3.34it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  25%|âââ       | 683/2680 [03:24<09:56,  3.35it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  26%|âââ       | 684/2680 [03:24<09:55,  3.35it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  26%|âââ       | 685/2680 [03:24<09:55,  3.35it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  26%|âââ       | 686/2680 [03:24<09:54,  3.36it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  26%|âââ       | 687/2680 [03:24<09:53,  3.36it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  26%|âââ       | 688/2680 [03:24<09:52,  3.36it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  26%|âââ       | 689/2680 [03:24<09:51,  3.36it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  26%|âââ       | 690/2680 [03:24<09:51,  3.37it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  26%|âââ       | 691/2680 [03:25<09:50,  3.37it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  26%|âââ       | 692/2680 [03:25<09:49,  3.37it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  26%|âââ       | 693/2680 [03:25<09:48,  3.37it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  26%|âââ       | 694/2680 [03:25<09:48,  3.38it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  26%|âââ       | 695/2680 [03:25<09:47,  3.38it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  26%|âââ       | 696/2680 [03:25<09:46,  3.38it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  26%|âââ       | 697/2680 [03:25<09:45,  3.39it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  26%|âââ       | 698/2680 [03:26<09:45,  3.39it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  26%|âââ       | 699/2680 [03:26<09:44,  3.39it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  26%|âââ       | 700/2680 [03:26<09:43,  3.39it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  26%|âââ       | 701/2680 [03:26<09:42,  3.40it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  26%|âââ       | 702/2680 [03:26<09:42,  3.40it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  26%|âââ       | 703/2680 [03:26<09:41,  3.40it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  26%|âââ       | 704/2680 [03:26<09:40,  3.40it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  26%|âââ       | 705/2680 [03:27<09:39,  3.41it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  26%|âââ       | 706/2680 [03:27<09:39,  3.41it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  26%|âââ       | 707/2680 [03:27<09:38,  3.41it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  26%|âââ       | 708/2680 [03:27<09:37,  3.41it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  26%|âââ       | 709/2680 [03:27<09:36,  3.42it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  26%|âââ       | 710/2680 [03:27<09:36,  3.42it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  27%|âââ       | 711/2680 [03:27<09:35,  3.42it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  27%|âââ       | 712/2680 [03:27<09:34,  3.42it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  27%|âââ       | 713/2680 [03:28<09:34,  3.43it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  27%|âââ       | 714/2680 [03:28<09:33,  3.43it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  27%|âââ       | 715/2680 [03:28<09:32,  3.43it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  27%|âââ       | 716/2680 [03:28<09:32,  3.43it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  27%|âââ       | 717/2680 [03:28<09:31,  3.44it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  27%|âââ       | 718/2680 [03:28<09:30,  3.44it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  27%|âââ       | 719/2680 [03:28<09:29,  3.44it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  27%|âââ       | 720/2680 [03:29<09:29,  3.44it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  27%|âââ       | 721/2680 [03:29<09:28,  3.45it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  27%|âââ       | 722/2680 [03:29<09:27,  3.45it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  27%|âââ       | 723/2680 [03:29<09:27,  3.45it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  27%|âââ       | 724/2680 [03:29<09:26,  3.45it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  27%|âââ       | 725/2680 [03:29<09:25,  3.46it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  27%|âââ       | 726/2680 [03:29<09:24,  3.46it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  27%|âââ       | 727/2680 [03:30<09:24,  3.46it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  27%|âââ       | 728/2680 [03:30<09:23,  3.46it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  27%|âââ       | 729/2680 [03:30<09:22,  3.47it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  27%|âââ       | 730/2680 [03:30<09:22,  3.47it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  27%|âââ       | 731/2680 [03:30<09:21,  3.47it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  27%|âââ       | 732/2680 [03:30<09:20,  3.47it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  27%|âââ       | 733/2680 [03:30<09:20,  3.48it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  27%|âââ       | 734/2680 [03:30<09:19,  3.48it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  27%|âââ       | 735/2680 [03:31<09:18,  3.48it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  27%|âââ       | 736/2680 [03:31<09:17,  3.48it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  28%|âââ       | 737/2680 [03:31<09:17,  3.49it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  28%|âââ       | 738/2680 [03:31<09:16,  3.49it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  28%|âââ       | 739/2680 [03:31<09:15,  3.49it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  28%|âââ       | 740/2680 [03:31<09:15,  3.49it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  28%|âââ       | 741/2680 [03:31<09:14,  3.50it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  28%|âââ       | 742/2680 [03:32<09:13,  3.50it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  28%|âââ       | 743/2680 [03:32<09:13,  3.50it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  28%|âââ       | 744/2680 [03:32<09:12,  3.50it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  28%|âââ       | 745/2680 [03:32<09:11,  3.51it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  28%|âââ       | 746/2680 [03:32<09:11,  3.51it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  28%|âââ       | 747/2680 [03:32<09:10,  3.51it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  28%|âââ       | 748/2680 [03:32<09:09,  3.51it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  28%|âââ       | 749/2680 [03:32<09:09,  3.52it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  28%|âââ       | 750/2680 [03:33<09:08,  3.52it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  28%|âââ       | 751/2680 [03:33<09:07,  3.52it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  28%|âââ       | 752/2680 [03:33<09:07,  3.52it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  28%|âââ       | 753/2680 [03:33<09:06,  3.53it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  28%|âââ       | 754/2680 [03:33<09:05,  3.53it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  28%|âââ       | 755/2680 [03:33<09:04,  3.53it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "Epoch 0:  28%|âââ       | 756/2680 [03:33<09:04,  3.53it/s, loss=1.96, v_num=aqub, reduced_train_loss=2.190, global_step=119.0, consumed_samples=1920.0]\n",
      "                                                                          \u001b[AEpoch 0, global step 120: 'val_TARGET_accuracy' was not in top 1\n",
      "[NeMo I 2023-12-06 18:45:28 nlp_overrides:231] Removing checkpoint: /tmp/nvflare/bionemo/local_site1_finetune_esm2nv_alpha100.0_unfreeze_encoder1_large_ds/simulate_job/app_site-1/esm2nv_flip/esm2nv_flip_scl_finetuning_encoder_frozen_False/checkpoints/esm2nv--val_loss=2.07-step=100-consumed_samples=1600.0-last.ckpt\n",
      "Epoch 0:  29%|âââ       | 776/2680 [03:52<09:30,  3.34it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/107 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/107 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  29%|âââ       | 777/2680 [03:53<09:31,  3.33it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  29%|âââ       | 778/2680 [03:53<09:30,  3.33it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  29%|âââ       | 779/2680 [03:53<09:29,  3.34it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  29%|âââ       | 780/2680 [03:53<09:29,  3.34it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  29%|âââ       | 781/2680 [03:53<09:28,  3.34it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  29%|âââ       | 782/2680 [03:53<09:27,  3.34it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  29%|âââ       | 783/2680 [03:54<09:27,  3.35it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  29%|âââ       | 784/2680 [03:54<09:26,  3.35it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  29%|âââ       | 785/2680 [03:54<09:25,  3.35it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  29%|âââ       | 786/2680 [03:54<09:24,  3.35it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  29%|âââ       | 787/2680 [03:54<09:24,  3.35it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  29%|âââ       | 788/2680 [03:54<09:23,  3.36it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  29%|âââ       | 789/2680 [03:54<09:22,  3.36it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  29%|âââ       | 790/2680 [03:55<09:22,  3.36it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  30%|âââ       | 791/2680 [03:55<09:21,  3.36it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  30%|âââ       | 792/2680 [03:55<09:20,  3.37it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  30%|âââ       | 793/2680 [03:55<09:20,  3.37it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  30%|âââ       | 794/2680 [03:55<09:19,  3.37it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  30%|âââ       | 795/2680 [03:55<09:18,  3.37it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  30%|âââ       | 796/2680 [03:55<09:18,  3.38it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  30%|âââ       | 797/2680 [03:55<09:17,  3.38it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  30%|âââ       | 798/2680 [03:56<09:16,  3.38it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  30%|âââ       | 799/2680 [03:56<09:16,  3.38it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  30%|âââ       | 800/2680 [03:56<09:15,  3.38it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  30%|âââ       | 801/2680 [03:56<09:14,  3.39it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  30%|âââ       | 802/2680 [03:56<09:14,  3.39it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  30%|âââ       | 803/2680 [03:56<09:13,  3.39it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  30%|âââ       | 804/2680 [03:56<09:12,  3.39it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  30%|âââ       | 805/2680 [03:57<09:12,  3.40it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  30%|âââ       | 806/2680 [03:57<09:11,  3.40it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  30%|âââ       | 807/2680 [03:57<09:10,  3.40it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  30%|âââ       | 808/2680 [03:57<09:10,  3.40it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  30%|âââ       | 809/2680 [03:57<09:09,  3.40it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  30%|âââ       | 810/2680 [03:57<09:08,  3.41it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  30%|âââ       | 811/2680 [03:57<09:08,  3.41it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  30%|âââ       | 812/2680 [03:58<09:07,  3.41it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  30%|âââ       | 813/2680 [03:58<09:06,  3.41it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  30%|âââ       | 814/2680 [03:58<09:06,  3.42it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  30%|âââ       | 815/2680 [03:58<09:05,  3.42it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  30%|âââ       | 816/2680 [03:58<09:04,  3.42it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  30%|âââ       | 817/2680 [03:58<09:04,  3.42it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  31%|âââ       | 818/2680 [03:58<09:03,  3.43it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  31%|âââ       | 819/2680 [03:58<09:02,  3.43it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  31%|âââ       | 820/2680 [03:59<09:02,  3.43it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  31%|âââ       | 821/2680 [03:59<09:01,  3.43it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  31%|âââ       | 822/2680 [03:59<09:00,  3.43it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  31%|âââ       | 823/2680 [03:59<09:00,  3.44it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  31%|âââ       | 824/2680 [03:59<08:59,  3.44it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  31%|âââ       | 825/2680 [03:59<08:59,  3.44it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  31%|âââ       | 826/2680 [03:59<08:58,  3.44it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  31%|âââ       | 827/2680 [03:59<08:57,  3.45it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  31%|âââ       | 828/2680 [04:00<08:57,  3.45it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  31%|âââ       | 829/2680 [04:00<08:56,  3.45it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  31%|âââ       | 830/2680 [04:00<08:55,  3.45it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  31%|âââ       | 831/2680 [04:00<08:55,  3.45it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  31%|âââ       | 832/2680 [04:00<08:54,  3.46it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  31%|âââ       | 833/2680 [04:00<08:53,  3.46it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  31%|âââ       | 834/2680 [04:00<08:53,  3.46it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  31%|âââ       | 835/2680 [04:01<08:52,  3.46it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  31%|âââ       | 836/2680 [04:01<08:52,  3.47it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  31%|âââ       | 837/2680 [04:01<08:51,  3.47it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  31%|ââââ      | 838/2680 [04:01<08:50,  3.47it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  31%|ââââ      | 839/2680 [04:01<08:50,  3.47it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  31%|ââââ      | 840/2680 [04:01<08:49,  3.47it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  31%|ââââ      | 841/2680 [04:01<08:49,  3.48it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  31%|ââââ      | 842/2680 [04:02<08:48,  3.48it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  31%|ââââ      | 843/2680 [04:02<08:47,  3.48it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  31%|ââââ      | 844/2680 [04:02<08:47,  3.48it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  32%|ââââ      | 845/2680 [04:02<08:46,  3.48it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  32%|ââââ      | 846/2680 [04:02<08:45,  3.49it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  32%|ââââ      | 847/2680 [04:02<08:45,  3.49it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  32%|ââââ      | 848/2680 [04:02<08:44,  3.49it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  32%|ââââ      | 849/2680 [04:03<08:44,  3.49it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  32%|ââââ      | 850/2680 [04:03<08:43,  3.50it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  32%|ââââ      | 851/2680 [04:03<08:42,  3.50it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  32%|ââââ      | 852/2680 [04:03<08:42,  3.50it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  32%|ââââ      | 853/2680 [04:03<08:41,  3.50it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  32%|ââââ      | 854/2680 [04:03<08:41,  3.50it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  32%|ââââ      | 855/2680 [04:03<08:40,  3.51it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  32%|ââââ      | 856/2680 [04:03<08:39,  3.51it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  32%|ââââ      | 857/2680 [04:04<08:39,  3.51it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  32%|ââââ      | 858/2680 [04:04<08:38,  3.51it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  32%|ââââ      | 859/2680 [04:04<08:38,  3.51it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  32%|ââââ      | 860/2680 [04:04<08:37,  3.52it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  32%|ââââ      | 861/2680 [04:04<08:36,  3.52it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  32%|ââââ      | 862/2680 [04:04<08:36,  3.52it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  32%|ââââ      | 863/2680 [04:04<08:35,  3.52it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  32%|ââââ      | 864/2680 [04:05<08:35,  3.53it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  32%|ââââ      | 865/2680 [04:05<08:34,  3.53it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  32%|ââââ      | 866/2680 [04:05<08:33,  3.53it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  32%|ââââ      | 867/2680 [04:05<08:33,  3.53it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  32%|ââââ      | 868/2680 [04:05<08:32,  3.53it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  32%|ââââ      | 869/2680 [04:05<08:32,  3.54it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  32%|ââââ      | 870/2680 [04:05<08:31,  3.54it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  32%|ââââ      | 871/2680 [04:05<08:30,  3.54it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  33%|ââââ      | 872/2680 [04:06<08:30,  3.54it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  33%|ââââ      | 873/2680 [04:06<08:29,  3.55it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  33%|ââââ      | 874/2680 [04:06<08:29,  3.55it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  33%|ââââ      | 875/2680 [04:06<08:28,  3.55it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  33%|ââââ      | 876/2680 [04:06<08:27,  3.55it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  33%|ââââ      | 877/2680 [04:06<08:27,  3.55it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  33%|ââââ      | 878/2680 [04:06<08:26,  3.56it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  33%|ââââ      | 879/2680 [04:07<08:26,  3.56it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  33%|ââââ      | 880/2680 [04:07<08:25,  3.56it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  33%|ââââ      | 881/2680 [04:07<08:24,  3.56it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "Epoch 0:  33%|ââââ      | 882/2680 [04:07<08:24,  3.56it/s, loss=1.97, v_num=aqub, reduced_train_loss=2.030, global_step=139.0, consumed_samples=2240.0]\n",
      "                                                                          \u001b[AEpoch 0, global step 140: 'val_TARGET_accuracy' was not in top 1\n",
      "[NeMo I 2023-12-06 18:46:01 nlp_overrides:231] Removing checkpoint: /tmp/nvflare/bionemo/local_site1_finetune_esm2nv_alpha100.0_unfreeze_encoder1_large_ds/simulate_job/app_site-1/esm2nv_flip/esm2nv_flip_scl_finetuning_encoder_frozen_False/checkpoints/esm2nv--val_loss=2.03-step=120-consumed_samples=1920.0-last.ckpt\n",
      "Epoch 0:  34%|ââââ      | 902/2680 [04:25<08:43,  3.40it/s, loss=2.02, v_num=aqub, reduced_train_loss=1.880, global_step=159.0, consumed_samples=2560.0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/107 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/107 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  34%|ââââ      | 903/2680 [04:26<08:43,  3.39it/s, loss=2.02, v_num=aqub, reduced_train_loss=1.880, global_step=159.0, consumed_samples=2560.0]\n",
      "Epoch 0:  34%|ââââ      | 904/2680 [04:26<08:43,  3.39it/s, loss=2.02, v_num=aqub, reduced_train_loss=1.880, global_step=159.0, consumed_samples=2560.0]\n",
      "Epoch 0:  34%|ââââ      | 905/2680 [04:26<08:42,  3.40it/s, loss=2.02, v_num=aqub, reduced_train_loss=1.880, global_step=159.0, consumed_samples=2560.0]\n",
      "Epoch 0:  34%|ââââ      | 906/2680 [04:26<08:42,  3.40it/s, loss=2.02, v_num=aqub, reduced_train_loss=1.880, global_step=159.0, consumed_samples=2560.0]\n",
      "Epoch 0:  34%|ââââ      | 907/2680 [04:26<08:41,  3.40it/s, loss=2.02, v_num=aqub, reduced_train_loss=1.880, global_step=159.0, consumed_samples=2560.0]\n",
      "Epoch 0:  34%|ââââ      | 908/2680 [04:26<08:40,  3.40it/s, loss=2.02, v_num=aqub, reduced_train_loss=1.880, global_step=159.0, consumed_samples=2560.0]\n",
      "Epoch 0:  34%|ââââ      | 909/2680 [04:27<08:40,  3.40it/s, loss=2.02, v_num=aqub, reduced_train_loss=1.880, global_step=159.0, consumed_samples=2560.0]\n",
      "Epoch 0:  34%|ââââ      | 910/2680 [04:27<08:39,  3.41it/s, loss=2.02, v_num=aqub, reduced_train_loss=1.880, global_step=159.0, consumed_samples=2560.0]\n",
      "Epoch 0:  34%|ââââ      | 911/2680 [04:27<08:39,  3.41it/s, loss=2.02, v_num=aqub, reduced_train_loss=1.880, global_step=159.0, consumed_samples=2560.0]\n",
      "Epoch 0:  34%|ââââ      | 912/2680 [04:27<08:38,  3.41it/s, loss=2.02, v_num=aqub, reduced_train_loss=1.880, global_step=159.0, consumed_samples=2560.0]\n",
      "Epoch 0:  34%|ââââ      | 913/2680 [04:27<08:37,  3.41it/s, loss=2.02, v_num=aqub, reduced_train_loss=1.880, global_step=159.0, consumed_samples=2560.0]\n",
      "Epoch 0:  34%|ââââ      | 914/2680 [04:27<08:37,  3.41it/s, loss=2.02, v_num=aqub, reduced_train_loss=1.880, global_step=159.0, consumed_samples=2560.0]\n",
      "Epoch 0:  34%|ââââ      | 915/2680 [04:27<08:36,  3.42it/s, loss=2.02, v_num=aqub, reduced_train_loss=1.880, global_step=159.0, consumed_samples=2560.0]\n",
      "Epoch 0:  34%|ââââ      | 916/2680 [04:28<08:36,  3.42it/s, loss=2.02, v_num=aqub, reduced_train_loss=1.880, global_step=159.0, consumed_samples=2560.0]\n",
      "Epoch 0:  34%|ââââ      | 917/2680 [04:28<08:35,  3.42it/s, loss=2.02, v_num=aqub, reduced_train_loss=1.880, global_step=159.0, consumed_samples=2560.0]\n",
      "Epoch 0:  34%|ââââ      | 918/2680 [04:28<08:35,  3.42it/s, loss=2.02, v_num=aqub, reduced_train_loss=1.880, global_step=159.0, consumed_samples=2560.0]\n",
      "Epoch 0:  34%|ââââ      | 919/2680 [04:28<08:34,  3.42it/s, loss=2.02, v_num=aqub, reduced_train_loss=1.880, global_step=159.0, consumed_samples=2560.0]\n",
      "Epoch 0:  34%|ââââ      | 920/2680 [04:28<08:33,  3.42it/s, loss=2.02, v_num=aqub, reduced_train_loss=1.880, global_step=159.0, consumed_samples=2560.0]\n",
      "Epoch 0:  34%|ââââ      | 921/2680 [04:28<08:33,  3.43it/s, loss=2.02, v_num=aqub, reduced_train_loss=1.880, global_step=159.0, consumed_samples=2560.0]\n",
      "Epoch 0:  34%|ââââ      | 922/2680 [04:28<08:32,  3.43it/s, loss=2.02, v_num=aqub, reduced_train_loss=1.880, global_step=159.0, consumed_samples=2560.0]"
     ]
    }
   ],
   "source": [
    "# DEBUG\n",
    "import os\n",
    "os.environ[\"SIM_LOCAL\"] = \"False\"\n",
    "from nvflare import SimulatorRunner    \n",
    "n_clients = 1\n",
    "split_alpha = 100.0\n",
    "\n",
    "simulator = SimulatorRunner(\n",
    "    #job_folder=\"jobs/fedavg_finetune_esm1nv\",\n",
    "    #workspace=f\"/tmp/nvflare/bionemo/fedavg_finetune_esm1nv_alpha{split_alpha}\",\n",
    "    #workspace=f\"/tmp/nvflare/bionemo/local_site1_finetune_esm1nv_alpha{split_alpha}_unfreeze_encoder4\",\n",
    "    job_folder=\"jobs/fedavg_finetune_esm2nv\",\n",
    "    #workspace=f\"/tmp/nvflare/bionemo/local_site1_finetune_esm2nv_alpha{split_alpha}_freeze_encoder2\",\n",
    "    workspace=f\"/tmp/nvflare/bionemo/local_site1_finetune_esm2nv_alpha{split_alpha}_unfreeze_encoder1_large_ds\",\n",
    "    n_clients=n_clients,\n",
    "    threads=n_clients\n",
    ")\n",
    "run_status = simulator.run()\n",
    "print(\"Simulator finished with run_status\", run_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ff0f8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
