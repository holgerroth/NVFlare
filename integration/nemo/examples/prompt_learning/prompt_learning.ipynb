{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24f538a7",
   "metadata": {},
   "source": [
    "# Prompt Learning with NeMo\n",
    "\n",
    "In this example, we utilize NeMo's [prompt learning](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/nlp/nemo_megatron/prompt_learning.html)\n",
    "feature to showcase how to adapt a large language model (LLM) to \n",
    "a downstream task, such as financial sentiment predictions. \n",
    "\n",
    "The prompt learning technique shown in the example is [p-tuning](https://arxiv.org/abs/2103.10385), which adds a small prompt encoder network to the LLM\n",
    "to produce virtual token embeddings that guide the model toward the desired output of the downstream task.\n",
    "\n",
    "For more details on how to change hyperparameters for prompt learning in NeMo, see this [tutorial](https://github.com/NVIDIA/NeMo/blob/main/tutorials/nlp/Multitask_Prompt_and_PTuning.ipynb) which is also the basis for this NVFlare tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0de9ec",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "We assume you followed the instructions [here](../../README.md#requirements) \n",
    "to install the NeMo framework and the NeMo-NVFlare package. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e0e4fc",
   "metadata": {},
   "source": [
    "## Download the pre-trained LLM\n",
    "In this example, we use a `MegatronGPTModel`, a transformer-based language model based on the GPT architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8541e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-05-03 21:53:35 experimental:27] Module <class 'nemo.collections.nlp.data.language_modeling.megatron.megatron_batch_samplers.MegatronPretrainingRandomBatchSampler'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-05-03 21:53:35 experimental:27] Module <class 'nemo.collections.nlp.models.text_normalization_as_tagging.thutmose_tagger.ThutmoseTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-05-03 21:53:37 experimental:27] Module <class 'nemo.collections.asr.modules.audio_modules.SpectrogramToMultichannelFeatures'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PretrainedModelInfo(\n",
       " \tpretrained_model_name=megatron_gpt_345m,\n",
       " \tdescription=345M parameter GPT generative Megatron model.,\n",
       " \tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/megatron_gpt_345m/versions/1/files/megatron_gpt_345m.nemo\n",
       " )]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check what GPT .nemo models we have available on NGC\n",
    "from nemo.collections.nlp.models.language_modeling.megatron_gpt_model import MegatronGPTModel\n",
    "MegatronGPTModel.list_available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "876d6752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "megatron_gpt_345m.nemo already downloaded.\n"
     ]
    }
   ],
   "source": [
    "# Download the model from NGC\n",
    "import os\n",
    "model_file = \"megatron_gpt_345m.nemo\"\n",
    "if not os.path.isfile(model_file):\n",
    "    !wget \"https://api.ngc.nvidia.com/v2/models/nvidia/nemo/megatron_gpt_345m/versions/1/files/$model_file\"\n",
    "else:\n",
    "    print(f\"{model_file} already downloaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c56c7f",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "As our downstream task, we will use the [Financial PhraseBank dataset](https://huggingface.co/datasets/financial_phrasebank) for sentiment analysis.\n",
    "\n",
    "The Financial PhraseBank dataset contains the sentiments for financial news headlines from a retail investor's perspective. Further details about the dataset can be found in Malo et al.'s [\"Good Debt or Bad Debt: Detecting Semantic Orientations in Economic Texts\"](https://asistdl.onlinelibrary.wiley.com/doi/abs/10.1002/asi.23062).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f93ba3",
   "metadata": {},
   "source": [
    "#### 1. Download the preprocessing scripts\n",
    "We use the preprocessing scripts provided by NeMo which can be downloaded from GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ddda927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt_learning_financial_phrase_bank_preprocessing.py already downloaded.\n"
     ]
    }
   ],
   "source": [
    "script_name = \"prompt_learning_financial_phrase_bank_preprocessing.py\"\n",
    "if not os.path.isfile(script_name):\n",
    "    !wget -N \"https://raw.githubusercontent.com/NVIDIA/NeMo/main/scripts/dataset_processing/nlp/financial_phrase_bank/$script_name\"\n",
    "else:\n",
    "    print(f\"{script_name} already downloaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf62da4",
   "metadata": {},
   "source": [
    "#### 2. Download the Financial PhraseBank Dataset\n",
    "\n",
    "Download the `FinancialPhraseBank-v1.0.zip` dataset from [here](https://www.researchgate.net/profile/Pekka_Malo/publication/251231364_FinancialPhraseBank-v1.0/data/0c96051eee4fb1d56e000000/FinancialPhraseBank-v1.0.zip).\n",
    "\n",
    "Then extract it under `./data`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277f56d5",
   "metadata": {},
   "source": [
    "#### 3. Preprocess the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dec8fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving train split to data/FinancialPhraseBank-v1.0/financial_phrase_bank_train.jsonl\n",
      "100%|███████████████████████████████████| 1811/1811 [00:00<00:00, 112901.27it/s]\n",
      "Saving val split to data/FinancialPhraseBank-v1.0/financial_phrase_bank_val.jsonl\n",
      "100%|█████████████████████████████████████| 226/226 [00:00<00:00, 113468.12it/s]\n",
      "Saving test split to data/FinancialPhraseBank-v1.0/financial_phrase_bank_test.jsonl\n",
      "100%|█████████████████████████████████████| 227/227 [00:00<00:00, 122457.49it/s]\n"
     ]
    }
   ],
   "source": [
    "!python3 prompt_learning_financial_phrase_bank_preprocessing.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca77088a",
   "metadata": {},
   "source": [
    "#### 4. Split the dataset to simulate clients\n",
    "Next, we use three clients to simulate federated learning for p-tuning with NeMo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "716cb817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded training data with 1811 entries\n",
      "Save split 1 of 3 with 604 entries to data/FinancialPhraseBank-v1.0_split/site-1.jsonl\n",
      "Save split 2 of 3 with 604 entries to data/FinancialPhraseBank-v1.0_split/site-2.jsonl\n",
      "Save split 3 of 3 with 603 entries to data/FinancialPhraseBank-v1.0_split/site-3.jsonl\n"
     ]
    }
   ],
   "source": [
    "!python3 data/split_financial_phrase_data.py --data_path data/FinancialPhraseBank-v1.0/financial_phrase_bank_train.jsonl --num_clients 3 --out_dir data/FinancialPhraseBank-v1.0_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63960a34",
   "metadata": {},
   "source": [
    "## Federated learning simulations\n",
    "Next, we are using NVFlare's [simulator](https://nvflare.readthedocs.io/en/latest/user_guide/fl_simulator.html) to simulate each client training on their own dataset locally and all three clients training together using the [FedAvg](https://arxiv.org/abs/1602.05629) algorithm implemented in NVFlare.\n",
    "\n",
    "With this setting, we require a GPU with at least 16GB memory to run all clients in parallel on the same GPU. \n",
    "If you have multiple GPUs in your system, you can use the `gpu` argument to assign one GPU for each client, e.g., `gpu=\"0,1\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42766cea",
   "metadata": {},
   "source": [
    "#### 1. Local P-Tuning\n",
    "First, we modify the configuration files to include the current directory path to access the dataset and pre-trained LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "264d82cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set ROOT_DIR to /workspace/Code/nvflare/nemo_nvflare/integration/nemo/examples/prompt_learning\n"
     ]
    }
   ],
   "source": [
    "!python3 modify_configs.py --job_folder \"jobs/gpt_p-tuning_local\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037b409b",
   "metadata": {},
   "source": [
    "Next, simulate each client p-tuning on their local dataset using the FL simulator. To do this, we only run 1 round of FL, with each client running 50 p-tuning epochs on their local dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670f2c8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-03 21:53:41,258 - SimulatorRunner - INFO - Create the Simulator Server.\n",
      "2023-05-03 21:53:41,263 - Cell - INFO - server: creating listener on tcp://0:59755\n",
      "2023-05-03 21:53:41,264 - Cell - INFO - server: created backbone external listener for tcp://0:59755\n",
      "2023-05-03 21:53:41,265 - ConnectorManager - INFO - 25901: Try start_listener Listener resources: {'secure': False, 'host': 'localhost'}\n",
      "2023-05-03 21:53:41,267 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connector [CH00002 PASSIVE tcp://0:2793] is starting\n",
      "2023-05-03 21:53:41,769 - Cell - INFO - server: created backbone internal listener for tcp://localhost:2793\n",
      "2023-05-03 21:53:41,771 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connector [CH00001 PASSIVE tcp://0:59755] is starting\n",
      "2023-05-03 21:53:41,969 - nvflare.fuel.hci.server.hci - INFO - Starting Admin Server localhost on Port 44111\n",
      "2023-05-03 21:53:41,970 - SimulatorRunner - INFO - Deploy the Apps.\n",
      "2023-05-03 21:53:41,980 - SimulatorRunner - INFO - Create the simulate clients.\n",
      "2023-05-03 21:53:41,985 - ClientManager - INFO - Client: New client site-1@100.96.208.68 joined. Sent token: 594c1a08-2ed1-45ea-b4ef-f9796e91bda9.  Total clients: 1\n",
      "2023-05-03 21:53:41,988 - FederatedClient - INFO - Successfully registered client:site-1 for project simulator_server. Token:594c1a08-2ed1-45ea-b4ef-f9796e91bda9 SSID:\n",
      "2023-05-03 21:53:41,993 - ClientManager - INFO - Client: New client site-2@100.96.208.68 joined. Sent token: 7dfd255a-cb98-4705-8bcc-65efd589a746.  Total clients: 2\n",
      "2023-05-03 21:53:41,995 - FederatedClient - INFO - Successfully registered client:site-2 for project simulator_server. Token:7dfd255a-cb98-4705-8bcc-65efd589a746 SSID:\n",
      "2023-05-03 21:53:42,000 - ClientManager - INFO - Client: New client site-3@100.96.208.68 joined. Sent token: 58c48373-51d2-4430-9643-0d47e529c4e6.  Total clients: 3\n",
      "2023-05-03 21:53:42,002 - FederatedClient - INFO - Successfully registered client:site-3 for project simulator_server. Token:58c48373-51d2-4430-9643-0d47e529c4e6 SSID:\n",
      "2023-05-03 21:53:42,004 - SimulatorRunner - INFO - Set the client status ready.\n",
      "2023-05-03 21:53:42,005 - SimulatorRunner - INFO - Deploy and start the Server App.\n",
      "2023-05-03 21:53:42,008 - ServerCommandAgent - INFO - ServerCommandAgent cell register_request_cb: server.simulate_job\n",
      "NEMO version 1.17.0\n",
      "2023-05-03 21:53:42,979 - IntimeModelSelector - INFO - model selection weights control: None\n",
      "2023-05-03 21:53:42,982 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job]: Server runner starting ...\n",
      "2023-05-03 21:53:43,015 - SimulatorClientRunner - INFO - Start the clients run simulation.\n",
      "2023-05-03 21:53:43,101 - ServerPromptEncoder - INFO - [identity=simulator_server, run=simulate_job]: Initialized prompt encoder type PromptEncoderType.MLP\n",
      "2023-05-03 21:53:43,102 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job]: starting workflow scatter_and_gather (<class 'nvflare.app_common.workflows.scatter_and_gather.ScatterAndGather'>) ...\n",
      "2023-05-03 21:53:43,103 - ScatterAndGather - INFO - [identity=simulator_server, run=simulate_job]: Initializing ScatterAndGather workflow.\n",
      "2023-05-03 21:53:43,105 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job]: Workflow scatter_and_gather (<class 'nvflare.app_common.workflows.scatter_and_gather.ScatterAndGather'>) started\n",
      "2023-05-03 21:53:43,107 - ScatterAndGather - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: Beginning ScatterAndGather training phase.\n",
      "2023-05-03 21:53:43,108 - ScatterAndGather - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: Round 0 started.\n",
      "2023-05-03 21:53:43,109 - ScatterAndGather - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: scheduled task train\n",
      "2023-05-03 21:53:44,039 - SimulatorClientRunner - INFO - Simulate Run client: site-1\n",
      "2023-05-03 21:53:44,041 - SimulatorClientRunner - INFO - Simulate Run client: site-2\n",
      "2023-05-03 21:53:44,064 - SimulatorClientRunner - INFO - Simulate Run client: site-3\n",
      "2023-05-03 21:53:45,105 - ClientTaskWorker - INFO - ClientTaskWorker started to run\n",
      "2023-05-03 21:53:45,106 - ClientTaskWorker - INFO - ClientTaskWorker started to run\n",
      "2023-05-03 21:53:45,107 - Cell - INFO - site-1.simulate_job: created backbone external connector to tcp://localhost:59755\n",
      "2023-05-03 21:53:45,107 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connector [CH00001 ACTIVE tcp://localhost:59755] is starting\n",
      "2023-05-03 21:53:45,107 - Cell - INFO - site-2.simulate_job: created backbone external connector to tcp://localhost:59755\n",
      "2023-05-03 21:53:45,108 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connector [CH00001 ACTIVE tcp://localhost:59755] is starting\n",
      "2023-05-03 21:53:45,127 - ClientTaskWorker - INFO - ClientTaskWorker started to run\n",
      "2023-05-03 21:53:45,129 - Cell - INFO - site-3.simulate_job: created backbone external connector to tcp://localhost:59755\n",
      "2023-05-03 21:53:45,129 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connector [CH00001 ACTIVE tcp://localhost:59755] is starting\n",
      "2023-05-03 21:53:51,847 - torch.distributed.nn.jit.instantiator - INFO - Created a temporary directory at /tmp/tmp5dmlbv2r\n",
      "2023-05-03 21:53:51,848 - torch.distributed.nn.jit.instantiator - INFO - Writing /tmp/tmp5dmlbv2r/_remote_module_non_scriptable.py\n",
      "2023-05-03 21:53:51,850 - torch.distributed.nn.jit.instantiator - INFO - Created a temporary directory at /tmp/tmpsdv_lt_v\n",
      "2023-05-03 21:53:51,851 - torch.distributed.nn.jit.instantiator - INFO - Writing /tmp/tmpsdv_lt_v/_remote_module_non_scriptable.py\n",
      "2023-05-03 21:53:52,011 - torch.distributed.nn.jit.instantiator - INFO - Created a temporary directory at /tmp/tmpldnkc1b5\n",
      "2023-05-03 21:53:52,011 - torch.distributed.nn.jit.instantiator - INFO - Writing /tmp/tmpldnkc1b5/_remote_module_non_scriptable.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-05-03 21:53:59 experimental:27] Module <class 'nemo.collections.nlp.data.language_modeling.megatron.megatron_batch_samplers.MegatronPretrainingRandomBatchSampler'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-05-03 21:53:59 experimental:27] Module <class 'nemo.collections.nlp.models.text_normalization_as_tagging.thutmose_tagger.ThutmoseTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-05-03 21:53:59 experimental:27] Module <class 'nemo.collections.nlp.data.language_modeling.megatron.megatron_batch_samplers.MegatronPretrainingRandomBatchSampler'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-05-03 21:53:59 experimental:27] Module <class 'nemo.collections.nlp.models.text_normalization_as_tagging.thutmose_tagger.ThutmoseTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-05-03 21:53:59 experimental:27] Module <class 'nemo.collections.nlp.data.language_modeling.megatron.megatron_batch_samplers.MegatronPretrainingRandomBatchSampler'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-05-03 21:53:59 experimental:27] Module <class 'nemo.collections.nlp.models.text_normalization_as_tagging.thutmose_tagger.ThutmoseTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-05-03 21:54:00 experimental:27] Module <class 'nemo.collections.asr.modules.audio_modules.SpectrogramToMultichannelFeatures'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-05-03 21:54:00 experimental:27] Module <class 'nemo.collections.asr.modules.audio_modules.SpectrogramToMultichannelFeatures'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-05-03 21:54:00 experimental:27] Module <class 'nemo.collections.asr.modules.audio_modules.SpectrogramToMultichannelFeatures'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEMO version 1.17.0\n",
      "2023-05-03 21:54:00,880 - PromptLearner - INFO - [identity=site-3, run=simulate_job]: Initializing the Learner...\n",
      "2023-05-03 21:54:00,880 - PromptLearner - INFO - [identity=site-3, run=simulate_job]: Using `MASTER_ADDR`: localhost and `MASTER_PORT`: None\n",
      "2023-05-03 21:54:00,931 - PromptLearner - INFO - [identity=site-3, run=simulate_job]: Load model configuration from /tmp/nvflare/nemo/gpt_p-tuning_local/simulate_job/app_site-3/config/megatron_gpt_prompt_learning_config.yaml\n",
      "2023-05-03 21:54:00,938 - PromptLearner - INFO - [identity=site-3, run=simulate_job]: Training with global_batch_size 8 and micro_batch_size 4\n",
      "2023-05-03 21:54:00,942 - pytorch_lightning.utilities.rank_zero - INFO - Using 16bit None Automatic Mixed Precision (AMP)\n",
      "2023-05-03 21:54:00,977 - pytorch_lightning.utilities.rank_zero - INFO - GPU available: True (cuda), used: True\n",
      "2023-05-03 21:54:00,978 - pytorch_lightning.utilities.rank_zero - INFO - TPU available: False, using: 0 TPU cores\n",
      "2023-05-03 21:54:00,978 - pytorch_lightning.utilities.rank_zero - INFO - IPU available: False, using: 0 IPUs\n",
      "2023-05-03 21:54:00,978 - pytorch_lightning.utilities.rank_zero - INFO - HPU available: False, using: 0 HPUs\n",
      "2023-05-03 21:54:00,979 - pytorch_lightning.utilities.rank_zero - INFO - `Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n",
      "2023-05-03 21:54:00,982 - PromptLearner - INFO - [identity=site-3, run=simulate_job]: Trainer config - devices: 1\n",
      "accelerator: gpu\n",
      "num_nodes: 1\n",
      "precision: 16\n",
      "logger: true\n",
      "enable_checkpointing: false\n",
      "replace_sampler_ddp: false\n",
      "max_epochs: -1\n",
      "max_steps: -1\n",
      "log_every_n_steps: 10\n",
      "val_check_interval: 1.0\n",
      "gradient_clip_val: 1.0\n",
      "resume_from_checkpoint: null\n",
      "benchmark: false\n",
      "default_root_dir: /tmp/nvflare/nemo/gpt_p-tuning_local/simulate_job/app_site-3\n",
      "\n",
      "NEMO version 1.17.0\n",
      "2023-05-03 21:54:01,044 - PromptLearner - INFO - [identity=site-1, run=simulate_job]: Initializing the Learner...\n",
      "2023-05-03 21:54:01,044 - PromptLearner - INFO - [identity=site-1, run=simulate_job]: Using `MASTER_ADDR`: localhost and `MASTER_PORT`: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0503 21:54:00.880515 140649257482048 fl_component.py:134] [identity=site-3, run=simulate_job]: Initializing the Learner...\n",
      "I0503 21:54:00.880946 140649257482048 fl_component.py:134] [identity=site-3, run=simulate_job]: Using `MASTER_ADDR`: localhost and `MASTER_PORT`: None\n",
      "I0503 21:54:00.931094 140649257482048 fl_component.py:134] [identity=site-3, run=simulate_job]: Load model configuration from /tmp/nvflare/nemo/gpt_p-tuning_local/simulate_job/app_site-3/config/megatron_gpt_prompt_learning_config.yaml\n",
      "I0503 21:54:00.938791 140649257482048 fl_component.py:134] [identity=site-3, run=simulate_job]: Training with global_batch_size 8 and micro_batch_size 4\n",
      "I0503 21:54:00.942872 140649257482048 accelerator_connector.py:758] Using 16bit None Automatic Mixed Precision (AMP)\n",
      "I0503 21:54:00.977674 140649257482048 setup.py:163] GPU available: True (cuda), used: True\n",
      "I0503 21:54:00.978206 140649257482048 setup.py:166] TPU available: False, using: 0 TPU cores\n",
      "I0503 21:54:00.978397 140649257482048 setup.py:169] IPU available: False, using: 0 IPUs\n",
      "I0503 21:54:00.978557 140649257482048 setup.py:172] HPU available: False, using: 0 HPUs\n",
      "I0503 21:54:00.979366 140649257482048 setup.py:121] `Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n",
      "I0503 21:54:00.982384 140649257482048 fl_component.py:134] [identity=site-3, run=simulate_job]: Trainer config - devices: 1\n",
      "accelerator: gpu\n",
      "num_nodes: 1\n",
      "precision: 16\n",
      "logger: true\n",
      "enable_checkpointing: false\n",
      "replace_sampler_ddp: false\n",
      "max_epochs: -1\n",
      "max_steps: -1\n",
      "log_every_n_steps: 10\n",
      "val_check_interval: 1.0\n",
      "gradient_clip_val: 1.0\n",
      "resume_from_checkpoint: null\n",
      "benchmark: false\n",
      "default_root_dir: /tmp/nvflare/nemo/gpt_p-tuning_local/simulate_job/app_site-3\n",
      "\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0503 21:54:01.044543 140522389899072 fl_component.py:134] [identity=site-1, run=simulate_job]: Initializing the Learner...\n",
      "I0503 21:54:01.044970 140522389899072 fl_component.py:134] [identity=site-1, run=simulate_job]: Using `MASTER_ADDR`: localhost and `MASTER_PORT`: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-03 21:54:01,096 - PromptLearner - INFO - [identity=site-1, run=simulate_job]: Load model configuration from /tmp/nvflare/nemo/gpt_p-tuning_local/simulate_job/app_site-1/config/megatron_gpt_prompt_learning_config.yaml\n",
      "2023-05-03 21:54:01,104 - PromptLearner - INFO - [identity=site-1, run=simulate_job]: Training with global_batch_size 8 and micro_batch_size 4\n",
      "NEMO version 1.17.0\n",
      "2023-05-03 21:54:01,111 - PromptLearner - INFO - [identity=site-2, run=simulate_job]: Initializing the Learner...\n",
      "2023-05-03 21:54:01,112 - PromptLearner - INFO - [identity=site-2, run=simulate_job]: Using `MASTER_ADDR`: localhost and `MASTER_PORT`: None\n",
      "2023-05-03 21:54:01,165 - PromptLearner - INFO - [identity=site-2, run=simulate_job]: Load model configuration from /tmp/nvflare/nemo/gpt_p-tuning_local/simulate_job/app_site-2/config/megatron_gpt_prompt_learning_config.yaml\n",
      "2023-05-03 21:54:01,173 - PromptLearner - INFO - [identity=site-2, run=simulate_job]: Training with global_batch_size 8 and micro_batch_size 4\n",
      "2023-05-03 21:54:01,260 - pytorch_lightning.utilities.rank_zero - INFO - Using 16bit None Automatic Mixed Precision (AMP)\n",
      "2023-05-03 21:54:01,274 - pytorch_lightning.utilities.rank_zero - INFO - Using 16bit None Automatic Mixed Precision (AMP)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 21:54:01.096453 140522389899072 fl_component.py:134] [identity=site-1, run=simulate_job]: Load model configuration from /tmp/nvflare/nemo/gpt_p-tuning_local/simulate_job/app_site-1/config/megatron_gpt_prompt_learning_config.yaml\n",
      "I0503 21:54:01.104016 140522389899072 fl_component.py:134] [identity=site-1, run=simulate_job]: Training with global_batch_size 8 and micro_batch_size 4\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0503 21:54:01.111409 139755592410944 fl_component.py:134] [identity=site-2, run=simulate_job]: Initializing the Learner...\n",
      "I0503 21:54:01.112121 139755592410944 fl_component.py:134] [identity=site-2, run=simulate_job]: Using `MASTER_ADDR`: localhost and `MASTER_PORT`: None\n",
      "I0503 21:54:01.165990 139755592410944 fl_component.py:134] [identity=site-2, run=simulate_job]: Load model configuration from /tmp/nvflare/nemo/gpt_p-tuning_local/simulate_job/app_site-2/config/megatron_gpt_prompt_learning_config.yaml\n",
      "I0503 21:54:01.173627 139755592410944 fl_component.py:134] [identity=site-2, run=simulate_job]: Training with global_batch_size 8 and micro_batch_size 4\n",
      "I0503 21:54:01.260519 140522389899072 accelerator_connector.py:758] Using 16bit None Automatic Mixed Precision (AMP)\n",
      "I0503 21:54:01.274351 139755592410944 accelerator_connector.py:758] Using 16bit None Automatic Mixed Precision (AMP)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-03 21:54:01,309 - pytorch_lightning.utilities.rank_zero - INFO - GPU available: True (cuda), used: True\n",
      "2023-05-03 21:54:01,310 - pytorch_lightning.utilities.rank_zero - INFO - TPU available: False, using: 0 TPU cores\n",
      "2023-05-03 21:54:01,310 - pytorch_lightning.utilities.rank_zero - INFO - IPU available: False, using: 0 IPUs\n",
      "2023-05-03 21:54:01,310 - pytorch_lightning.utilities.rank_zero - INFO - HPU available: False, using: 0 HPUs\n",
      "2023-05-03 21:54:01,311 - pytorch_lightning.utilities.rank_zero - INFO - GPU available: True (cuda), used: True\n",
      "2023-05-03 21:54:01,311 - pytorch_lightning.utilities.rank_zero - INFO - `Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n",
      "2023-05-03 21:54:01,311 - pytorch_lightning.utilities.rank_zero - INFO - TPU available: False, using: 0 TPU cores\n",
      "2023-05-03 21:54:01,312 - pytorch_lightning.utilities.rank_zero - INFO - IPU available: False, using: 0 IPUs\n",
      "2023-05-03 21:54:01,312 - pytorch_lightning.utilities.rank_zero - INFO - HPU available: False, using: 0 HPUs\n",
      "2023-05-03 21:54:01,313 - pytorch_lightning.utilities.rank_zero - INFO - `Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n",
      "2023-05-03 21:54:01,314 - PromptLearner - INFO - [identity=site-1, run=simulate_job]: Trainer config - devices: 1\n",
      "accelerator: gpu\n",
      "num_nodes: 1\n",
      "precision: 16\n",
      "logger: true\n",
      "enable_checkpointing: false\n",
      "replace_sampler_ddp: false\n",
      "max_epochs: -1\n",
      "max_steps: -1\n",
      "log_every_n_steps: 10\n",
      "val_check_interval: 1.0\n",
      "gradient_clip_val: 1.0\n",
      "resume_from_checkpoint: null\n",
      "benchmark: false\n",
      "default_root_dir: /tmp/nvflare/nemo/gpt_p-tuning_local/simulate_job/app_site-1\n",
      "\n",
      "2023-05-03 21:54:01,316 - PromptLearner - INFO - [identity=site-2, run=simulate_job]: Trainer config - devices: 1\n",
      "accelerator: gpu\n",
      "num_nodes: 1\n",
      "precision: 16\n",
      "logger: true\n",
      "enable_checkpointing: false\n",
      "replace_sampler_ddp: false\n",
      "max_epochs: -1\n",
      "max_steps: -1\n",
      "log_every_n_steps: 10\n",
      "val_check_interval: 1.0\n",
      "gradient_clip_val: 1.0\n",
      "resume_from_checkpoint: null\n",
      "benchmark: false\n",
      "default_root_dir: /tmp/nvflare/nemo/gpt_p-tuning_local/simulate_job/app_site-2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 21:54:01.309562 140522389899072 setup.py:163] GPU available: True (cuda), used: True\n",
      "I0503 21:54:01.310195 140522389899072 setup.py:166] TPU available: False, using: 0 TPU cores\n",
      "I0503 21:54:01.310412 140522389899072 setup.py:169] IPU available: False, using: 0 IPUs\n",
      "I0503 21:54:01.310610 140522389899072 setup.py:172] HPU available: False, using: 0 HPUs\n",
      "I0503 21:54:01.311291 139755592410944 setup.py:163] GPU available: True (cuda), used: True\n",
      "I0503 21:54:01.311563 140522389899072 setup.py:121] `Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n",
      "I0503 21:54:01.311942 139755592410944 setup.py:166] TPU available: False, using: 0 TPU cores\n",
      "I0503 21:54:01.312154 139755592410944 setup.py:169] IPU available: False, using: 0 IPUs\n",
      "I0503 21:54:01.312330 139755592410944 setup.py:172] HPU available: False, using: 0 HPUs\n",
      "I0503 21:54:01.313292 139755592410944 setup.py:121] `Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n",
      "I0503 21:54:01.314815 140522389899072 fl_component.py:134] [identity=site-1, run=simulate_job]: Trainer config - devices: 1\n",
      "accelerator: gpu\n",
      "num_nodes: 1\n",
      "precision: 16\n",
      "logger: true\n",
      "enable_checkpointing: false\n",
      "replace_sampler_ddp: false\n",
      "max_epochs: -1\n",
      "max_steps: -1\n",
      "log_every_n_steps: 10\n",
      "val_check_interval: 1.0\n",
      "gradient_clip_val: 1.0\n",
      "resume_from_checkpoint: null\n",
      "benchmark: false\n",
      "default_root_dir: /tmp/nvflare/nemo/gpt_p-tuning_local/simulate_job/app_site-1\n",
      "\n",
      "I0503 21:54:01.316548 139755592410944 fl_component.py:134] [identity=site-2, run=simulate_job]: Trainer config - devices: 1\n",
      "accelerator: gpu\n",
      "num_nodes: 1\n",
      "precision: 16\n",
      "logger: true\n",
      "enable_checkpointing: false\n",
      "replace_sampler_ddp: false\n",
      "max_epochs: -1\n",
      "max_steps: -1\n",
      "log_every_n_steps: 10\n",
      "val_check_interval: 1.0\n",
      "gradient_clip_val: 1.0\n",
      "resume_from_checkpoint: null\n",
      "benchmark: false\n",
      "default_root_dir: /tmp/nvflare/nemo/gpt_p-tuning_local/simulate_job/app_site-2\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-05-03 21:54:01 megatron_init:225] Rank 0 has data parallel group: [0]\n",
      "[NeMo I 2023-05-03 21:54:01 megatron_init:228] All data parallel group ranks: [[0]]\n",
      "[NeMo I 2023-05-03 21:54:01 megatron_init:229] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2023-05-03 21:54:01 megatron_init:237] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2023-05-03 21:54:01 megatron_init:238] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2023-05-03 21:54:01 megatron_init:248] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2023-05-03 21:54:01 megatron_init:252] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2023-05-03 21:54:01 megatron_init:253] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2023-05-03 21:54:01 megatron_init:267] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2023-05-03 21:54:01 megatron_init:279] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2023-05-03 21:54:01 megatron_init:285] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2023-05-03 21:54:01 megatron_init:286] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2023-05-03 21:54:01 megatron_init:287] All embedding group ranks: [[0]]\n",
      "[NeMo I 2023-05-03 21:54:01 megatron_init:288] Rank 0 has embedding rank: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23-05-03 21:54:01 - PID:26006 - rank:(0, 0, 0, 0) - microbatches.py:39 - INFO - setting number of micro-batches to constant 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-05-03 21:54:02 megatron_init:225] Rank 0 has data parallel group: [0]\n",
      "[NeMo I 2023-05-03 21:54:02 megatron_init:228] All data parallel group ranks: [[0]]\n",
      "[NeMo I 2023-05-03 21:54:02 megatron_init:229] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2023-05-03 21:54:02 megatron_init:237] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2023-05-03 21:54:02 megatron_init:238] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2023-05-03 21:54:02 megatron_init:248] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2023-05-03 21:54:02 megatron_init:252] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2023-05-03 21:54:02 megatron_init:253] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2023-05-03 21:54:02 megatron_init:267] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2023-05-03 21:54:02 megatron_init:279] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2023-05-03 21:54:02 megatron_init:285] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2023-05-03 21:54:02 megatron_init:286] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2023-05-03 21:54:02 megatron_init:287] All embedding group ranks: [[0]]\n",
      "[NeMo I 2023-05-03 21:54:02 megatron_init:288] Rank 0 has embedding rank: 0\n",
      "[NeMo I 2023-05-03 21:54:02 megatron_init:225] Rank 0 has data parallel group: [0]\n",
      "[NeMo I 2023-05-03 21:54:02 megatron_init:228] All data parallel group ranks: [[0]]\n",
      "[NeMo I 2023-05-03 21:54:02 megatron_init:229] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2023-05-03 21:54:02 megatron_init:237] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2023-05-03 21:54:02 megatron_init:238] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2023-05-03 21:54:02 megatron_init:248] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2023-05-03 21:54:02 megatron_init:252] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2023-05-03 21:54:02 megatron_init:253] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2023-05-03 21:54:02 megatron_init:267] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2023-05-03 21:54:02 megatron_init:279] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2023-05-03 21:54:02 megatron_init:285] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2023-05-03 21:54:02 megatron_init:286] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2023-05-03 21:54:02 megatron_init:287] All embedding group ranks: [[0]]\n",
      "[NeMo I 2023-05-03 21:54:02 megatron_init:288] Rank 0 has embedding rank: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23-05-03 21:54:02 - PID:26002 - rank:(0, 0, 0, 0) - microbatches.py:39 - INFO - setting number of micro-batches to constant 2\n",
      "23-05-03 21:54:02 - PID:26005 - rank:(0, 0, 0, 0) - microbatches.py:39 - INFO - setting number of micro-batches to constant 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-05-03 21:54:47 megatron_init:225] Rank 0 has data parallel group: [0]\n",
      "[NeMo I 2023-05-03 21:54:47 megatron_init:228] All data parallel group ranks: [[0]]\n",
      "[NeMo I 2023-05-03 21:54:47 megatron_init:229] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2023-05-03 21:54:47 megatron_init:237] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2023-05-03 21:54:47 megatron_init:238] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2023-05-03 21:54:47 megatron_init:248] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2023-05-03 21:54:47 megatron_init:252] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2023-05-03 21:54:47 megatron_init:253] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2023-05-03 21:54:47 megatron_init:267] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2023-05-03 21:54:47 megatron_init:279] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2023-05-03 21:54:47 megatron_init:285] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2023-05-03 21:54:47 megatron_init:286] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2023-05-03 21:54:47 megatron_init:287] All embedding group ranks: [[0]]\n",
      "[NeMo I 2023-05-03 21:54:47 megatron_init:288] Rank 0 has embedding rank: 0\n",
      "[NeMo I 2023-05-03 21:54:47 tokenizer_utils:204] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: /tmp/tmpd89a7h63/bfcdca5e44814366bdb5dcd651325152_gpt2-vocab.json, and merges file: /tmp/tmpd89a7h63/315a11fd68be49d6abdb34363e8c4997_gpt2-merge.txt\n",
      "[NeMo I 2023-05-03 21:54:47 tokenizer_utils:130] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /tmp/tmpd89a7h63/bfcdca5e44814366bdb5dcd651325152_gpt2-vocab.json, merges_files: /tmp/tmpd89a7h63/315a11fd68be49d6abdb34363e8c4997_gpt2-merge.txt, special_tokens_dict: {}, and use_fast: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-05-03 21:54:47 modelPT:245] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact for it has already been registered.\n",
      "Using sep_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using pad_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n",
      "[NeMo W 2023-05-03 21:54:51 modelPT:245] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact for it has already been registered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-05-03 21:54:51 megatron_base_model:205] Padded vocab_size: 50304, original vocab_size: 50257, dummy tokens: 47.\n",
      "[NeMo I 2023-05-03 21:54:51 megatron_init:225] Rank 0 has data parallel group: [0]\n",
      "[NeMo I 2023-05-03 21:54:51 megatron_init:228] All data parallel group ranks: [[0]]\n",
      "[NeMo I 2023-05-03 21:54:51 megatron_init:229] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2023-05-03 21:54:51 megatron_init:237] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2023-05-03 21:54:51 megatron_init:238] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2023-05-03 21:54:51 megatron_init:248] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2023-05-03 21:54:51 megatron_init:252] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2023-05-03 21:54:51 megatron_init:253] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2023-05-03 21:54:51 megatron_init:267] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2023-05-03 21:54:51 megatron_init:279] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2023-05-03 21:54:51 megatron_init:285] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2023-05-03 21:54:51 megatron_init:286] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2023-05-03 21:54:51 megatron_init:287] All embedding group ranks: [[0]]\n",
      "[NeMo I 2023-05-03 21:54:51 megatron_init:288] Rank 0 has embedding rank: 0\n",
      "[NeMo I 2023-05-03 21:54:51 tokenizer_utils:204] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: /tmp/tmp8r6xvav9/bfcdca5e44814366bdb5dcd651325152_gpt2-vocab.json, and merges file: /tmp/tmp8r6xvav9/315a11fd68be49d6abdb34363e8c4997_gpt2-merge.txt\n",
      "[NeMo I 2023-05-03 21:54:51 tokenizer_utils:130] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /tmp/tmp8r6xvav9/bfcdca5e44814366bdb5dcd651325152_gpt2-vocab.json, merges_files: /tmp/tmp8r6xvav9/315a11fd68be49d6abdb34363e8c4997_gpt2-merge.txt, special_tokens_dict: {}, and use_fast: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using sep_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using pad_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-05-03 21:54:52 megatron_base_model:205] Padded vocab_size: 50304, original vocab_size: 50257, dummy tokens: 47.\n",
      "[NeMo I 2023-05-03 21:54:53 megatron_init:225] Rank 0 has data parallel group: [0]\n",
      "[NeMo I 2023-05-03 21:54:53 megatron_init:228] All data parallel group ranks: [[0]]\n",
      "[NeMo I 2023-05-03 21:54:53 megatron_init:229] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2023-05-03 21:54:53 megatron_init:237] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2023-05-03 21:54:53 megatron_init:238] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2023-05-03 21:54:53 megatron_init:248] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2023-05-03 21:54:53 megatron_init:252] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2023-05-03 21:54:53 megatron_init:253] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2023-05-03 21:54:53 megatron_init:267] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2023-05-03 21:54:53 megatron_init:279] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2023-05-03 21:54:53 megatron_init:285] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2023-05-03 21:54:53 megatron_init:286] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2023-05-03 21:54:53 megatron_init:287] All embedding group ranks: [[0]]\n",
      "[NeMo I 2023-05-03 21:54:53 megatron_init:288] Rank 0 has embedding rank: 0\n",
      "[NeMo I 2023-05-03 21:54:53 tokenizer_utils:204] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: /tmp/tmp8nnfhuvh/bfcdca5e44814366bdb5dcd651325152_gpt2-vocab.json, and merges file: /tmp/tmp8nnfhuvh/315a11fd68be49d6abdb34363e8c4997_gpt2-merge.txt\n",
      "[NeMo I 2023-05-03 21:54:53 tokenizer_utils:130] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /tmp/tmp8nnfhuvh/bfcdca5e44814366bdb5dcd651325152_gpt2-vocab.json, merges_files: /tmp/tmp8nnfhuvh/315a11fd68be49d6abdb34363e8c4997_gpt2-merge.txt, special_tokens_dict: {}, and use_fast: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-05-03 21:54:53 modelPT:245] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact for it has already been registered.\n",
      "Using sep_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using pad_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-05-03 21:54:54 megatron_base_model:205] Padded vocab_size: 50304, original vocab_size: 50257, dummy tokens: 47.\n",
      "[NeMo I 2023-05-03 21:54:55 nlp_overrides:374] Model MegatronGPTModel was successfully restored from /workspace/Code/nvflare/nemo_nvflare/integration/nemo/examples/prompt_learning/megatron_gpt_345m.nemo.\n",
      "[NeMo I 2023-05-03 21:54:55 nlp_overrides:374] Model MegatronGPTModel was successfully restored from /workspace/Code/nvflare/nemo_nvflare/integration/nemo/examples/prompt_learning/megatron_gpt_345m.nemo.\n",
      "[NeMo I 2023-05-03 21:54:55 auto_tokenizer:172] 10 special tokens added, resize your model accordingly.\n",
      "[NeMo I 2023-05-03 21:54:55 auto_tokenizer:172] 10 special tokens added, resize your model accordingly.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n",
      "Using pad_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-05-03 21:54:56 nlp_overrides:374] Model MegatronGPTModel was successfully restored from /workspace/Code/nvflare/nemo_nvflare/integration/nemo/examples/prompt_learning/megatron_gpt_345m.nemo.\n",
      "[NeMo I 2023-05-03 21:54:56 auto_tokenizer:172] 10 special tokens added, resize your model accordingly.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-05-03 21:55:47 megatron_init:225] Rank 0 has data parallel group: [0]\n",
      "[NeMo I 2023-05-03 21:55:47 megatron_init:228] All data parallel group ranks: [[0]]\n",
      "[NeMo I 2023-05-03 21:55:47 megatron_init:229] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2023-05-03 21:55:47 megatron_init:237] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2023-05-03 21:55:47 megatron_init:238] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2023-05-03 21:55:47 megatron_init:248] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2023-05-03 21:55:47 megatron_init:252] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2023-05-03 21:55:47 megatron_init:253] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2023-05-03 21:55:47 megatron_init:267] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2023-05-03 21:55:47 megatron_init:279] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2023-05-03 21:55:47 megatron_init:285] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2023-05-03 21:55:47 megatron_init:286] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2023-05-03 21:55:47 megatron_init:287] All embedding group ranks: [[0]]\n",
      "[NeMo I 2023-05-03 21:55:47 megatron_init:288] Rank 0 has embedding rank: 0\n",
      "[NeMo I 2023-05-03 21:55:47 megatron_init:225] Rank 0 has data parallel group: [0]\n",
      "[NeMo I 2023-05-03 21:55:47 megatron_init:228] All data parallel group ranks: [[0]]\n",
      "[NeMo I 2023-05-03 21:55:47 megatron_init:229] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2023-05-03 21:55:47 megatron_init:237] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2023-05-03 21:55:47 megatron_init:238] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2023-05-03 21:55:47 megatron_init:248] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2023-05-03 21:55:47 megatron_init:252] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2023-05-03 21:55:47 megatron_init:253] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2023-05-03 21:55:47 megatron_init:267] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2023-05-03 21:55:47 megatron_init:279] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2023-05-03 21:55:47 megatron_init:285] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2023-05-03 21:55:47 megatron_init:286] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2023-05-03 21:55:47 megatron_init:287] All embedding group ranks: [[0]]\n",
      "[NeMo I 2023-05-03 21:55:47 megatron_init:288] Rank 0 has embedding rank: 0\n",
      "[NeMo I 2023-05-03 21:55:47 tokenizer_utils:204] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: /tmp/tmp68f7zdvk/bfcdca5e44814366bdb5dcd651325152_gpt2-vocab.json, and merges file: /tmp/tmp68f7zdvk/315a11fd68be49d6abdb34363e8c4997_gpt2-merge.txt\n",
      "[NeMo I 2023-05-03 21:55:47 tokenizer_utils:130] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /tmp/tmp68f7zdvk/bfcdca5e44814366bdb5dcd651325152_gpt2-vocab.json, merges_files: /tmp/tmp68f7zdvk/315a11fd68be49d6abdb34363e8c4997_gpt2-merge.txt, special_tokens_dict: {}, and use_fast: False\n",
      "[NeMo I 2023-05-03 21:55:47 tokenizer_utils:204] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: /tmp/tmpoebmfcwf/bfcdca5e44814366bdb5dcd651325152_gpt2-vocab.json, and merges file: /tmp/tmpoebmfcwf/315a11fd68be49d6abdb34363e8c4997_gpt2-merge.txt\n",
      "[NeMo I 2023-05-03 21:55:47 tokenizer_utils:130] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /tmp/tmpoebmfcwf/bfcdca5e44814366bdb5dcd651325152_gpt2-vocab.json, merges_files: /tmp/tmpoebmfcwf/315a11fd68be49d6abdb34363e8c4997_gpt2-merge.txt, special_tokens_dict: {}, and use_fast: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-05-03 21:55:47 modelPT:245] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact for it has already been registered.\n",
      "[NeMo W 2023-05-03 21:55:47 modelPT:245] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact for it has already been registered.\n",
      "Using sep_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using pad_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n",
      "Using sep_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using pad_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-05-03 21:55:48 megatron_base_model:205] Padded vocab_size: 50304, original vocab_size: 50257, dummy tokens: 47.\n",
      "[NeMo I 2023-05-03 21:55:48 megatron_base_model:205] Padded vocab_size: 50304, original vocab_size: 50257, dummy tokens: 47.\n",
      "[NeMo I 2023-05-03 21:55:50 megatron_init:225] Rank 0 has data parallel group: [0]\n",
      "[NeMo I 2023-05-03 21:55:50 megatron_init:228] All data parallel group ranks: [[0]]\n",
      "[NeMo I 2023-05-03 21:55:50 megatron_init:229] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2023-05-03 21:55:50 megatron_init:237] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2023-05-03 21:55:50 megatron_init:238] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2023-05-03 21:55:50 megatron_init:248] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2023-05-03 21:55:50 megatron_init:252] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2023-05-03 21:55:50 megatron_init:253] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2023-05-03 21:55:50 megatron_init:267] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2023-05-03 21:55:50 megatron_init:279] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2023-05-03 21:55:50 megatron_init:285] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2023-05-03 21:55:50 megatron_init:286] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2023-05-03 21:55:50 megatron_init:287] All embedding group ranks: [[0]]\n",
      "[NeMo I 2023-05-03 21:55:50 megatron_init:288] Rank 0 has embedding rank: 0\n",
      "[NeMo I 2023-05-03 21:55:50 tokenizer_utils:204] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: /tmp/tmpnm33f182/bfcdca5e44814366bdb5dcd651325152_gpt2-vocab.json, and merges file: /tmp/tmpnm33f182/315a11fd68be49d6abdb34363e8c4997_gpt2-merge.txt\n",
      "[NeMo I 2023-05-03 21:55:50 tokenizer_utils:130] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /tmp/tmpnm33f182/bfcdca5e44814366bdb5dcd651325152_gpt2-vocab.json, merges_files: /tmp/tmpnm33f182/315a11fd68be49d6abdb34363e8c4997_gpt2-merge.txt, special_tokens_dict: {}, and use_fast: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-05-03 21:55:50 modelPT:245] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact for it has already been registered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-03 21:55:50,572 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-3, peer_run=simulate_job, task_name=train, task_id=372f6920-d35f-4934-bae0-efd9db3fbb7f]: assigned task to client site-3: name=train, id=372f6920-d35f-4934-bae0-efd9db3fbb7f\n",
      "2023-05-03 21:55:50,574 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-3, peer_run=simulate_job, task_name=train, task_id=372f6920-d35f-4934-bae0-efd9db3fbb7f]: sent task assignment to client. client_name:site-3 task_id:372f6920-d35f-4934-bae0-efd9db3fbb7f\n",
      "2023-05-03 21:55:50,575 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-1, peer_run=simulate_job, task_name=train, task_id=c9c177bd-ced9-4dc3-a7c0-39683ab84cff]: assigned task to client site-1: name=train, id=c9c177bd-ced9-4dc3-a7c0-39683ab84cff\n",
      "2023-05-03 21:55:50,630 - GetTaskCommand - INFO - return task to client.  client_name: site-3  task_name: train   task_id: 372f6920-d35f-4934-bae0-efd9db3fbb7f  sharable_header_task_id: 372f6920-d35f-4934-bae0-efd9db3fbb7f\n",
      "2023-05-03 21:55:50,632 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-1, peer_run=simulate_job, task_name=train, task_id=c9c177bd-ced9-4dc3-a7c0-39683ab84cff]: sent task assignment to client. client_name:site-1 task_id:c9c177bd-ced9-4dc3-a7c0-39683ab84cff\n",
      "[NeMo I 2023-05-03 21:55:50 nlp_overrides:374] Model MegatronGPTModel was successfully restored from /workspace/Code/nvflare/nemo_nvflare/integration/nemo/examples/prompt_learning/megatron_gpt_345m.nemo.\n",
      "[NeMo I 2023-05-03 21:55:50 nlp_overrides:374] Model MegatronGPTModel was successfully restored from /workspace/Code/nvflare/nemo_nvflare/integration/nemo/examples/prompt_learning/megatron_gpt_345m.nemo.\n",
      "[NeMo I 2023-05-03 21:55:50 auto_tokenizer:172] 10 special tokens added, resize your model accordingly.\n",
      "[NeMo I 2023-05-03 21:55:50 auto_tokenizer:172] 10 special tokens added, resize your model accordingly.\n",
      "2023-05-03 21:55:50,563 - PromptLearner - INFO - [identity=site-3, run=simulate_job]: Initialized model <class 'nemo_nvflare.fed_megatron_gpt_prompt_learning_model.FedMegatronGPTPromptLearningModel'> and prompt encoder <class 'nemo.collections.nlp.modules.common.prompt_encoder.PromptEncoder'>\n",
      "2023-05-03 21:55:50,564 - PromptLearner - INFO - [identity=site-1, run=simulate_job]: Initialized model <class 'nemo_nvflare.fed_megatron_gpt_prompt_learning_model.FedMegatronGPTPromptLearningModel'> and prompt encoder <class 'nemo.collections.nlp.modules.common.prompt_encoder.PromptEncoder'>\n",
      "2023-05-03 21:55:50,564 - ClientRunner - INFO - [identity=site-3, run=simulate_job]: client runner started\n",
      "2023-05-03 21:55:50,564 - ClientTaskWorker - INFO - Initialize ClientRunner for client: site-3\n",
      "2023-05-03 21:55:50,565 - ClientRunner - INFO - [identity=site-1, run=simulate_job]: client runner started\n",
      "2023-05-03 21:55:50,565 - ClientTaskWorker - INFO - Initialize ClientRunner for client: site-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n",
      "Using pad_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n",
      "I0503 21:55:50.563429 140649257482048 fl_component.py:134] [identity=site-3, run=simulate_job]: Initialized model <class 'nemo_nvflare.fed_megatron_gpt_prompt_learning_model.FedMegatronGPTPromptLearningModel'> and prompt encoder <class 'nemo.collections.nlp.modules.common.prompt_encoder.PromptEncoder'>\n",
      "I0503 21:55:50.564302 140649257482048 fl_component.py:134] [identity=site-3, run=simulate_job]: client runner started\n",
      "I0503 21:55:50.564096 140522389899072 fl_component.py:134] [identity=site-1, run=simulate_job]: Initialized model <class 'nemo_nvflare.fed_megatron_gpt_prompt_learning_model.FedMegatronGPTPromptLearningModel'> and prompt encoder <class 'nemo.collections.nlp.modules.common.prompt_encoder.PromptEncoder'>\n",
      "I0503 21:55:50.564533 140649257482048 simulator_worker.py:85] Initialize ClientRunner for client: site-3\n",
      "I0503 21:55:50.565179 140522389899072 fl_component.py:134] [identity=site-1, run=simulate_job]: client runner started\n",
      "I0503 21:55:50.565428 140522389899072 simulator_worker.py:85] Initialize ClientRunner for client: site-1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-03 21:55:50,713 - GetTaskCommand - INFO - return task to client.  client_name: site-1  task_name: train   task_id: c9c177bd-ced9-4dc3-a7c0-39683ab84cff  sharable_header_task_id: c9c177bd-ced9-4dc3-a7c0-39683ab84cff\n",
      "2023-05-03 21:55:50,709 - Communicator - INFO - Received from simulator_server server  (16873478 Bytes). getTask: train time: 0.11954808235168457 seconds\n",
      "2023-05-03 21:55:50,710 - FederatedClient - INFO - pull_task completed. Task name:train Status:True \n",
      "2023-05-03 21:55:50,711 - ClientRunner - INFO - [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job]: got task assignment: name=train, id=372f6920-d35f-4934-bae0-efd9db3fbb7f\n",
      "2023-05-03 21:55:50,711 - ClientRunner - INFO - [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=372f6920-d35f-4934-bae0-efd9db3fbb7f]: invoking task executor <class 'nvflare.app_common.executors.learner_executor.LearnerExecutor'>\n",
      "2023-05-03 21:55:50,712 - LearnerExecutor - INFO - [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=372f6920-d35f-4934-bae0-efd9db3fbb7f]: Client trainer got task: train\n",
      "2023-05-03 21:55:50,719 - PromptLearner - INFO - [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=372f6920-d35f-4934-bae0-efd9db3fbb7f]: Loaded 7 of 7 weights\n",
      "2023-05-03 21:55:50,723 - lightning_fabric.utilities.distributed - INFO - Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "2023-05-03 21:55:50,726 - torch.distributed.distributed_c10d - INFO - Added key: store_based_barrier_key:1 to store for rank: 0\n",
      "2023-05-03 21:55:50,726 - torch.distributed.distributed_c10d - INFO - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.\n",
      "2023-05-03 21:55:50,726 - pytorch_lightning.utilities.rank_zero - INFO - ----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "2023-05-03 21:55:50,728 - torch.distributed.distributed_c10d - INFO - Added key: store_based_barrier_key:2 to store for rank: 0\n",
      "2023-05-03 21:55:50,728 - torch.distributed.distributed_c10d - INFO - Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 1 nodes.\n",
      "2023-05-03 21:55:50,729 - torch.distributed.distributed_c10d - INFO - Added key: store_based_barrier_key:3 to store for rank: 0\n",
      "2023-05-03 21:55:50,730 - torch.distributed.distributed_c10d - INFO - Rank 0: Completed store-based barrier for key:store_based_barrier_key:3 with 1 nodes.\n",
      "2023-05-03 21:55:50,731 - torch.distributed.distributed_c10d - INFO - Added key: store_based_barrier_key:4 to store for rank: 0\n",
      "2023-05-03 21:55:50,731 - torch.distributed.distributed_c10d - INFO - Rank 0: Completed store-based barrier for key:store_based_barrier_key:4 with 1 nodes.\n",
      "2023-05-03 21:55:50,732 - torch.distributed.distributed_c10d - INFO - Added key: store_based_barrier_key:5 to store for rank: 0\n",
      "2023-05-03 21:55:50,732 - torch.distributed.distributed_c10d - INFO - Rank 0: Completed store-based barrier for key:store_based_barrier_key:5 with 1 nodes.\n",
      "2023-05-03 21:55:50,733 - torch.distributed.distributed_c10d - INFO - Added key: store_based_barrier_key:6 to store for rank: 0\n",
      "2023-05-03 21:55:50,734 - torch.distributed.distributed_c10d - INFO - Rank 0: Completed store-based barrier for key:store_based_barrier_key:6 with 1 nodes.\n",
      "2023-05-03 21:55:50,735 - torch.distributed.distributed_c10d - INFO - Added key: store_based_barrier_key:7 to store for rank: 0\n",
      "2023-05-03 21:55:50,735 - torch.distributed.distributed_c10d - INFO - Rank 0: Completed store-based barrier for key:store_based_barrier_key:7 with 1 nodes.\n",
      "2023-05-03 21:55:50,736 - torch.distributed.distributed_c10d - INFO - Added key: store_based_barrier_key:8 to store for rank: 0\n",
      "2023-05-03 21:55:50,736 - torch.distributed.distributed_c10d - INFO - Rank 0: Completed store-based barrier for key:store_based_barrier_key:8 with 1 nodes.\n",
      "2023-05-03 21:55:50,737 - torch.distributed.distributed_c10d - INFO - Added key: store_based_barrier_key:9 to store for rank: 0\n",
      "2023-05-03 21:55:50,737 - torch.distributed.distributed_c10d - INFO - Rank 0: Completed store-based barrier for key:store_based_barrier_key:9 with 1 nodes.\n",
      "2023-05-03 21:55:50,738 - pytorch_lightning.loggers.tensorboard - WARNING - Missing logger folder: /tmp/nvflare/nemo/gpt_p-tuning_local/simulate_job/app_site-3/lightning_logs\n",
      "2023-05-03 21:55:50,791 - Communicator - INFO - Received from simulator_server server  (16873478 Bytes). getTask: train time: 0.1997835636138916 seconds\n",
      "2023-05-03 21:55:50,793 - FederatedClient - INFO - pull_task completed. Task name:train Status:True \n",
      "2023-05-03 21:55:50,793 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job]: got task assignment: name=train, id=c9c177bd-ced9-4dc3-a7c0-39683ab84cff\n",
      "2023-05-03 21:55:50,794 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=c9c177bd-ced9-4dc3-a7c0-39683ab84cff]: invoking task executor <class 'nvflare.app_common.executors.learner_executor.LearnerExecutor'>\n",
      "2023-05-03 21:55:50,794 - LearnerExecutor - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=c9c177bd-ced9-4dc3-a7c0-39683ab84cff]: Client trainer got task: train\n",
      "2023-05-03 21:55:50,801 - PromptLearner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=c9c177bd-ced9-4dc3-a7c0-39683ab84cff]: Loaded 7 of 7 weights\n",
      "2023-05-03 21:55:50,806 - lightning_fabric.utilities.distributed - INFO - Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "2023-05-03 21:55:50,809 - torch.distributed.distributed_c10d - INFO - Added key: store_based_barrier_key:1 to store for rank: 0\n",
      "2023-05-03 21:55:50,810 - torch.distributed.distributed_c10d - INFO - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.\n",
      "2023-05-03 21:55:50,810 - pytorch_lightning.utilities.rank_zero - INFO - ----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "2023-05-03 21:55:50,812 - torch.distributed.distributed_c10d - INFO - Added key: store_based_barrier_key:2 to store for rank: 0\n",
      "2023-05-03 21:55:50,812 - torch.distributed.distributed_c10d - INFO - Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 1 nodes.\n",
      "2023-05-03 21:55:50,813 - torch.distributed.distributed_c10d - INFO - Added key: store_based_barrier_key:3 to store for rank: 0\n",
      "2023-05-03 21:55:50,813 - torch.distributed.distributed_c10d - INFO - Rank 0: Completed store-based barrier for key:store_based_barrier_key:3 with 1 nodes.\n",
      "2023-05-03 21:55:50,814 - torch.distributed.distributed_c10d - INFO - Added key: store_based_barrier_key:4 to store for rank: 0\n",
      "2023-05-03 21:55:50,815 - torch.distributed.distributed_c10d - INFO - Rank 0: Completed store-based barrier for key:store_based_barrier_key:4 with 1 nodes.\n",
      "2023-05-03 21:55:50,816 - torch.distributed.distributed_c10d - INFO - Added key: store_based_barrier_key:5 to store for rank: 0\n",
      "2023-05-03 21:55:50,816 - torch.distributed.distributed_c10d - INFO - Rank 0: Completed store-based barrier for key:store_based_barrier_key:5 with 1 nodes.\n",
      "2023-05-03 21:55:50,817 - torch.distributed.distributed_c10d - INFO - Added key: store_based_barrier_key:6 to store for rank: 0\n",
      "2023-05-03 21:55:50,817 - torch.distributed.distributed_c10d - INFO - Rank 0: Completed store-based barrier for key:store_based_barrier_key:6 with 1 nodes.\n",
      "2023-05-03 21:55:50,818 - torch.distributed.distributed_c10d - INFO - Added key: store_based_barrier_key:7 to store for rank: 0\n",
      "2023-05-03 21:55:50,818 - torch.distributed.distributed_c10d - INFO - Rank 0: Completed store-based barrier for key:store_based_barrier_key:7 with 1 nodes.\n",
      "2023-05-03 21:55:50,820 - torch.distributed.distributed_c10d - INFO - Added key: store_based_barrier_key:8 to store for rank: 0\n",
      "2023-05-03 21:55:50,820 - torch.distributed.distributed_c10d - INFO - Rank 0: Completed store-based barrier for key:store_based_barrier_key:8 with 1 nodes.\n",
      "2023-05-03 21:55:50,821 - torch.distributed.distributed_c10d - INFO - Added key: store_based_barrier_key:9 to store for rank: 0\n",
      "2023-05-03 21:55:50,821 - torch.distributed.distributed_c10d - INFO - Rank 0: Completed store-based barrier for key:store_based_barrier_key:9 with 1 nodes.\n",
      "2023-05-03 21:55:50,822 - pytorch_lightning.loggers.tensorboard - WARNING - Missing logger folder: /tmp/nvflare/nemo/gpt_p-tuning_local/simulate_job/app_site-1/lightning_logs\n",
      "[NeMo I 2023-05-03 21:55:50 megatron_base_model:205] Padded vocab_size: 50304, original vocab_size: 50257, dummy tokens: 47.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 21:55:50.709390 140635516434176 communicator.py:200] Received from simulator_server server  (16873478 Bytes). getTask: train time: 0.11954808235168457 seconds\n",
      "I0503 21:55:50.710824 140649257482048 fed_client.py:91] pull_task completed. Task name:train Status:True \n",
      "I0503 21:55:50.711157 140649257482048 fl_component.py:134] [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job]: got task assignment: name=train, id=372f6920-d35f-4934-bae0-efd9db3fbb7f\n",
      "I0503 21:55:50.711871 140649257482048 fl_component.py:134] [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=372f6920-d35f-4934-bae0-efd9db3fbb7f]: invoking task executor <class 'nvflare.app_common.executors.learner_executor.LearnerExecutor'>\n",
      "I0503 21:55:50.712136 140649257482048 fl_component.py:134] [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=372f6920-d35f-4934-bae0-efd9db3fbb7f]: Client trainer got task: train\n",
      "I0503 21:55:50.719060 140649257482048 fl_component.py:134] [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=372f6920-d35f-4934-bae0-efd9db3fbb7f]: Loaded 7 of 7 weights\n",
      "I0503 21:55:50.723367 140649257482048 distributed.py:244] Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "I0503 21:55:50.726253 140649257482048 distributed_c10d.py:393] Added key: store_based_barrier_key:1 to store for rank: 0\n",
      "I0503 21:55:50.726523 140649257482048 distributed_c10d.py:427] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.\n",
      "I0503 21:55:50.726794 140649257482048 distributed.py:248] ----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "I0503 21:55:50.728565 140649257482048 distributed_c10d.py:393] Added key: store_based_barrier_key:2 to store for rank: 0\n",
      "I0503 21:55:50.728828 140649257482048 distributed_c10d.py:427] Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 1 nodes.\n",
      "I0503 21:55:50.729912 140649257482048 distributed_c10d.py:393] Added key: store_based_barrier_key:3 to store for rank: 0\n",
      "I0503 21:55:50.730168 140649257482048 distributed_c10d.py:427] Rank 0: Completed store-based barrier for key:store_based_barrier_key:3 with 1 nodes.\n",
      "I0503 21:55:50.731200 140649257482048 distributed_c10d.py:393] Added key: store_based_barrier_key:4 to store for rank: 0\n",
      "I0503 21:55:50.731450 140649257482048 distributed_c10d.py:427] Rank 0: Completed store-based barrier for key:store_based_barrier_key:4 with 1 nodes.\n",
      "I0503 21:55:50.732476 140649257482048 distributed_c10d.py:393] Added key: store_based_barrier_key:5 to store for rank: 0\n",
      "I0503 21:55:50.732723 140649257482048 distributed_c10d.py:427] Rank 0: Completed store-based barrier for key:store_based_barrier_key:5 with 1 nodes.\n",
      "I0503 21:55:50.733770 140649257482048 distributed_c10d.py:393] Added key: store_based_barrier_key:6 to store for rank: 0\n",
      "I0503 21:55:50.734023 140649257482048 distributed_c10d.py:427] Rank 0: Completed store-based barrier for key:store_based_barrier_key:6 with 1 nodes.\n",
      "I0503 21:55:50.735060 140649257482048 distributed_c10d.py:393] Added key: store_based_barrier_key:7 to store for rank: 0\n",
      "I0503 21:55:50.735310 140649257482048 distributed_c10d.py:427] Rank 0: Completed store-based barrier for key:store_based_barrier_key:7 with 1 nodes.\n",
      "I0503 21:55:50.736334 140649257482048 distributed_c10d.py:393] Added key: store_based_barrier_key:8 to store for rank: 0\n",
      "I0503 21:55:50.736585 140649257482048 distributed_c10d.py:427] Rank 0: Completed store-based barrier for key:store_based_barrier_key:8 with 1 nodes.\n",
      "I0503 21:55:50.737620 140649257482048 distributed_c10d.py:393] Added key: store_based_barrier_key:9 to store for rank: 0\n",
      "I0503 21:55:50.737886 140649257482048 distributed_c10d.py:427] Rank 0: Completed store-based barrier for key:store_based_barrier_key:9 with 1 nodes.\n",
      "W0503 21:55:50.738517 140649257482048 tensorboard.py:237] Missing logger folder: /tmp/nvflare/nemo/gpt_p-tuning_local/simulate_job/app_site-3/lightning_logs\n",
      "I0503 21:55:50.791913 140504285050624 communicator.py:200] Received from simulator_server server  (16873478 Bytes). getTask: train time: 0.1997835636138916 seconds\n",
      "I0503 21:55:50.793385 140522389899072 fed_client.py:91] pull_task completed. Task name:train Status:True \n",
      "I0503 21:55:50.793715 140522389899072 fl_component.py:134] [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job]: got task assignment: name=train, id=c9c177bd-ced9-4dc3-a7c0-39683ab84cff\n",
      "I0503 21:55:50.794510 140522389899072 fl_component.py:134] [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=c9c177bd-ced9-4dc3-a7c0-39683ab84cff]: invoking task executor <class 'nvflare.app_common.executors.learner_executor.LearnerExecutor'>\n",
      "I0503 21:55:50.794794 140522389899072 fl_component.py:134] [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=c9c177bd-ced9-4dc3-a7c0-39683ab84cff]: Client trainer got task: train\n",
      "I0503 21:55:50.801733 140522389899072 fl_component.py:134] [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=c9c177bd-ced9-4dc3-a7c0-39683ab84cff]: Loaded 7 of 7 weights\n",
      "I0503 21:55:50.806706 140522389899072 distributed.py:244] Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "I0503 21:55:50.809785 140522389899072 distributed_c10d.py:393] Added key: store_based_barrier_key:1 to store for rank: 0\n",
      "I0503 21:55:50.810059 140522389899072 distributed_c10d.py:427] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.\n",
      "I0503 21:55:50.810339 140522389899072 distributed.py:248] ----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "I0503 21:55:50.812262 140522389899072 distributed_c10d.py:393] Added key: store_based_barrier_key:2 to store for rank: 0\n",
      "I0503 21:55:50.812524 140522389899072 distributed_c10d.py:427] Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 1 nodes.\n",
      "I0503 21:55:50.813558 140522389899072 distributed_c10d.py:393] Added key: store_based_barrier_key:3 to store for rank: 0\n",
      "I0503 21:55:50.813827 140522389899072 distributed_c10d.py:427] Rank 0: Completed store-based barrier for key:store_based_barrier_key:3 with 1 nodes.\n",
      "I0503 21:55:50.814840 140522389899072 distributed_c10d.py:393] Added key: store_based_barrier_key:4 to store for rank: 0\n",
      "I0503 21:55:50.815090 140522389899072 distributed_c10d.py:427] Rank 0: Completed store-based barrier for key:store_based_barrier_key:4 with 1 nodes.\n",
      "I0503 21:55:50.816125 140522389899072 distributed_c10d.py:393] Added key: store_based_barrier_key:5 to store for rank: 0\n",
      "I0503 21:55:50.816373 140522389899072 distributed_c10d.py:427] Rank 0: Completed store-based barrier for key:store_based_barrier_key:5 with 1 nodes.\n",
      "I0503 21:55:50.817403 140522389899072 distributed_c10d.py:393] Added key: store_based_barrier_key:6 to store for rank: 0\n",
      "I0503 21:55:50.817666 140522389899072 distributed_c10d.py:427] Rank 0: Completed store-based barrier for key:store_based_barrier_key:6 with 1 nodes.\n",
      "I0503 21:55:50.818739 140522389899072 distributed_c10d.py:393] Added key: store_based_barrier_key:7 to store for rank: 0\n",
      "I0503 21:55:50.818994 140522389899072 distributed_c10d.py:427] Rank 0: Completed store-based barrier for key:store_based_barrier_key:7 with 1 nodes.\n",
      "I0503 21:55:50.820034 140522389899072 distributed_c10d.py:393] Added key: store_based_barrier_key:8 to store for rank: 0\n",
      "I0503 21:55:50.820290 140522389899072 distributed_c10d.py:427] Rank 0: Completed store-based barrier for key:store_based_barrier_key:8 with 1 nodes.\n",
      "I0503 21:55:50.821327 140522389899072 distributed_c10d.py:393] Added key: store_based_barrier_key:9 to store for rank: 0\n",
      "I0503 21:55:50.821580 140522389899072 distributed_c10d.py:427] Rank 0: Completed store-based barrier for key:store_based_barrier_key:9 with 1 nodes.\n",
      "W0503 21:55:50.822228 140522389899072 tensorboard.py:237] Missing logger folder: /tmp/nvflare/nemo/gpt_p-tuning_local/simulate_job/app_site-1/lightning_logs\n",
      "Using sep_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using pad_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-05-03 21:55:51 gpt_prompt_learning_dataset:85] Loading and tokenizing dataset ... \n",
      "[NeMo I 2023-05-03 21:55:51 gpt_prompt_learning_dataset:85] Loading and tokenizing dataset ... \n",
      "[NeMo I 2023-05-03 21:55:51 nlp_overrides:374] Model MegatronGPTModel was successfully restored from /workspace/Code/nvflare/nemo_nvflare/integration/nemo/examples/prompt_learning/megatron_gpt_345m.nemo.\n",
      "[NeMo I 2023-05-03 21:55:52 auto_tokenizer:172] 10 special tokens added, resize your model accordingly.\n",
      "2023-05-03 21:55:52,065 - PromptLearner - INFO - [identity=site-2, run=simulate_job]: Initialized model <class 'nemo_nvflare.fed_megatron_gpt_prompt_learning_model.FedMegatronGPTPromptLearningModel'> and prompt encoder <class 'nemo.collections.nlp.modules.common.prompt_encoder.PromptEncoder'>\n",
      "2023-05-03 21:55:52,066 - ClientRunner - INFO - [identity=site-2, run=simulate_job]: client runner started\n",
      "2023-05-03 21:55:52,066 - ClientTaskWorker - INFO - Initialize ClientRunner for client: site-2\n",
      "2023-05-03 21:55:52,074 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-2, peer_run=simulate_job, task_name=train, task_id=f5aa2c14-c507-4451-8d91-362c9d433102]: assigned task to client site-2: name=train, id=f5aa2c14-c507-4451-8d91-362c9d433102\n",
      "2023-05-03 21:55:52,075 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-2, peer_run=simulate_job, task_name=train, task_id=f5aa2c14-c507-4451-8d91-362c9d433102]: sent task assignment to client. client_name:site-2 task_id:f5aa2c14-c507-4451-8d91-362c9d433102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "70it [00:00, 687.54it/s]Using pad_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n",
      "62it [00:00, 615.65it/s]I0503 21:55:52.065337 139755592410944 fl_component.py:134] [identity=site-2, run=simulate_job]: Initialized model <class 'nemo_nvflare.fed_megatron_gpt_prompt_learning_model.FedMegatronGPTPromptLearningModel'> and prompt encoder <class 'nemo.collections.nlp.modules.common.prompt_encoder.PromptEncoder'>\n",
      "I0503 21:55:52.066189 139755592410944 fl_component.py:134] [identity=site-2, run=simulate_job]: client runner started\n",
      "I0503 21:55:52.066437 139755592410944 simulator_worker.py:85] Initialize ClientRunner for client: site-2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-03 21:55:52,128 - GetTaskCommand - INFO - return task to client.  client_name: site-2  task_name: train   task_id: f5aa2c14-c507-4451-8d91-362c9d433102  sharable_header_task_id: f5aa2c14-c507-4451-8d91-362c9d433102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "218it [00:00, 726.46it/s]I0503 21:55:52.204099 139737599833856 communicator.py:200] Received from simulator_server server  (16873478 Bytes). getTask: train time: 0.1115117073059082 seconds\n",
      "I0503 21:55:52.206012 139755592410944 fed_client.py:91] pull_task completed. Task name:train Status:True \n",
      "I0503 21:55:52.206760 139755592410944 fl_component.py:134] [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job]: got task assignment: name=train, id=f5aa2c14-c507-4451-8d91-362c9d433102\n",
      "I0503 21:55:52.207579 139755592410944 fl_component.py:134] [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=f5aa2c14-c507-4451-8d91-362c9d433102]: invoking task executor <class 'nvflare.app_common.executors.learner_executor.LearnerExecutor'>\n",
      "I0503 21:55:52.207908 139755592410944 fl_component.py:134] [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=f5aa2c14-c507-4451-8d91-362c9d433102]: Client trainer got task: train\n",
      "I0503 21:55:52.214981 139755592410944 fl_component.py:134] [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=f5aa2c14-c507-4451-8d91-362c9d433102]: Loaded 7 of 7 weights\n",
      "I0503 21:55:52.219283 139755592410944 distributed.py:244] Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "I0503 21:55:52.222234 139755592410944 distributed_c10d.py:393] Added key: store_based_barrier_key:1 to store for rank: 0\n",
      "I0503 21:55:52.222495 139755592410944 distributed_c10d.py:427] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.\n",
      "I0503 21:55:52.222805 139755592410944 distributed.py:248] ----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "I0503 21:55:52.224601 139755592410944 distributed_c10d.py:393] Added key: store_based_barrier_key:2 to store for rank: 0\n",
      "I0503 21:55:52.224853 139755592410944 distributed_c10d.py:427] Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 1 nodes.\n",
      "I0503 21:55:52.225882 139755592410944 distributed_c10d.py:393] Added key: store_based_barrier_key:3 to store for rank: 0\n",
      "I0503 21:55:52.226126 139755592410944 distributed_c10d.py:427] Rank 0: Completed store-based barrier for key:store_based_barrier_key:3 with 1 nodes.\n",
      "I0503 21:55:52.227134 139755592410944 distributed_c10d.py:393] Added key: store_based_barrier_key:4 to store for rank: 0\n",
      "I0503 21:55:52.227374 139755592410944 distributed_c10d.py:427] Rank 0: Completed store-based barrier for key:store_based_barrier_key:4 with 1 nodes.\n",
      "I0503 21:55:52.228374 139755592410944 distributed_c10d.py:393] Added key: store_based_barrier_key:5 to store for rank: 0\n",
      "I0503 21:55:52.228613 139755592410944 distributed_c10d.py:427] Rank 0: Completed store-based barrier for key:store_based_barrier_key:5 with 1 nodes.\n",
      "I0503 21:55:52.229630 139755592410944 distributed_c10d.py:393] Added key: store_based_barrier_key:6 to store for rank: 0\n",
      "I0503 21:55:52.229886 139755592410944 distributed_c10d.py:427] Rank 0: Completed store-based barrier for key:store_based_barrier_key:6 with 1 nodes.\n",
      "I0503 21:55:52.230924 139755592410944 distributed_c10d.py:393] Added key: store_based_barrier_key:7 to store for rank: 0\n",
      "I0503 21:55:52.231167 139755592410944 distributed_c10d.py:427] Rank 0: Completed store-based barrier for key:store_based_barrier_key:7 with 1 nodes.\n",
      "I0503 21:55:52.232182 139755592410944 distributed_c10d.py:393] Added key: store_based_barrier_key:8 to store for rank: 0\n",
      "I0503 21:55:52.232424 139755592410944 distributed_c10d.py:427] Rank 0: Completed store-based barrier for key:store_based_barrier_key:8 with 1 nodes.\n",
      "I0503 21:55:52.233420 139755592410944 distributed_c10d.py:393] Added key: store_based_barrier_key:9 to store for rank: 0\n",
      "I0503 21:55:52.233669 139755592410944 distributed_c10d.py:427] Rank 0: Completed store-based barrier for key:store_based_barrier_key:9 with 1 nodes.\n",
      "W0503 21:55:52.234266 139755592410944 tensorboard.py:237] Missing logger folder: /tmp/nvflare/nemo/gpt_p-tuning_local/simulate_job/app_site-2/lightning_logs\n",
      "300it [00:00, 762.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-03 21:55:52,204 - Communicator - INFO - Received from simulator_server server  (16873478 Bytes). getTask: train time: 0.1115117073059082 seconds\n",
      "2023-05-03 21:55:52,206 - FederatedClient - INFO - pull_task completed. Task name:train Status:True \n",
      "2023-05-03 21:55:52,206 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job]: got task assignment: name=train, id=f5aa2c14-c507-4451-8d91-362c9d433102\n",
      "2023-05-03 21:55:52,207 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=f5aa2c14-c507-4451-8d91-362c9d433102]: invoking task executor <class 'nvflare.app_common.executors.learner_executor.LearnerExecutor'>\n",
      "2023-05-03 21:55:52,207 - LearnerExecutor - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=f5aa2c14-c507-4451-8d91-362c9d433102]: Client trainer got task: train\n",
      "2023-05-03 21:55:52,214 - PromptLearner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=f5aa2c14-c507-4451-8d91-362c9d433102]: Loaded 7 of 7 weights\n",
      "2023-05-03 21:55:52,219 - lightning_fabric.utilities.distributed - INFO - Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "2023-05-03 21:55:52,222 - torch.distributed.distributed_c10d - INFO - Added key: store_based_barrier_key:1 to store for rank: 0\n",
      "2023-05-03 21:55:52,222 - torch.distributed.distributed_c10d - INFO - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.\n",
      "2023-05-03 21:55:52,222 - pytorch_lightning.utilities.rank_zero - INFO - ----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "2023-05-03 21:55:52,224 - torch.distributed.distributed_c10d - INFO - Added key: store_based_barrier_key:2 to store for rank: 0\n",
      "2023-05-03 21:55:52,224 - torch.distributed.distributed_c10d - INFO - Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 1 nodes.\n",
      "2023-05-03 21:55:52,225 - torch.distributed.distributed_c10d - INFO - Added key: store_based_barrier_key:3 to store for rank: 0\n",
      "2023-05-03 21:55:52,226 - torch.distributed.distributed_c10d - INFO - Rank 0: Completed store-based barrier for key:store_based_barrier_key:3 with 1 nodes.\n",
      "2023-05-03 21:55:52,227 - torch.distributed.distributed_c10d - INFO - Added key: store_based_barrier_key:4 to store for rank: 0\n",
      "2023-05-03 21:55:52,227 - torch.distributed.distributed_c10d - INFO - Rank 0: Completed store-based barrier for key:store_based_barrier_key:4 with 1 nodes.\n",
      "2023-05-03 21:55:52,228 - torch.distributed.distributed_c10d - INFO - Added key: store_based_barrier_key:5 to store for rank: 0\n",
      "2023-05-03 21:55:52,228 - torch.distributed.distributed_c10d - INFO - Rank 0: Completed store-based barrier for key:store_based_barrier_key:5 with 1 nodes.\n",
      "2023-05-03 21:55:52,229 - torch.distributed.distributed_c10d - INFO - Added key: store_based_barrier_key:6 to store for rank: 0\n",
      "2023-05-03 21:55:52,229 - torch.distributed.distributed_c10d - INFO - Rank 0: Completed store-based barrier for key:store_based_barrier_key:6 with 1 nodes.\n",
      "2023-05-03 21:55:52,230 - torch.distributed.distributed_c10d - INFO - Added key: store_based_barrier_key:7 to store for rank: 0\n",
      "2023-05-03 21:55:52,231 - torch.distributed.distributed_c10d - INFO - Rank 0: Completed store-based barrier for key:store_based_barrier_key:7 with 1 nodes.\n",
      "2023-05-03 21:55:52,232 - torch.distributed.distributed_c10d - INFO - Added key: store_based_barrier_key:8 to store for rank: 0\n",
      "2023-05-03 21:55:52,232 - torch.distributed.distributed_c10d - INFO - Rank 0: Completed store-based barrier for key:store_based_barrier_key:8 with 1 nodes.\n",
      "2023-05-03 21:55:52,233 - torch.distributed.distributed_c10d - INFO - Added key: store_based_barrier_key:9 to store for rank: 0\n",
      "2023-05-03 21:55:52,233 - torch.distributed.distributed_c10d - INFO - Rank 0: Completed store-based barrier for key:store_based_barrier_key:9 with 1 nodes.\n",
      "2023-05-03 21:55:52,234 - pytorch_lightning.loggers.tensorboard - WARNING - Missing logger folder: /tmp/nvflare/nemo/gpt_p-tuning_local/simulate_job/app_site-2/lightning_logs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "603it [00:00, 804.07it/s]\n",
      "604it [00:00, 796.94it/s]\n",
      "85it [00:00, 847.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-05-03 21:55:52 gpt_prompt_learning_dataset:196] Skipped 0 sentences, sequence length too short or too long even after truncation\n",
      "[NeMo I 2023-05-03 21:55:52 gpt_prompt_learning_dataset:85] Loading and tokenizing dataset ... \n",
      "[NeMo I 2023-05-03 21:55:52 gpt_prompt_learning_dataset:196] Skipped 0 sentences, sequence length too short or too long even after truncation\n",
      "[NeMo I 2023-05-03 21:55:52 gpt_prompt_learning_dataset:85] Loading and tokenizing dataset ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "226it [00:00, 859.33it/s]\n",
      "I0503 21:55:52.904081 140649257482048 cuda.py:58] LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "226it [00:00, 878.10it/s]\n",
      "I0503 21:55:52.989405 140522389899072 cuda.py:58] LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[NeMo W 2023-05-03 21:55:53 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/apex/transformer/pipeline_parallel/utils.py:81: UserWarning: This function is only for unittest\n",
      "      warnings.warn(\"This function is only for unittest\")\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-05-03 21:55:52 gpt_prompt_learning_dataset:196] Skipped 0 sentences, sequence length too short or too long even after truncation\n",
      "2023-05-03 21:55:52,904 - pytorch_lightning.accelerators.cuda - INFO - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[NeMo I 2023-05-03 21:55:52 gpt_prompt_learning_dataset:196] Skipped 0 sentences, sequence length too short or too long even after truncation\n",
      "2023-05-03 21:55:52,989 - pytorch_lightning.accelerators.cuda - INFO - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Validation: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-05-03 21:55:53 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/apex/transformer/pipeline_parallel/utils.py:81: UserWarning: This function is only for unittest\n",
      "      warnings.warn(\"This function is only for unittest\")\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0:   0%|          | 0/29 [00:00<?, ?it/s][NeMo I 2023-05-03 21:55:53 gpt_prompt_learning_dataset:85] Loading and tokenizing dataset ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [00:00, 434.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0:   0%|          | 0/29 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "604it [00:00, 742.96it/s]\n",
      "90it [00:00, 887.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-05-03 21:55:54 gpt_prompt_learning_dataset:196] Skipped 0 sentences, sequence length too short or too long even after truncation\n",
      "[NeMo I 2023-05-03 21:55:54 gpt_prompt_learning_dataset:85] Loading and tokenizing dataset ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "226it [00:00, 907.89it/s]\n",
      "I0503 21:55:55.066386 139755592410944 cuda.py:58] LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[NeMo W 2023-05-03 21:55:55 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/apex/transformer/pipeline_parallel/utils.py:81: UserWarning: This function is only for unittest\n",
      "      warnings.warn(\"This function is only for unittest\")\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-05-03 21:55:55 gpt_prompt_learning_dataset:196] Skipped 0 sentences, sequence length too short or too long even after truncation\n",
      "2023-05-03 21:55:55,066 - pytorch_lightning.accelerators.cuda - INFO - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Validation DataLoader 0: 100%|██████████| 29/29 [00:06<00:00,  4.22it/s]2023-05-03 21:56:00,705 - root - INFO - global_model_val_loss: 9.124295234680176\n",
      "Validation DataLoader 0: 100%|██████████| 29/29 [00:06<00:00,  4.21it/s]\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1m     Validate metric     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
      "│\u001b[36m \u001b[0m\u001b[36m  global_model_val_loss  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    9.124295234680176    \u001b[0m\u001b[35m \u001b[0m│\n",
      "└───────────────────────────┴───────────────────────────┘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 21:56:00.705590 140649257482048 fed_megatron_gpt_prompt_learning_model.py:99] global_model_val_loss: 9.124295234680176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0:  62%|██████▏   | 18/29 [00:05<00:03,  3.32it/s]2023-05-03 21:56:01,259 - PromptLearner - INFO - [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=372f6920-d35f-4934-bae0-efd9db3fbb7f]: Global_model global_model_val_loss: 9.124295234680176\n",
      "2023-05-03 21:56:01,259 - PromptLearner - INFO - [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=372f6920-d35f-4934-bae0-efd9db3fbb7f]: Current/Total Round: 1/1\n",
      "2023-05-03 21:56:01,260 - PromptLearner - INFO - [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=372f6920-d35f-4934-bae0-efd9db3fbb7f]: Client identity: site-3\n",
      "2023-05-03 21:56:01,268 - PromptLearner - INFO - [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=372f6920-d35f-4934-bae0-efd9db3fbb7f]: Loaded 7 of 7 weights\n",
      "2023-05-03 21:56:01,269 - PromptLearner - INFO - [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=372f6920-d35f-4934-bae0-efd9db3fbb7f]: Start training in round 0\n",
      "2023-05-03 21:56:01,275 - pytorch_lightning.accelerators.cuda - INFO - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Validation DataLoader 0:  66%|██████▌   | 19/29 [00:05<00:02,  3.43it/s]2023-05-03 21:56:01,370 - root - INFO - global_model_val_loss: 9.124295234680176\n",
      "Validation DataLoader 0: 100%|██████████| 29/29 [00:07<00:00,  4.00it/s]\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1m     Validate metric     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
      "│\u001b[36m \u001b[0m\u001b[36m  global_model_val_loss  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    9.124295234680176    \u001b[0m\u001b[35m \u001b[0m│\n",
      "└───────────────────────────┴───────────────────────────┘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 21:56:01.259375 140649257482048 fl_component.py:134] [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=372f6920-d35f-4934-bae0-efd9db3fbb7f]: Global_model global_model_val_loss: 9.124295234680176\n",
      "I0503 21:56:01.259870 140649257482048 fl_component.py:134] [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=372f6920-d35f-4934-bae0-efd9db3fbb7f]: Current/Total Round: 1/1\n",
      "I0503 21:56:01.260066 140649257482048 fl_component.py:134] [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=372f6920-d35f-4934-bae0-efd9db3fbb7f]: Client identity: site-3\n",
      "I0503 21:56:01.268986 140649257482048 fl_component.py:134] [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=372f6920-d35f-4934-bae0-efd9db3fbb7f]: Loaded 7 of 7 weights\n",
      "I0503 21:56:01.269212 140649257482048 fl_component.py:134] [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=372f6920-d35f-4934-bae0-efd9db3fbb7f]: Start training in round 0\n",
      "I0503 21:56:01.275575 140649257482048 cuda.py:58] LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "I0503 21:56:01.370312 140522389899072 fed_megatron_gpt_prompt_learning_model.py:99] global_model_val_loss: 9.124295234680176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-05-03 21:56:01 nlp_overrides:105] Configuring DDP for model parallelism.\n",
      "Validation DataLoader 0:  69%|██████▉   | 20/29 [00:05<00:02,  3.54it/s][NeMo I 2023-05-03 21:56:01 modelPT:722] Optimizer config = FusedAdam (\n",
      "    Parameter Group 0\n",
      "        betas: [0.9, 0.98]\n",
      "        bias_correction: True\n",
      "        eps: 1e-08\n",
      "        lr: 0.0001\n",
      "        weight_decay: 0.01\n",
      "    )\n",
      "[NeMo I 2023-05-03 21:56:01 lr_scheduler:910] Scheduler \"<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x7feb31484040>\" \n",
      "    will be used during training (effective maximum steps = 3750) - \n",
      "    Parameters : \n",
      "    (warmup_steps: 50\n",
      "    min_lr: 0.0\n",
      "    constant_steps: 0\n",
      "    max_steps: 3750\n",
      "    )\n",
      "2023-05-03 21:56:01,507 - pytorch_lightning.callbacks.model_summary - INFO - \n",
      "  | Name            | Type                   | Params\n",
      "-----------------------------------------------------------\n",
      "0 | frozen_model    | MegatronGPTModel       | 354 M \n",
      "1 | word_embeddings | VocabParallelEmbedding | 51.5 M\n",
      "2 | prompt_encoder  | PromptEncoder          | 4.2 M \n",
      "-----------------------------------------------------------\n",
      "4.2 M     Trainable params\n",
      "354 M     Non-trainable params\n",
      "359 M     Total params\n",
      "718.178   Total estimated model params size (MB)\n",
      "Validation DataLoader 0:  72%|███████▏  | 21/29 [00:05<00:02,  3.64it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 21:56:01.507802 140649257482048 model_summary.py:83] \n",
      "  | Name            | Type                   | Params\n",
      "-----------------------------------------------------------\n",
      "0 | frozen_model    | MegatronGPTModel       | 354 M \n",
      "1 | word_embeddings | VocabParallelEmbedding | 51.5 M\n",
      "2 | prompt_encoder  | PromptEncoder          | 4.2 M \n",
      "-----------------------------------------------------------\n",
      "4.2 M     Trainable params\n",
      "354 M     Non-trainable params\n",
      "359 M     Total params\n",
      "718.178   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0:  83%|████████▎ | 24/29 [00:06<00:01,  3.92it/s]2023-05-03 21:56:01,964 - PromptLearner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=c9c177bd-ced9-4dc3-a7c0-39683ab84cff]: Global_model global_model_val_loss: 9.124295234680176\n",
      "2023-05-03 21:56:01,965 - PromptLearner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=c9c177bd-ced9-4dc3-a7c0-39683ab84cff]: Current/Total Round: 1/1\n",
      "2023-05-03 21:56:01,965 - PromptLearner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=c9c177bd-ced9-4dc3-a7c0-39683ab84cff]: Client identity: site-1\n",
      "2023-05-03 21:56:01,973 - PromptLearner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=c9c177bd-ced9-4dc3-a7c0-39683ab84cff]: Loaded 7 of 7 weights\n",
      "2023-05-03 21:56:01,973 - PromptLearner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=c9c177bd-ced9-4dc3-a7c0-39683ab84cff]: Start training in round 0\n",
      "2023-05-03 21:56:01,980 - pytorch_lightning.accelerators.cuda - INFO - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Validation DataLoader 0:  86%|████████▌ | 25/29 [00:06<00:00,  4.01it/s]/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 21:56:01.964596 140522389899072 fl_component.py:134] [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=c9c177bd-ced9-4dc3-a7c0-39683ab84cff]: Global_model global_model_val_loss: 9.124295234680176\n",
      "I0503 21:56:01.965180 140522389899072 fl_component.py:134] [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=c9c177bd-ced9-4dc3-a7c0-39683ab84cff]: Current/Total Round: 1/1\n",
      "I0503 21:56:01.965396 140522389899072 fl_component.py:134] [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=c9c177bd-ced9-4dc3-a7c0-39683ab84cff]: Client identity: site-1\n",
      "I0503 21:56:01.973066 140522389899072 fl_component.py:134] [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=c9c177bd-ced9-4dc3-a7c0-39683ab84cff]: Loaded 7 of 7 weights\n",
      "I0503 21:56:01.973309 140522389899072 fl_component.py:134] [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=c9c177bd-ced9-4dc3-a7c0-39683ab84cff]: Start training in round 0\n",
      "I0503 21:56:01.980267 140522389899072 cuda.py:58] LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00,  6.25it/s][NeMo I 2023-05-03 21:56:02 nlp_overrides:105] Configuring DDP for model parallelism.\n",
      "2023-05-03 21:56:02,177 - root - INFO - val_loss: 6.063295364379883\n",
      "Epoch 0:   0%|          | 0/104 [00:00<?, ?it/s] [NeMo I 2023-05-03 21:56:02 modelPT:722] Optimizer config = FusedAdam (\n",
      "    Parameter Group 0\n",
      "        betas: [0.9, 0.98]\n",
      "        bias_correction: True\n",
      "        eps: 1e-08\n",
      "        lr: 0.0001\n",
      "        weight_decay: 0.01\n",
      "    )\n",
      "[NeMo I 2023-05-03 21:56:02 lr_scheduler:910] Scheduler \"<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x7fcda7630580>\" \n",
      "    will be used during training (effective maximum steps = 3750) - \n",
      "    Parameters : \n",
      "    (warmup_steps: 50\n",
      "    min_lr: 0.0\n",
      "    constant_steps: 0\n",
      "    max_steps: 3750\n",
      "    )\n",
      "2023-05-03 21:56:02,238 - pytorch_lightning.callbacks.model_summary - INFO - \n",
      "  | Name            | Type                   | Params\n",
      "-----------------------------------------------------------\n",
      "0 | frozen_model    | MegatronGPTModel       | 354 M \n",
      "1 | word_embeddings | VocabParallelEmbedding | 51.5 M\n",
      "2 | prompt_encoder  | PromptEncoder          | 4.2 M \n",
      "-----------------------------------------------------------\n",
      "4.2 M     Trainable params\n",
      "354 M     Non-trainable params\n",
      "359 M     Total params\n",
      "718.178   Total estimated model params size (MB)\n",
      "Validation DataLoader 0:  93%|█████████▎| 27/29 [00:06<00:00,  4.17it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 21:56:02.177571 140649257482048 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 6.063295364379883\n",
      "I0503 21:56:02.238523 140522389899072 model_summary.py:83] \n",
      "  | Name            | Type                   | Params\n",
      "-----------------------------------------------------------\n",
      "0 | frozen_model    | MegatronGPTModel       | 354 M \n",
      "1 | word_embeddings | VocabParallelEmbedding | 51.5 M\n",
      "2 | prompt_encoder  | PromptEncoder          | 4.2 M \n",
      "-----------------------------------------------------------\n",
      "4.2 M     Trainable params\n",
      "354 M     Non-trainable params\n",
      "359 M     Total params\n",
      "718.178   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0: 100%|██████████| 29/29 [00:06<00:00,  4.36it/s]2023-05-03 21:56:02,484 - root - INFO - global_model_val_loss: 9.124295234680176\n",
      "Validation DataLoader 0: 100%|██████████| 29/29 [00:06<00:00,  4.35it/s]\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1m     Validate metric     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
      "│\u001b[36m \u001b[0m\u001b[36m  global_model_val_loss  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    9.124295234680176    \u001b[0m\u001b[35m \u001b[0m│\n",
      "└───────────────────────────┴───────────────────────────┘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 21:56:02.484618 139755592410944 fed_megatron_gpt_prompt_learning_model.py:99] global_model_val_loss: 9.124295234680176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00,  6.29it/s]2023-05-03 21:56:02,977 - root - INFO - val_loss: 6.063295364379883\n",
      "                                                                           2023-05-03 21:56:02,979 - PromptLearner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=f5aa2c14-c507-4451-8d91-362c9d433102]: Global_model global_model_val_loss: 9.124295234680176\n",
      "2023-05-03 21:56:02,980 - PromptLearner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=f5aa2c14-c507-4451-8d91-362c9d433102]: Current/Total Round: 1/1\n",
      "2023-05-03 21:56:02,980 - PromptLearner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=f5aa2c14-c507-4451-8d91-362c9d433102]: Client identity: site-2\n",
      "2023-05-03 21:56:02,987 - PromptLearner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=f5aa2c14-c507-4451-8d91-362c9d433102]: Loaded 7 of 7 weights\n",
      "2023-05-03 21:56:02,987 - PromptLearner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=f5aa2c14-c507-4451-8d91-362c9d433102]: Start training in round 0\n",
      "Epoch 0:   0%|          | 0/104 [00:00<?, ?it/s] 2023-05-03 21:56:02,992 - pytorch_lightning.accelerators.cuda - INFO - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[NeMo I 2023-05-03 21:56:03 nlp_overrides:105] Configuring DDP for model parallelism.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 21:56:02.977072 140522389899072 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 6.063295364379883\n",
      "I0503 21:56:02.979708 139755592410944 fl_component.py:134] [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=f5aa2c14-c507-4451-8d91-362c9d433102]: Global_model global_model_val_loss: 9.124295234680176\n",
      "I0503 21:56:02.980204 139755592410944 fl_component.py:134] [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=f5aa2c14-c507-4451-8d91-362c9d433102]: Current/Total Round: 1/1\n",
      "I0503 21:56:02.980409 139755592410944 fl_component.py:134] [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=f5aa2c14-c507-4451-8d91-362c9d433102]: Client identity: site-2\n",
      "I0503 21:56:02.987103 139755592410944 fl_component.py:134] [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=f5aa2c14-c507-4451-8d91-362c9d433102]: Loaded 7 of 7 weights\n",
      "I0503 21:56:02.987344 139755592410944 fl_component.py:134] [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=f5aa2c14-c507-4451-8d91-362c9d433102]: Start training in round 0\n",
      "I0503 21:56:02.992549 139755592410944 cuda.py:58] LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-05-03 21:56:03 modelPT:722] Optimizer config = FusedAdam (\n",
      "    Parameter Group 0\n",
      "        betas: [0.9, 0.98]\n",
      "        bias_correction: True\n",
      "        eps: 1e-08\n",
      "        lr: 0.0001\n",
      "        weight_decay: 0.01\n",
      "    )\n",
      "[NeMo I 2023-05-03 21:56:03 lr_scheduler:910] Scheduler \"<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x7f1b1eb14700>\" \n",
      "    will be used during training (effective maximum steps = 3750) - \n",
      "    Parameters : \n",
      "    (warmup_steps: 50\n",
      "    min_lr: 0.0\n",
      "    constant_steps: 0\n",
      "    max_steps: 3750\n",
      "    )\n",
      "2023-05-03 21:56:03,207 - pytorch_lightning.callbacks.model_summary - INFO - \n",
      "  | Name            | Type                   | Params\n",
      "-----------------------------------------------------------\n",
      "0 | frozen_model    | MegatronGPTModel       | 354 M \n",
      "1 | word_embeddings | VocabParallelEmbedding | 51.5 M\n",
      "2 | prompt_encoder  | PromptEncoder          | 4.2 M \n",
      "-----------------------------------------------------------\n",
      "4.2 M     Trainable params\n",
      "354 M     Non-trainable params\n",
      "359 M     Total params\n",
      "718.178   Total estimated model params size (MB)\n",
      "Sanity Checking: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 21:56:03.207578 139755592410944 model_summary.py:83] \n",
      "  | Name            | Type                   | Params\n",
      "-----------------------------------------------------------\n",
      "0 | frozen_model    | MegatronGPTModel       | 354 M \n",
      "1 | word_embeddings | VocabParallelEmbedding | 51.5 M\n",
      "2 | prompt_encoder  | PromptEncoder          | 4.2 M \n",
      "-----------------------------------------------------------\n",
      "4.2 M     Trainable params\n",
      "354 M     Non-trainable params\n",
      "359 M     Total params\n",
      "718.178   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00,  6.37it/s]2023-05-03 21:56:03,917 - root - INFO - val_loss: 6.063295364379883\n",
      "Epoch 0:   0%|          | 0/104 [00:00<?, ?it/s]                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 21:56:03.917259 139755592410944 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 6.063295364379883\n",
      "[NeMo W 2023-05-03 21:56:05 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:232: UserWarning: You called `self.log('global_step', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.\n",
      "      warning_cache.warn(\n",
      "    \n",
      "[NeMo W 2023-05-03 21:56:05 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "      warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   1%|          | 1/104 [00:03<06:20,  3.70s/it, loss=11.8, v_num=0, reduced_train_loss=11.80, global_step=0.000]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-05-03 21:56:06 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:232: UserWarning: You called `self.log('global_step', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.\n",
      "      warning_cache.warn(\n",
      "    \n",
      "[NeMo W 2023-05-03 21:56:06 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "      warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   5%|▍         | 5/104 [00:04<01:30,  1.09it/s, loss=10.2, v_num=0, reduced_train_loss=9.090, global_step=4.000]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-05-03 21:56:07 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:232: UserWarning: You called `self.log('global_step', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.\n",
      "      warning_cache.warn(\n",
      "    \n",
      "[NeMo W 2023-05-03 21:56:07 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "      warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  72%|███████▏  | 75/104 [00:20<00:07,  3.69it/s, loss=0.513, v_num=0, reduced_train_loss=0.689, global_step=74.00]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  66%|██████▋   | 69/104 [00:19<00:09,  3.52it/s, loss=0.544, v_num=0, reduced_train_loss=0.660, global_step=68.00]\n",
      "Epoch 0:  64%|██████▍   | 67/104 [00:18<00:10,  3.55it/s, loss=1.67, v_num=0, reduced_train_loss=1.200, global_step=66.00]]\n",
      "Epoch 0:  67%|██████▋   | 70/104 [00:19<00:09,  3.53it/s, loss=0.53, v_num=0, reduced_train_loss=0.394, global_step=69.00] \n",
      "Epoch 0:  68%|██████▊   | 71/104 [00:20<00:09,  3.53it/s, loss=0.509, v_num=0, reduced_train_loss=0.276, global_step=70.00]\n",
      "Epoch 0:  76%|███████▌  | 79/104 [00:20<00:06,  3.78it/s, loss=0.513, v_num=0, reduced_train_loss=0.689, global_step=74.00]\n",
      "Epoch 0:  69%|██████▉   | 72/104 [00:20<00:09,  3.54it/s, loss=0.496, v_num=0, reduced_train_loss=0.293, global_step=71.00]\n",
      "Epoch 0:  67%|██████▋   | 70/104 [00:19<00:09,  3.58it/s, loss=1.55, v_num=0, reduced_train_loss=1.160, global_step=69.00]]\n",
      "Epoch 0:  70%|███████   | 73/104 [00:20<00:08,  3.55it/s, loss=0.505, v_num=0, reduced_train_loss=0.871, global_step=72.00]\n",
      "Epoch 0:  68%|██████▊   | 71/104 [00:19<00:09,  3.59it/s, loss=1.5, v_num=0, reduced_train_loss=1.050, global_step=70.00] ]\n",
      "Epoch 0:  71%|███████   | 74/104 [00:20<00:08,  3.56it/s, loss=0.503, v_num=0, reduced_train_loss=0.663, global_step=73.00]\n",
      "Epoch 0:  69%|██████▉   | 72/104 [00:19<00:08,  3.61it/s, loss=1.49, v_num=0, reduced_train_loss=1.150, global_step=71.00]]\n",
      "Epoch 0:  72%|███████▏  | 75/104 [00:21<00:08,  3.56it/s, loss=0.506, v_num=0, reduced_train_loss=0.608, global_step=74.00]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  70%|███████   | 73/104 [00:20<00:08,  3.62it/s, loss=1.48, v_num=0, reduced_train_loss=1.190, global_step=72.00]]\n",
      "Epoch 0:  73%|███████▎  | 76/104 [00:21<00:07,  3.58it/s, loss=0.506, v_num=0, reduced_train_loss=0.608, global_step=74.00]\n",
      "Epoch 0:  85%|████████▍ | 88/104 [00:22<00:04,  3.99it/s, loss=0.513, v_num=0, reduced_train_loss=0.689, global_step=74.00]\n",
      "Epoch 0:  71%|███████   | 74/104 [00:20<00:08,  3.63it/s, loss=1.42, v_num=0, reduced_train_loss=0.735, global_step=73.00]]\n",
      "Epoch 0:  86%|████████▌ | 89/104 [00:22<00:03,  4.02it/s, loss=0.513, v_num=0, reduced_train_loss=0.689, global_step=74.00]\n",
      "Epoch 0:  75%|███████▌  | 78/104 [00:21<00:07,  3.63it/s, loss=0.506, v_num=0, reduced_train_loss=0.608, global_step=74.00]\n",
      "Epoch 0:  72%|███████▏  | 75/104 [00:20<00:07,  3.63it/s, loss=1.36, v_num=0, reduced_train_loss=1.140, global_step=74.00]]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 0:  76%|███████▌  | 79/104 [00:21<00:06,  3.66it/s, loss=0.506, v_num=0, reduced_train_loss=0.608, global_step=74.00]\n",
      "Epoch 0:  88%|████████▊ | 91/104 [00:22<00:03,  4.06it/s, loss=0.513, v_num=0, reduced_train_loss=0.689, global_step=74.00]\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  77%|███████▋  | 80/104 [00:21<00:06,  3.69it/s, loss=0.506, v_num=0, reduced_train_loss=0.608, global_step=74.00]\n",
      "Epoch 0:  88%|████████▊ | 92/104 [00:22<00:02,  4.08it/s, loss=0.513, v_num=0, reduced_train_loss=0.689, global_step=74.00]\n",
      "Epoch 0:  73%|███████▎  | 76/104 [00:20<00:07,  3.64it/s, loss=1.36, v_num=0, reduced_train_loss=1.140, global_step=74.00]\n",
      "Epoch 0:  78%|███████▊  | 81/104 [00:21<00:06,  3.71it/s, loss=0.506, v_num=0, reduced_train_loss=0.608, global_step=74.00]\n",
      "Epoch 0:  89%|████████▉ | 93/104 [00:22<00:02,  4.10it/s, loss=0.513, v_num=0, reduced_train_loss=0.689, global_step=74.00]\n",
      "Epoch 0:  74%|███████▍  | 77/104 [00:20<00:07,  3.67it/s, loss=1.36, v_num=0, reduced_train_loss=1.140, global_step=74.00]\n",
      "Epoch 0:  79%|███████▉  | 82/104 [00:21<00:05,  3.74it/s, loss=0.506, v_num=0, reduced_train_loss=0.608, global_step=74.00]\n",
      "Epoch 0:  90%|█████████ | 94/104 [00:22<00:02,  4.13it/s, loss=0.513, v_num=0, reduced_train_loss=0.689, global_step=74.00]\n",
      "Epoch 0:  75%|███████▌  | 78/104 [00:21<00:07,  3.70it/s, loss=1.36, v_num=0, reduced_train_loss=1.140, global_step=74.00]\n",
      "Epoch 0:  80%|███████▉  | 83/104 [00:22<00:05,  3.76it/s, loss=0.506, v_num=0, reduced_train_loss=0.608, global_step=74.00]\n",
      "Epoch 0:  91%|█████████▏| 95/104 [00:22<00:02,  4.15it/s, loss=0.513, v_num=0, reduced_train_loss=0.689, global_step=74.00]\n",
      "Epoch 0:  76%|███████▌  | 79/104 [00:21<00:06,  3.72it/s, loss=1.36, v_num=0, reduced_train_loss=1.140, global_step=74.00]\n",
      "Epoch 0:  81%|████████  | 84/104 [00:22<00:05,  3.79it/s, loss=0.506, v_num=0, reduced_train_loss=0.608, global_step=74.00]\n",
      "Epoch 0:  92%|█████████▏| 96/104 [00:23<00:01,  4.17it/s, loss=0.513, v_num=0, reduced_train_loss=0.689, global_step=74.00]\n",
      "Epoch 0:  77%|███████▋  | 80/104 [00:21<00:06,  3.75it/s, loss=1.36, v_num=0, reduced_train_loss=1.140, global_step=74.00]\n",
      "Epoch 0:  82%|████████▏ | 85/104 [00:22<00:04,  3.81it/s, loss=0.506, v_num=0, reduced_train_loss=0.608, global_step=74.00]\n",
      "Epoch 0:  93%|█████████▎| 97/104 [00:23<00:01,  4.19it/s, loss=0.513, v_num=0, reduced_train_loss=0.689, global_step=74.00]\n",
      "Epoch 0:  78%|███████▊  | 81/104 [00:21<00:06,  3.77it/s, loss=1.36, v_num=0, reduced_train_loss=1.140, global_step=74.00]\n",
      "Epoch 0:  83%|████████▎ | 86/104 [00:22<00:04,  3.84it/s, loss=0.506, v_num=0, reduced_train_loss=0.608, global_step=74.00]\n",
      "Epoch 0:  94%|█████████▍| 98/104 [00:23<00:01,  4.21it/s, loss=0.513, v_num=0, reduced_train_loss=0.689, global_step=74.00]\n",
      "Epoch 0:  79%|███████▉  | 82/104 [00:21<00:05,  3.80it/s, loss=1.36, v_num=0, reduced_train_loss=1.140, global_step=74.00]\n",
      "Epoch 0:  84%|████████▎ | 87/104 [00:22<00:04,  3.86it/s, loss=0.506, v_num=0, reduced_train_loss=0.608, global_step=74.00]\n",
      "Epoch 0:  95%|█████████▌| 99/104 [00:23<00:01,  4.23it/s, loss=0.513, v_num=0, reduced_train_loss=0.689, global_step=74.00]\n",
      "Epoch 0:  80%|███████▉  | 83/104 [00:21<00:05,  3.82it/s, loss=1.36, v_num=0, reduced_train_loss=1.140, global_step=74.00]\n",
      "Epoch 0:  85%|████████▍ | 88/104 [00:22<00:04,  3.88it/s, loss=0.506, v_num=0, reduced_train_loss=0.608, global_step=74.00]\n",
      "Epoch 0:  96%|█████████▌| 100/104 [00:23<00:00,  4.25it/s, loss=0.513, v_num=0, reduced_train_loss=0.689, global_step=74.00]\n",
      "Epoch 0:  81%|████████  | 84/104 [00:21<00:05,  3.85it/s, loss=1.36, v_num=0, reduced_train_loss=1.140, global_step=74.00]\n",
      "Epoch 0:  86%|████████▌ | 89/104 [00:22<00:03,  3.91it/s, loss=0.506, v_num=0, reduced_train_loss=0.608, global_step=74.00]\n",
      "Epoch 0:  97%|█████████▋| 101/104 [00:23<00:00,  4.27it/s, loss=0.513, v_num=0, reduced_train_loss=0.689, global_step=74.00]\n",
      "Epoch 0:  82%|████████▏ | 85/104 [00:21<00:04,  3.87it/s, loss=1.36, v_num=0, reduced_train_loss=1.140, global_step=74.00]\n",
      "Epoch 0:  87%|████████▋ | 90/104 [00:22<00:03,  3.93it/s, loss=0.506, v_num=0, reduced_train_loss=0.608, global_step=74.00]\n",
      "Epoch 0:  98%|█████████▊| 102/104 [00:23<00:00,  4.29it/s, loss=0.513, v_num=0, reduced_train_loss=0.689, global_step=74.00]\n",
      "Epoch 0:  88%|████████▊ | 91/104 [00:23<00:03,  3.95it/s, loss=0.506, v_num=0, reduced_train_loss=0.608, global_step=74.00]\n",
      "Epoch 0:  83%|████████▎ | 86/104 [00:22<00:04,  3.90it/s, loss=1.36, v_num=0, reduced_train_loss=1.140, global_step=74.00]\n",
      "Epoch 0:  99%|█████████▉| 103/104 [00:23<00:00,  4.31it/s, loss=0.513, v_num=0, reduced_train_loss=0.689, global_step=74.00]\n",
      "Epoch 0:  88%|████████▊ | 92/104 [00:23<00:03,  3.98it/s, loss=0.506, v_num=0, reduced_train_loss=0.608, global_step=74.00]\n",
      "Epoch 0:  84%|████████▎ | 87/104 [00:22<00:04,  3.92it/s, loss=1.36, v_num=0, reduced_train_loss=1.140, global_step=74.00]\n",
      "Epoch 0: 100%|██████████| 104/104 [00:23<00:00,  4.34it/s, loss=0.513, v_num=0, reduced_train_loss=0.689, global_step=74.00]2023-05-03 21:56:26,156 - root - INFO - val_loss: 0.41569310426712036\n",
      "Epoch 0: 100%|██████████| 104/104 [00:23<00:00,  4.34it/s, loss=0.513, v_num=0, reduced_train_loss=0.689, global_step=74.00, val_loss=0.416]\n",
      "Epoch 1:   0%|          | 0/104 [00:00<?, ?it/s, loss=0.513, v_num=0, reduced_train_loss=0.689, global_step=74.00, val_loss=0.416]          "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 21:56:26.156550 140649257482048 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.41569310426712036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0:  89%|████████▉ | 93/104 [00:23<00:02,  4.00it/s, loss=0.506, v_num=0, reduced_train_loss=0.608, global_step=74.00]\n",
      "Epoch 0:  85%|████████▍ | 88/104 [00:22<00:04,  3.94it/s, loss=1.36, v_num=0, reduced_train_loss=1.140, global_step=74.00]\n",
      "Epoch 0:  90%|█████████ | 94/104 [00:23<00:02,  4.02it/s, loss=0.506, v_num=0, reduced_train_loss=0.608, global_step=74.00]\n",
      "Epoch 1:   1%|          | 1/104 [00:00<00:28,  3.57it/s, loss=0.503, v_num=0, reduced_train_loss=0.597, global_step=75.00, val_loss=0.416]\n",
      "Epoch 0:  91%|█████████▏| 95/104 [00:23<00:02,  4.05it/s, loss=0.506, v_num=0, reduced_train_loss=0.608, global_step=74.00]\n",
      "Epoch 0:  87%|████████▋ | 90/104 [00:22<00:03,  3.99it/s, loss=1.36, v_num=0, reduced_train_loss=1.140, global_step=74.00]\n",
      "Epoch 0:  92%|█████████▏| 96/104 [00:23<00:01,  4.07it/s, loss=0.506, v_num=0, reduced_train_loss=0.608, global_step=74.00]\n",
      "Epoch 1:   2%|▏         | 2/104 [00:00<00:26,  3.86it/s, loss=0.507, v_num=0, reduced_train_loss=0.339, global_step=76.00, val_loss=0.416]\n",
      "Epoch 0:  93%|█████████▎| 97/104 [00:23<00:01,  4.09it/s, loss=0.506, v_num=0, reduced_train_loss=0.608, global_step=74.00]\n",
      "Epoch 0:  88%|████████▊ | 92/104 [00:22<00:02,  4.03it/s, loss=1.36, v_num=0, reduced_train_loss=1.140, global_step=74.00]\n",
      "Epoch 0:  94%|█████████▍| 98/104 [00:23<00:01,  4.11it/s, loss=0.506, v_num=0, reduced_train_loss=0.608, global_step=74.00]\n",
      "Epoch 1:   3%|▎         | 3/104 [00:00<00:25,  4.02it/s, loss=0.509, v_num=0, reduced_train_loss=0.639, global_step=77.00, val_loss=0.416]\n",
      "Epoch 0:  95%|█████████▌| 99/104 [00:23<00:01,  4.13it/s, loss=0.506, v_num=0, reduced_train_loss=0.608, global_step=74.00]\n",
      "Epoch 0:  90%|█████████ | 94/104 [00:23<00:02,  4.08it/s, loss=1.36, v_num=0, reduced_train_loss=1.140, global_step=74.00]\n",
      "Epoch 0:  96%|█████████▌| 100/104 [00:24<00:00,  4.15it/s, loss=0.506, v_num=0, reduced_train_loss=0.608, global_step=74.00]\n",
      "Epoch 1:   4%|▍         | 4/104 [00:00<00:24,  4.11it/s, loss=0.507, v_num=0, reduced_train_loss=0.360, global_step=78.00, val_loss=0.416]\n",
      "Epoch 0:  97%|█████████▋| 101/104 [00:24<00:00,  4.17it/s, loss=0.506, v_num=0, reduced_train_loss=0.608, global_step=74.00]\n",
      "Epoch 0:  92%|█████████▏| 96/104 [00:23<00:01,  4.12it/s, loss=1.36, v_num=0, reduced_train_loss=1.140, global_step=74.00]\n",
      "Epoch 0:  98%|█████████▊| 102/104 [00:24<00:00,  4.20it/s, loss=0.506, v_num=0, reduced_train_loss=0.608, global_step=74.00]\n",
      "Epoch 1:   5%|▍         | 5/104 [00:01<00:23,  4.16it/s, loss=0.51, v_num=0, reduced_train_loss=0.734, global_step=79.00, val_loss=0.416] \n",
      "Epoch 0:  99%|█████████▉| 103/104 [00:24<00:00,  4.22it/s, loss=0.506, v_num=0, reduced_train_loss=0.608, global_step=74.00]\n",
      "Epoch 0:  94%|█████████▍| 98/104 [00:23<00:01,  4.17it/s, loss=1.36, v_num=0, reduced_train_loss=1.140, global_step=74.00]\n",
      "Epoch 0: 100%|██████████| 104/104 [00:24<00:00,  4.25it/s, loss=0.506, v_num=0, reduced_train_loss=0.608, global_step=74.00]2023-05-03 21:56:27,484 - root - INFO - val_loss: 0.3508816659450531\n",
      "Epoch 0: 100%|██████████| 104/104 [00:24<00:00,  4.25it/s, loss=0.506, v_num=0, reduced_train_loss=0.608, global_step=74.00, val_loss=0.351]\n",
      "Epoch 1:   0%|          | 0/104 [00:00<?, ?it/s, loss=0.506, v_num=0, reduced_train_loss=0.608, global_step=74.00, val_loss=0.351]          "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 21:56:27.484981 140522389899072 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.3508816659450531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1:   6%|▌         | 6/104 [00:01<00:23,  4.15it/s, loss=0.533, v_num=0, reduced_train_loss=0.888, global_step=80.00, val_loss=0.416]\n",
      "Epoch 1:   1%|          | 1/104 [00:00<00:29,  3.47it/s, loss=0.568, v_num=0, reduced_train_loss=1.880, global_step=75.00, val_loss=0.351]\n",
      "Epoch 1:   7%|▋         | 7/104 [00:01<00:23,  4.17it/s, loss=0.53, v_num=0, reduced_train_loss=0.828, global_step=81.00, val_loss=0.416] \n",
      "Epoch 1:   2%|▏         | 2/104 [00:00<00:26,  3.82it/s, loss=0.57, v_num=0, reduced_train_loss=0.489, global_step=76.00, val_loss=0.351] \n",
      "Epoch 1:   8%|▊         | 8/104 [00:01<00:22,  4.20it/s, loss=0.552, v_num=0, reduced_train_loss=0.874, global_step=82.00, val_loss=0.416]\n",
      "Epoch 0: 100%|██████████| 104/104 [00:24<00:00,  4.30it/s, loss=1.36, v_num=0, reduced_train_loss=1.140, global_step=74.00]2023-05-03 21:56:28,117 - root - INFO - val_loss: 0.44989368319511414\n",
      "Epoch 0: 100%|██████████| 104/104 [00:24<00:00,  4.30it/s, loss=1.36, v_num=0, reduced_train_loss=1.140, global_step=74.00, val_loss=0.450]\n",
      "Epoch 1:   0%|          | 0/104 [00:00<?, ?it/s, loss=1.36, v_num=0, reduced_train_loss=1.140, global_step=74.00, val_loss=0.450]          "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 21:56:28.117239 139755592410944 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.44989368319511414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  72%|███████▏  | 75/104 [00:16<00:06,  4.53it/s, loss=0.375, v_num=0, reduced_train_loss=0.135, global_step=149.0, val_loss=0.351] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  68%|██████▊   | 71/104 [00:16<00:07,  4.40it/s, loss=0.578, v_num=0, reduced_train_loss=0.648, global_step=145.0, val_loss=0.450]\n",
      "Epoch 1:  74%|███████▍  | 77/104 [00:16<00:05,  4.57it/s, loss=0.375, v_num=0, reduced_train_loss=0.135, global_step=149.0, val_loss=0.351]\n",
      "Epoch 1:  72%|███████▏  | 75/104 [00:18<00:07,  4.10it/s, loss=0.429, v_num=0, reduced_train_loss=0.216, global_step=149.0, val_loss=0.416]\n",
      "Epoch 1:  69%|██████▉   | 72/104 [00:16<00:07,  4.40it/s, loss=0.581, v_num=0, reduced_train_loss=0.578, global_step=146.0, val_loss=0.450]\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  76%|███████▌  | 79/104 [00:17<00:05,  4.62it/s, loss=0.375, v_num=0, reduced_train_loss=0.135, global_step=149.0, val_loss=0.351]\n",
      "Epoch 1:  73%|███████▎  | 76/104 [00:18<00:06,  4.11it/s, loss=0.429, v_num=0, reduced_train_loss=0.216, global_step=149.0, val_loss=0.416]\n",
      "Epoch 1:  70%|███████   | 73/104 [00:16<00:07,  4.40it/s, loss=0.571, v_num=0, reduced_train_loss=0.381, global_step=147.0, val_loss=0.450]\n",
      "Epoch 1:  74%|███████▍  | 77/104 [00:18<00:06,  4.14it/s, loss=0.429, v_num=0, reduced_train_loss=0.216, global_step=149.0, val_loss=0.416]\n",
      "Epoch 1:  78%|███████▊  | 81/104 [00:17<00:04,  4.67it/s, loss=0.375, v_num=0, reduced_train_loss=0.135, global_step=149.0, val_loss=0.351]\n",
      "Epoch 1:  71%|███████   | 74/104 [00:16<00:06,  4.40it/s, loss=0.566, v_num=0, reduced_train_loss=0.839, global_step=148.0, val_loss=0.450]\n",
      "Epoch 1:  79%|███████▉  | 82/104 [00:17<00:04,  4.70it/s, loss=0.375, v_num=0, reduced_train_loss=0.135, global_step=149.0, val_loss=0.351]\n",
      "Epoch 1:  76%|███████▌  | 79/104 [00:18<00:05,  4.19it/s, loss=0.429, v_num=0, reduced_train_loss=0.216, global_step=149.0, val_loss=0.416]\n",
      "Epoch 1:  72%|███████▏  | 75/104 [00:17<00:06,  4.40it/s, loss=0.566, v_num=0, reduced_train_loss=0.839, global_step=148.0, val_loss=0.450]\n",
      "Epoch 1:  77%|███████▋  | 80/104 [00:18<00:05,  4.21it/s, loss=0.429, v_num=0, reduced_train_loss=0.216, global_step=149.0, val_loss=0.416]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  81%|████████  | 84/104 [00:17<00:04,  4.75it/s, loss=0.375, v_num=0, reduced_train_loss=0.135, global_step=149.0, val_loss=0.351]\n",
      "Epoch 1:  78%|███████▊  | 81/104 [00:19<00:05,  4.24it/s, loss=0.429, v_num=0, reduced_train_loss=0.216, global_step=149.0, val_loss=0.416]\n",
      "Epoch 1:  82%|████████▏ | 85/104 [00:17<00:03,  4.77it/s, loss=0.375, v_num=0, reduced_train_loss=0.135, global_step=149.0, val_loss=0.351]\n",
      "Epoch 1:  73%|███████▎  | 76/104 [00:17<00:06,  4.42it/s, loss=0.542, v_num=0, reduced_train_loss=0.443, global_step=149.0, val_loss=0.450]\n",
      "Epoch 1:  79%|███████▉  | 82/104 [00:19<00:05,  4.26it/s, loss=0.429, v_num=0, reduced_train_loss=0.216, global_step=149.0, val_loss=0.416]\n",
      "Epoch 1:  83%|████████▎ | 86/104 [00:17<00:03,  4.79it/s, loss=0.375, v_num=0, reduced_train_loss=0.135, global_step=149.0, val_loss=0.351]\n",
      "Epoch 1:  74%|███████▍  | 77/104 [00:17<00:06,  4.44it/s, loss=0.542, v_num=0, reduced_train_loss=0.443, global_step=149.0, val_loss=0.450]\n",
      "Epoch 1:  80%|███████▉  | 83/104 [00:19<00:04,  4.29it/s, loss=0.429, v_num=0, reduced_train_loss=0.216, global_step=149.0, val_loss=0.416]\n",
      "Epoch 1:  84%|████████▎ | 87/104 [00:18<00:03,  4.82it/s, loss=0.375, v_num=0, reduced_train_loss=0.135, global_step=149.0, val_loss=0.351]\n",
      "Epoch 1:  75%|███████▌  | 78/104 [00:17<00:05,  4.47it/s, loss=0.542, v_num=0, reduced_train_loss=0.443, global_step=149.0, val_loss=0.450]\n",
      "Epoch 1:  81%|████████  | 84/104 [00:19<00:04,  4.31it/s, loss=0.429, v_num=0, reduced_train_loss=0.216, global_step=149.0, val_loss=0.416]\n",
      "Epoch 1:  85%|████████▍ | 88/104 [00:18<00:03,  4.84it/s, loss=0.375, v_num=0, reduced_train_loss=0.135, global_step=149.0, val_loss=0.351]\n",
      "Epoch 1:  76%|███████▌  | 79/104 [00:17<00:05,  4.49it/s, loss=0.542, v_num=0, reduced_train_loss=0.443, global_step=149.0, val_loss=0.450]\n",
      "Epoch 1:  82%|████████▏ | 85/104 [00:19<00:04,  4.33it/s, loss=0.429, v_num=0, reduced_train_loss=0.216, global_step=149.0, val_loss=0.416]\n",
      "Epoch 1:  86%|████████▌ | 89/104 [00:18<00:03,  4.87it/s, loss=0.375, v_num=0, reduced_train_loss=0.135, global_step=149.0, val_loss=0.351]\n",
      "Epoch 1:  77%|███████▋  | 80/104 [00:17<00:05,  4.52it/s, loss=0.542, v_num=0, reduced_train_loss=0.443, global_step=149.0, val_loss=0.450]\n",
      "Epoch 1:  87%|████████▋ | 90/104 [00:18<00:02,  4.89it/s, loss=0.375, v_num=0, reduced_train_loss=0.135, global_step=149.0, val_loss=0.351]\n",
      "Epoch 1:  83%|████████▎ | 86/104 [00:19<00:04,  4.36it/s, loss=0.429, v_num=0, reduced_train_loss=0.216, global_step=149.0, val_loss=0.416]\n",
      "Epoch 1:  78%|███████▊  | 81/104 [00:17<00:05,  4.54it/s, loss=0.542, v_num=0, reduced_train_loss=0.443, global_step=149.0, val_loss=0.450]\n",
      "Epoch 1:  88%|████████▊ | 91/104 [00:18<00:02,  4.91it/s, loss=0.375, v_num=0, reduced_train_loss=0.135, global_step=149.0, val_loss=0.351]\n",
      "Epoch 1:  84%|████████▎ | 87/104 [00:19<00:03,  4.38it/s, loss=0.429, v_num=0, reduced_train_loss=0.216, global_step=149.0, val_loss=0.416]\n",
      "Epoch 1:  79%|███████▉  | 82/104 [00:17<00:04,  4.57it/s, loss=0.542, v_num=0, reduced_train_loss=0.443, global_step=149.0, val_loss=0.450]\n",
      "Epoch 1:  88%|████████▊ | 92/104 [00:18<00:02,  4.93it/s, loss=0.375, v_num=0, reduced_train_loss=0.135, global_step=149.0, val_loss=0.351]\n",
      "Epoch 1:  85%|████████▍ | 88/104 [00:19<00:03,  4.40it/s, loss=0.429, v_num=0, reduced_train_loss=0.216, global_step=149.0, val_loss=0.416]\n",
      "Epoch 1:  80%|███████▉  | 83/104 [00:18<00:04,  4.59it/s, loss=0.542, v_num=0, reduced_train_loss=0.443, global_step=149.0, val_loss=0.450]\n",
      "Epoch 1:  89%|████████▉ | 93/104 [00:18<00:02,  4.96it/s, loss=0.375, v_num=0, reduced_train_loss=0.135, global_step=149.0, val_loss=0.351]\n",
      "Epoch 1:  86%|████████▌ | 89/104 [00:20<00:03,  4.42it/s, loss=0.429, v_num=0, reduced_train_loss=0.216, global_step=149.0, val_loss=0.416]\n",
      "Epoch 1:  81%|████████  | 84/104 [00:18<00:04,  4.61it/s, loss=0.542, v_num=0, reduced_train_loss=0.443, global_step=149.0, val_loss=0.450]\n",
      "Epoch 1:  90%|█████████ | 94/104 [00:18<00:02,  4.98it/s, loss=0.375, v_num=0, reduced_train_loss=0.135, global_step=149.0, val_loss=0.351]\n",
      "Epoch 1:  87%|████████▋ | 90/104 [00:20<00:03,  4.45it/s, loss=0.429, v_num=0, reduced_train_loss=0.216, global_step=149.0, val_loss=0.416]\n",
      "Epoch 1:  82%|████████▏ | 85/104 [00:18<00:04,  4.64it/s, loss=0.542, v_num=0, reduced_train_loss=0.443, global_step=149.0, val_loss=0.450]\n",
      "Epoch 1:  91%|█████████▏| 95/104 [00:18<00:01,  5.00it/s, loss=0.375, v_num=0, reduced_train_loss=0.135, global_step=149.0, val_loss=0.351]\n",
      "Epoch 1:  88%|████████▊ | 91/104 [00:20<00:02,  4.47it/s, loss=0.429, v_num=0, reduced_train_loss=0.216, global_step=149.0, val_loss=0.416]\n",
      "Epoch 1:  83%|████████▎ | 86/104 [00:18<00:03,  4.66it/s, loss=0.542, v_num=0, reduced_train_loss=0.443, global_step=149.0, val_loss=0.450]\n",
      "Epoch 1:  92%|█████████▏| 96/104 [00:19<00:01,  5.02it/s, loss=0.375, v_num=0, reduced_train_loss=0.135, global_step=149.0, val_loss=0.351]\n",
      "Epoch 1:  88%|████████▊ | 92/104 [00:20<00:02,  4.49it/s, loss=0.429, v_num=0, reduced_train_loss=0.216, global_step=149.0, val_loss=0.416]\n",
      "Epoch 1:  84%|████████▎ | 87/104 [00:18<00:03,  4.68it/s, loss=0.542, v_num=0, reduced_train_loss=0.443, global_step=149.0, val_loss=0.450]\n",
      "Epoch 1:  93%|█████████▎| 97/104 [00:19<00:01,  5.04it/s, loss=0.375, v_num=0, reduced_train_loss=0.135, global_step=149.0, val_loss=0.351]\n",
      "Epoch 1:  89%|████████▉ | 93/104 [00:20<00:02,  4.51it/s, loss=0.429, v_num=0, reduced_train_loss=0.216, global_step=149.0, val_loss=0.416]\n",
      "Epoch 1:  85%|████████▍ | 88/104 [00:18<00:03,  4.71it/s, loss=0.542, v_num=0, reduced_train_loss=0.443, global_step=149.0, val_loss=0.450]\n",
      "Epoch 1:  94%|█████████▍| 98/104 [00:19<00:01,  5.07it/s, loss=0.375, v_num=0, reduced_train_loss=0.135, global_step=149.0, val_loss=0.351]\n",
      "Epoch 1:  90%|█████████ | 94/104 [00:20<00:02,  4.53it/s, loss=0.429, v_num=0, reduced_train_loss=0.216, global_step=149.0, val_loss=0.416]\n",
      "Epoch 1:  86%|████████▌ | 89/104 [00:18<00:03,  4.73it/s, loss=0.542, v_num=0, reduced_train_loss=0.443, global_step=149.0, val_loss=0.450]\n",
      "Epoch 1:  95%|█████████▌| 99/104 [00:19<00:00,  5.09it/s, loss=0.375, v_num=0, reduced_train_loss=0.135, global_step=149.0, val_loss=0.351]\n",
      "Epoch 1:  91%|█████████▏| 95/104 [00:20<00:01,  4.56it/s, loss=0.429, v_num=0, reduced_train_loss=0.216, global_step=149.0, val_loss=0.416]\n",
      "Epoch 1:  87%|████████▋ | 90/104 [00:18<00:02,  4.75it/s, loss=0.542, v_num=0, reduced_train_loss=0.443, global_step=149.0, val_loss=0.450]\n",
      "Epoch 1:  96%|█████████▌| 100/104 [00:19<00:00,  5.11it/s, loss=0.375, v_num=0, reduced_train_loss=0.135, global_step=149.0, val_loss=0.351]\n",
      "Epoch 1:  92%|█████████▏| 96/104 [00:20<00:01,  4.58it/s, loss=0.429, v_num=0, reduced_train_loss=0.216, global_step=149.0, val_loss=0.416]\n",
      "Epoch 1:  88%|████████▊ | 91/104 [00:19<00:02,  4.78it/s, loss=0.542, v_num=0, reduced_train_loss=0.443, global_step=149.0, val_loss=0.450]\n",
      "Epoch 1:  97%|█████████▋| 101/104 [00:19<00:00,  5.13it/s, loss=0.375, v_num=0, reduced_train_loss=0.135, global_step=149.0, val_loss=0.351]\n",
      "Epoch 1:  93%|█████████▎| 97/104 [00:21<00:01,  4.60it/s, loss=0.429, v_num=0, reduced_train_loss=0.216, global_step=149.0, val_loss=0.416]\n",
      "Epoch 1:  88%|████████▊ | 92/104 [00:19<00:02,  4.80it/s, loss=0.542, v_num=0, reduced_train_loss=0.443, global_step=149.0, val_loss=0.450]\n",
      "Epoch 1:  98%|█████████▊| 102/104 [00:19<00:00,  5.15it/s, loss=0.375, v_num=0, reduced_train_loss=0.135, global_step=149.0, val_loss=0.351]\n",
      "Epoch 1:  94%|█████████▍| 98/104 [00:21<00:01,  4.62it/s, loss=0.429, v_num=0, reduced_train_loss=0.216, global_step=149.0, val_loss=0.416]\n",
      "Epoch 1:  89%|████████▉ | 93/104 [00:19<00:02,  4.82it/s, loss=0.542, v_num=0, reduced_train_loss=0.443, global_step=149.0, val_loss=0.450]\n",
      "Epoch 1:  99%|█████████▉| 103/104 [00:19<00:00,  5.17it/s, loss=0.375, v_num=0, reduced_train_loss=0.135, global_step=149.0, val_loss=0.351]\n",
      "Epoch 1: 100%|██████████| 104/104 [00:19<00:00,  5.20it/s, loss=0.375, v_num=0, reduced_train_loss=0.135, global_step=149.0, val_loss=0.351]2023-05-03 21:56:47,482 - root - INFO - val_loss: 0.14852215349674225\n",
      "Epoch 1: 100%|██████████| 104/104 [00:19<00:00,  5.20it/s, loss=0.375, v_num=0, reduced_train_loss=0.135, global_step=149.0, val_loss=0.149]\n",
      "Epoch 2:   0%|          | 0/104 [00:00<?, ?it/s, loss=0.375, v_num=0, reduced_train_loss=0.135, global_step=149.0, val_loss=0.149]          \n",
      "Epoch 1:  95%|█████████▌| 99/104 [00:21<00:01,  4.64it/s, loss=0.429, v_num=0, reduced_train_loss=0.216, global_step=149.0, val_loss=0.416]\n",
      "Epoch 1:  90%|█████████ | 94/104 [00:19<00:02,  4.84it/s, loss=0.542, v_num=0, reduced_train_loss=0.443, global_step=149.0, val_loss=0.450]\n",
      "Epoch 1:  96%|█████████▌| 100/104 [00:21<00:00,  4.66it/s, loss=0.429, v_num=0, reduced_train_loss=0.216, global_step=149.0, val_loss=0.416]\n",
      "Epoch 1:  91%|█████████▏| 95/104 [00:19<00:01,  4.86it/s, loss=0.542, v_num=0, reduced_train_loss=0.443, global_step=149.0, val_loss=0.450]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 21:56:47.482541 140522389899072 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.14852215349674225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2:   1%|          | 1/104 [00:00<00:27,  3.80it/s, loss=0.37, v_num=0, reduced_train_loss=0.0829, global_step=150.0, val_loss=0.149]6]\n",
      "Epoch 1:  92%|█████████▏| 96/104 [00:19<00:01,  4.88it/s, loss=0.542, v_num=0, reduced_train_loss=0.443, global_step=149.0, val_loss=0.450]\n",
      "Epoch 1:  98%|█████████▊| 102/104 [00:21<00:00,  4.70it/s, loss=0.429, v_num=0, reduced_train_loss=0.216, global_step=149.0, val_loss=0.416]\n",
      "Epoch 2:   2%|▏         | 2/104 [00:00<00:24,  4.15it/s, loss=0.373, v_num=0, reduced_train_loss=0.214, global_step=151.0, val_loss=0.149]]\n",
      "Epoch 1:  99%|█████████▉| 103/104 [00:21<00:00,  4.72it/s, loss=0.429, v_num=0, reduced_train_loss=0.216, global_step=149.0, val_loss=0.416]\n",
      "Epoch 1:  94%|█████████▍| 98/104 [00:19<00:01,  4.93it/s, loss=0.542, v_num=0, reduced_train_loss=0.443, global_step=149.0, val_loss=0.450]\n",
      "Epoch 1: 100%|██████████| 104/104 [00:21<00:00,  4.75it/s, loss=0.429, v_num=0, reduced_train_loss=0.216, global_step=149.0, val_loss=0.416]2023-05-03 21:56:48,058 - root - INFO - val_loss: 0.2391786277294159\n",
      "Epoch 1: 100%|██████████| 104/104 [00:21<00:00,  4.75it/s, loss=0.429, v_num=0, reduced_train_loss=0.216, global_step=149.0, val_loss=0.239]\n",
      "Epoch 2:   0%|          | 0/104 [00:00<?, ?it/s, loss=0.429, v_num=0, reduced_train_loss=0.216, global_step=149.0, val_loss=0.239]          \n",
      "Epoch 1:  95%|█████████▌| 99/104 [00:20<00:01,  4.95it/s, loss=0.542, v_num=0, reduced_train_loss=0.443, global_step=149.0, val_loss=0.450]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 21:56:48.058345 140649257482048 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.2391786277294159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:   3%|▎         | 3/104 [00:00<00:23,  4.27it/s, loss=0.371, v_num=0, reduced_train_loss=0.302, global_step=152.0, val_loss=0.149]\n",
      "Epoch 2:   1%|          | 1/104 [00:00<00:28,  3.62it/s, loss=0.431, v_num=0, reduced_train_loss=0.497, global_step=150.0, val_loss=0.239]0]\n",
      "Epoch 2:   4%|▍         | 4/104 [00:00<00:23,  4.32it/s, loss=0.388, v_num=0, reduced_train_loss=0.683, global_step=153.0, val_loss=0.149]0]\n",
      "Epoch 2:   2%|▏         | 2/104 [00:00<00:25,  3.95it/s, loss=0.436, v_num=0, reduced_train_loss=0.438, global_step=151.0, val_loss=0.239]0]\n",
      "Epoch 2:   5%|▍         | 5/104 [00:01<00:22,  4.35it/s, loss=0.371, v_num=0, reduced_train_loss=0.402, global_step=154.0, val_loss=0.149]0]\n",
      "Epoch 1: 100%|██████████| 104/104 [00:20<00:00,  5.06it/s, loss=0.542, v_num=0, reduced_train_loss=0.443, global_step=149.0, val_loss=0.450]2023-05-03 21:56:48,685 - root - INFO - val_loss: 0.4168568253517151\n",
      "Epoch 1: 100%|██████████| 104/104 [00:20<00:00,  5.06it/s, loss=0.542, v_num=0, reduced_train_loss=0.443, global_step=149.0, val_loss=0.417]\n",
      "Epoch 2:   3%|▎         | 3/104 [00:00<00:25,  3.97it/s, loss=0.438, v_num=0, reduced_train_loss=0.220, global_step=152.0, val_loss=0.239]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 21:56:48.685668 139755592410944 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.4168568253517151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  72%|███████▏  | 75/104 [00:16<00:06,  4.56it/s, loss=0.287, v_num=0, reduced_train_loss=0.546, global_step=224.0, val_loss=0.149]  \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:  67%|██████▋   | 70/104 [00:16<00:07,  4.37it/s, loss=0.22, v_num=0, reduced_train_loss=0.0774, global_step=219.0, val_loss=0.239]\n",
      "Epoch 2:  65%|██████▌   | 68/104 [00:15<00:08,  4.39it/s, loss=0.487, v_num=0, reduced_train_loss=0.503, global_step=217.0, val_loss=0.417]\n",
      "Epoch 2:  68%|██████▊   | 71/104 [00:16<00:07,  4.38it/s, loss=0.223, v_num=0, reduced_train_loss=0.383, global_step=220.0, val_loss=0.239]\n",
      "Epoch 2:  66%|██████▋   | 69/104 [00:15<00:07,  4.39it/s, loss=0.492, v_num=0, reduced_train_loss=0.444, global_step=218.0, val_loss=0.417]\n",
      "Epoch 2:  69%|██████▉   | 72/104 [00:16<00:07,  4.38it/s, loss=0.229, v_num=0, reduced_train_loss=0.267, global_step=221.0, val_loss=0.239]\n",
      "Epoch 2:  67%|██████▋   | 70/104 [00:15<00:07,  4.39it/s, loss=0.472, v_num=0, reduced_train_loss=0.223, global_step=219.0, val_loss=0.417]\n",
      "Epoch 2:  70%|███████   | 73/104 [00:16<00:07,  4.38it/s, loss=0.218, v_num=0, reduced_train_loss=0.427, global_step=222.0, val_loss=0.239]\n",
      "Epoch 2:  68%|██████▊   | 71/104 [00:16<00:07,  4.39it/s, loss=0.482, v_num=0, reduced_train_loss=0.410, global_step=220.0, val_loss=0.417]\n",
      "Epoch 2:  71%|███████   | 74/104 [00:16<00:06,  4.38it/s, loss=0.235, v_num=0, reduced_train_loss=0.614, global_step=223.0, val_loss=0.239]\n",
      "Epoch 2:  69%|██████▉   | 72/104 [00:16<00:07,  4.39it/s, loss=0.481, v_num=0, reduced_train_loss=0.458, global_step=221.0, val_loss=0.417]\n",
      "Epoch 2:  72%|███████▏  | 75/104 [00:17<00:06,  4.38it/s, loss=0.239, v_num=0, reduced_train_loss=0.173, global_step=224.0, val_loss=0.239]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:  70%|███████   | 73/104 [00:16<00:07,  4.39it/s, loss=0.468, v_num=0, reduced_train_loss=0.197, global_step=222.0, val_loss=0.417]\n",
      "Epoch 2:  73%|███████▎  | 76/104 [00:17<00:06,  4.39it/s, loss=0.239, v_num=0, reduced_train_loss=0.173, global_step=224.0, val_loss=0.239]\n",
      "Epoch 2:  84%|████████▎ | 87/104 [00:17<00:03,  4.85it/s, loss=0.287, v_num=0, reduced_train_loss=0.546, global_step=224.0, val_loss=0.149]\n",
      "Epoch 2:  74%|███████▍  | 77/104 [00:17<00:06,  4.42it/s, loss=0.239, v_num=0, reduced_train_loss=0.173, global_step=224.0, val_loss=0.239]\n",
      "Epoch 2:  71%|███████   | 74/104 [00:16<00:06,  4.39it/s, loss=0.464, v_num=0, reduced_train_loss=0.337, global_step=223.0, val_loss=0.417]\n",
      "Epoch 2:  75%|███████▌  | 78/104 [00:17<00:05,  4.45it/s, loss=0.239, v_num=0, reduced_train_loss=0.173, global_step=224.0, val_loss=0.239]\n",
      "Epoch 2:  86%|████████▌ | 89/104 [00:18<00:03,  4.90it/s, loss=0.287, v_num=0, reduced_train_loss=0.546, global_step=224.0, val_loss=0.149]\n",
      "Epoch 2:  76%|███████▌  | 79/104 [00:17<00:05,  4.47it/s, loss=0.239, v_num=0, reduced_train_loss=0.173, global_step=224.0, val_loss=0.239]\n",
      "Epoch 2:  72%|███████▏  | 75/104 [00:17<00:06,  4.38it/s, loss=0.469, v_num=0, reduced_train_loss=0.506, global_step=224.0, val_loss=0.417]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:  77%|███████▋  | 80/104 [00:17<00:05,  4.50it/s, loss=0.239, v_num=0, reduced_train_loss=0.173, global_step=224.0, val_loss=0.239]\n",
      "Epoch 2:  88%|████████▊ | 91/104 [00:18<00:02,  4.95it/s, loss=0.287, v_num=0, reduced_train_loss=0.546, global_step=224.0, val_loss=0.149]\n",
      "Epoch 2:  78%|███████▊  | 81/104 [00:17<00:05,  4.52it/s, loss=0.239, v_num=0, reduced_train_loss=0.173, global_step=224.0, val_loss=0.239]\n",
      "Epoch 2:  73%|███████▎  | 76/104 [00:17<00:06,  4.39it/s, loss=0.469, v_num=0, reduced_train_loss=0.506, global_step=224.0, val_loss=0.417]\n",
      "Epoch 2:  88%|████████▊ | 92/104 [00:18<00:02,  4.97it/s, loss=0.287, v_num=0, reduced_train_loss=0.546, global_step=224.0, val_loss=0.149]\n",
      "Epoch 2:  79%|███████▉  | 82/104 [00:18<00:04,  4.54it/s, loss=0.239, v_num=0, reduced_train_loss=0.173, global_step=224.0, val_loss=0.239]\n",
      "Epoch 2:  74%|███████▍  | 77/104 [00:17<00:06,  4.42it/s, loss=0.469, v_num=0, reduced_train_loss=0.506, global_step=224.0, val_loss=0.417]\n",
      "Epoch 2:  89%|████████▉ | 93/104 [00:18<00:02,  4.99it/s, loss=0.287, v_num=0, reduced_train_loss=0.546, global_step=224.0, val_loss=0.149]\n",
      "Epoch 2:  80%|███████▉  | 83/104 [00:18<00:04,  4.57it/s, loss=0.239, v_num=0, reduced_train_loss=0.173, global_step=224.0, val_loss=0.239]\n",
      "Epoch 2:  90%|█████████ | 94/104 [00:18<00:01,  5.01it/s, loss=0.287, v_num=0, reduced_train_loss=0.546, global_step=224.0, val_loss=0.149]\n",
      "Epoch 2:  75%|███████▌  | 78/104 [00:17<00:05,  4.44it/s, loss=0.469, v_num=0, reduced_train_loss=0.506, global_step=224.0, val_loss=0.417]\n",
      "Epoch 2:  81%|████████  | 84/104 [00:18<00:04,  4.59it/s, loss=0.239, v_num=0, reduced_train_loss=0.173, global_step=224.0, val_loss=0.239]\n",
      "Epoch 2:  91%|█████████▏| 95/104 [00:18<00:01,  5.03it/s, loss=0.287, v_num=0, reduced_train_loss=0.546, global_step=224.0, val_loss=0.149]\n",
      "Epoch 2:  76%|███████▌  | 79/104 [00:17<00:05,  4.47it/s, loss=0.469, v_num=0, reduced_train_loss=0.506, global_step=224.0, val_loss=0.417]\n",
      "Epoch 2:  92%|█████████▏| 96/104 [00:18<00:01,  5.05it/s, loss=0.287, v_num=0, reduced_train_loss=0.546, global_step=224.0, val_loss=0.149]\n",
      "Epoch 2:  82%|████████▏ | 85/104 [00:18<00:04,  4.62it/s, loss=0.239, v_num=0, reduced_train_loss=0.173, global_step=224.0, val_loss=0.239]\n",
      "Epoch 2:  77%|███████▋  | 80/104 [00:17<00:05,  4.49it/s, loss=0.469, v_num=0, reduced_train_loss=0.506, global_step=224.0, val_loss=0.417]\n",
      "Epoch 2:  93%|█████████▎| 97/104 [00:19<00:01,  5.08it/s, loss=0.287, v_num=0, reduced_train_loss=0.546, global_step=224.0, val_loss=0.149]\n",
      "Epoch 2:  83%|████████▎ | 86/104 [00:18<00:03,  4.64it/s, loss=0.239, v_num=0, reduced_train_loss=0.173, global_step=224.0, val_loss=0.239]\n",
      "Epoch 2:  78%|███████▊  | 81/104 [00:17<00:05,  4.52it/s, loss=0.469, v_num=0, reduced_train_loss=0.506, global_step=224.0, val_loss=0.417]\n",
      "Epoch 2:  94%|█████████▍| 98/104 [00:19<00:01,  5.10it/s, loss=0.287, v_num=0, reduced_train_loss=0.546, global_step=224.0, val_loss=0.149]\n",
      "Epoch 2:  84%|████████▎ | 87/104 [00:18<00:03,  4.66it/s, loss=0.239, v_num=0, reduced_train_loss=0.173, global_step=224.0, val_loss=0.239]\n",
      "Epoch 2:  79%|███████▉  | 82/104 [00:18<00:04,  4.54it/s, loss=0.469, v_num=0, reduced_train_loss=0.506, global_step=224.0, val_loss=0.417]\n",
      "Epoch 2:  95%|█████████▌| 99/104 [00:19<00:00,  5.12it/s, loss=0.287, v_num=0, reduced_train_loss=0.546, global_step=224.0, val_loss=0.149]\n",
      "Epoch 2:  85%|████████▍ | 88/104 [00:18<00:03,  4.68it/s, loss=0.239, v_num=0, reduced_train_loss=0.173, global_step=224.0, val_loss=0.239]\n",
      "Epoch 2:  80%|███████▉  | 83/104 [00:18<00:04,  4.56it/s, loss=0.469, v_num=0, reduced_train_loss=0.506, global_step=224.0, val_loss=0.417]\n",
      "Epoch 2:  96%|█████████▌| 100/104 [00:19<00:00,  5.14it/s, loss=0.287, v_num=0, reduced_train_loss=0.546, global_step=224.0, val_loss=0.149]\n",
      "Epoch 2:  86%|████████▌ | 89/104 [00:18<00:03,  4.71it/s, loss=0.239, v_num=0, reduced_train_loss=0.173, global_step=224.0, val_loss=0.239]\n",
      "Epoch 2:  81%|████████  | 84/104 [00:18<00:04,  4.58it/s, loss=0.469, v_num=0, reduced_train_loss=0.506, global_step=224.0, val_loss=0.417]\n",
      "Epoch 2:  97%|█████████▋| 101/104 [00:19<00:00,  5.16it/s, loss=0.287, v_num=0, reduced_train_loss=0.546, global_step=224.0, val_loss=0.149]\n",
      "Epoch 2:  87%|████████▋ | 90/104 [00:19<00:02,  4.73it/s, loss=0.239, v_num=0, reduced_train_loss=0.173, global_step=224.0, val_loss=0.239]\n",
      "Epoch 2:  82%|████████▏ | 85/104 [00:18<00:04,  4.61it/s, loss=0.469, v_num=0, reduced_train_loss=0.506, global_step=224.0, val_loss=0.417]\n",
      "Epoch 2:  98%|█████████▊| 102/104 [00:19<00:00,  5.18it/s, loss=0.287, v_num=0, reduced_train_loss=0.546, global_step=224.0, val_loss=0.149]\n",
      "Epoch 2:  88%|████████▊ | 91/104 [00:19<00:02,  4.75it/s, loss=0.239, v_num=0, reduced_train_loss=0.173, global_step=224.0, val_loss=0.239]\n",
      "Epoch 2:  83%|████████▎ | 86/104 [00:18<00:03,  4.63it/s, loss=0.469, v_num=0, reduced_train_loss=0.506, global_step=224.0, val_loss=0.417]\n",
      "Epoch 2:  99%|█████████▉| 103/104 [00:19<00:00,  5.20it/s, loss=0.287, v_num=0, reduced_train_loss=0.546, global_step=224.0, val_loss=0.149]\n",
      "Epoch 2:  88%|████████▊ | 92/104 [00:19<00:02,  4.77it/s, loss=0.239, v_num=0, reduced_train_loss=0.173, global_step=224.0, val_loss=0.239]\n",
      "Epoch 2: 100%|██████████| 104/104 [00:19<00:00,  5.23it/s, loss=0.287, v_num=0, reduced_train_loss=0.546, global_step=224.0, val_loss=0.149]2023-05-03 21:57:07,361 - root - INFO - val_loss: 0.16108885407447815\n",
      "Epoch 2: 100%|██████████| 104/104 [00:19<00:00,  5.23it/s, loss=0.287, v_num=0, reduced_train_loss=0.546, global_step=224.0, val_loss=0.161]\n",
      "Epoch 3:   0%|          | 0/104 [00:00<?, ?it/s, loss=0.287, v_num=0, reduced_train_loss=0.546, global_step=224.0, val_loss=0.161]          "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 21:57:07.361057 140522389899072 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.16108885407447815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2:  84%|████████▎ | 87/104 [00:18<00:03,  4.65it/s, loss=0.469, v_num=0, reduced_train_loss=0.506, global_step=224.0, val_loss=0.417]\n",
      "Epoch 2:  89%|████████▉ | 93/104 [00:19<00:02,  4.79it/s, loss=0.239, v_num=0, reduced_train_loss=0.173, global_step=224.0, val_loss=0.239]\n",
      "Epoch 2:  85%|████████▍ | 88/104 [00:18<00:03,  4.67it/s, loss=0.469, v_num=0, reduced_train_loss=0.506, global_step=224.0, val_loss=0.417]\n",
      "Epoch 3:   1%|          | 1/104 [00:00<00:27,  3.71it/s, loss=0.283, v_num=0, reduced_train_loss=0.142, global_step=225.0, val_loss=0.161]]\n",
      "Epoch 2:  86%|████████▌ | 89/104 [00:18<00:03,  4.70it/s, loss=0.469, v_num=0, reduced_train_loss=0.506, global_step=224.0, val_loss=0.417]\n",
      "Epoch 2:  91%|█████████▏| 95/104 [00:19<00:01,  4.84it/s, loss=0.239, v_num=0, reduced_train_loss=0.173, global_step=224.0, val_loss=0.239]\n",
      "Epoch 2:  87%|████████▋ | 90/104 [00:19<00:02,  4.72it/s, loss=0.469, v_num=0, reduced_train_loss=0.506, global_step=224.0, val_loss=0.417]\n",
      "Epoch 3:   2%|▏         | 2/104 [00:00<00:24,  4.08it/s, loss=0.277, v_num=0, reduced_train_loss=0.117, global_step=226.0, val_loss=0.161]]\n",
      "Epoch 2:  88%|████████▊ | 91/104 [00:19<00:02,  4.74it/s, loss=0.469, v_num=0, reduced_train_loss=0.506, global_step=224.0, val_loss=0.417]\n",
      "Epoch 2:  93%|█████████▎| 97/104 [00:19<00:01,  4.88it/s, loss=0.239, v_num=0, reduced_train_loss=0.173, global_step=224.0, val_loss=0.239]\n",
      "Epoch 3:   3%|▎         | 3/104 [00:00<00:23,  4.25it/s, loss=0.277, v_num=0, reduced_train_loss=0.117, global_step=226.0, val_loss=0.161]]\n",
      "Epoch 2:  94%|█████████▍| 98/104 [00:20<00:01,  4.90it/s, loss=0.239, v_num=0, reduced_train_loss=0.173, global_step=224.0, val_loss=0.239]\n",
      "Epoch 2:  89%|████████▉ | 93/104 [00:19<00:02,  4.78it/s, loss=0.469, v_num=0, reduced_train_loss=0.506, global_step=224.0, val_loss=0.417]\n",
      "Epoch 2:  95%|█████████▌| 99/104 [00:20<00:01,  4.92it/s, loss=0.239, v_num=0, reduced_train_loss=0.173, global_step=224.0, val_loss=0.239]\n",
      "Epoch 3:   4%|▍         | 4/104 [00:00<00:23,  4.32it/s, loss=0.271, v_num=0, reduced_train_loss=0.0235, global_step=228.0, val_loss=0.161]\n",
      "Epoch 2:  96%|█████████▌| 100/104 [00:20<00:00,  4.94it/s, loss=0.239, v_num=0, reduced_train_loss=0.173, global_step=224.0, val_loss=0.239]\n",
      "Epoch 2:  91%|█████████▏| 95/104 [00:19<00:01,  4.82it/s, loss=0.469, v_num=0, reduced_train_loss=0.506, global_step=224.0, val_loss=0.417]\n",
      "Epoch 2:  97%|█████████▋| 101/104 [00:20<00:00,  4.96it/s, loss=0.239, v_num=0, reduced_train_loss=0.173, global_step=224.0, val_loss=0.239]\n",
      "Epoch 3:   5%|▍         | 5/104 [00:01<00:22,  4.35it/s, loss=0.267, v_num=0, reduced_train_loss=0.199, global_step=229.0, val_loss=0.161] \n",
      "Epoch 2:  98%|█████████▊| 102/104 [00:20<00:00,  4.98it/s, loss=0.239, v_num=0, reduced_train_loss=0.173, global_step=224.0, val_loss=0.239]\n",
      "Epoch 2:  93%|█████████▎| 97/104 [00:19<00:01,  4.86it/s, loss=0.469, v_num=0, reduced_train_loss=0.506, global_step=224.0, val_loss=0.417]\n",
      "Epoch 3:   6%|▌         | 6/104 [00:01<00:22,  4.37it/s, loss=0.279, v_num=0, reduced_train_loss=0.460, global_step=230.0, val_loss=0.161]9]\n",
      "Epoch 2: 100%|██████████| 104/104 [00:20<00:00,  5.03it/s, loss=0.239, v_num=0, reduced_train_loss=0.173, global_step=224.0, val_loss=0.239]2023-05-03 21:57:08,755 - root - INFO - val_loss: 0.10812219977378845\n",
      "Epoch 2: 100%|██████████| 104/104 [00:20<00:00,  5.03it/s, loss=0.239, v_num=0, reduced_train_loss=0.173, global_step=224.0, val_loss=0.108]\n",
      "Epoch 3:   0%|          | 0/104 [00:00<?, ?it/s, loss=0.239, v_num=0, reduced_train_loss=0.173, global_step=224.0, val_loss=0.108]          \n",
      "Epoch 2:  94%|█████████▍| 98/104 [00:20<00:01,  4.88it/s, loss=0.469, v_num=0, reduced_train_loss=0.506, global_step=224.0, val_loss=0.417]\n",
      "Epoch 2:  95%|█████████▌| 99/104 [00:20<00:01,  4.90it/s, loss=0.469, v_num=0, reduced_train_loss=0.506, global_step=224.0, val_loss=0.417]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 21:57:08.755060 140649257482048 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.10812219977378845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:   7%|▋         | 7/104 [00:01<00:22,  4.39it/s, loss=0.279, v_num=0, reduced_train_loss=0.321, global_step=231.0, val_loss=0.161]\n",
      "Epoch 3:   1%|          | 1/104 [00:00<00:27,  3.78it/s, loss=0.23, v_num=0, reduced_train_loss=0.130, global_step=225.0, val_loss=0.108] 7]\n",
      "Epoch 3:   2%|▏         | 2/104 [00:00<00:25,  4.08it/s, loss=0.228, v_num=0, reduced_train_loss=0.221, global_step=226.0, val_loss=0.108]7]\n",
      "Epoch 2:  98%|█████████▊| 102/104 [00:20<00:00,  4.96it/s, loss=0.469, v_num=0, reduced_train_loss=0.506, global_step=224.0, val_loss=0.417]\n",
      "Epoch 3:   9%|▊         | 9/104 [00:02<00:21,  4.44it/s, loss=0.225, v_num=0, reduced_train_loss=0.394, global_step=233.0, val_loss=0.161]7]\n",
      "Epoch 2: 100%|██████████| 104/104 [00:20<00:00,  5.01it/s, loss=0.469, v_num=0, reduced_train_loss=0.506, global_step=224.0, val_loss=0.417]2023-05-03 21:57:09,456 - root - INFO - val_loss: 0.3517796993255615\n",
      "Epoch 2: 100%|██████████| 104/104 [00:20<00:00,  5.01it/s, loss=0.469, v_num=0, reduced_train_loss=0.506, global_step=224.0, val_loss=0.352]\n",
      "Epoch 3:   3%|▎         | 3/104 [00:00<00:24,  4.19it/s, loss=0.225, v_num=0, reduced_train_loss=0.0903, global_step=227.0, val_loss=0.108] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 21:57:09.456427 139755592410944 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.3517796993255615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:  72%|███████▏  | 75/104 [00:16<00:06,  4.52it/s, loss=0.222, v_num=0, reduced_train_loss=0.218, global_step=299.0, val_loss=0.161]  \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3:  66%|██████▋   | 69/104 [00:15<00:07,  4.50it/s, loss=0.184, v_num=0, reduced_train_loss=0.032, global_step=293.0, val_loss=0.108] \n",
      "Epoch 3:  62%|██████▏   | 64/104 [00:14<00:09,  4.35it/s, loss=0.543, v_num=0, reduced_train_loss=0.459, global_step=288.0, val_loss=0.352]\n",
      "Epoch 3:  67%|██████▋   | 70/104 [00:15<00:07,  4.50it/s, loss=0.163, v_num=0, reduced_train_loss=0.143, global_step=294.0, val_loss=0.108]\n",
      "Epoch 3:  62%|██████▎   | 65/104 [00:14<00:08,  4.35it/s, loss=0.553, v_num=0, reduced_train_loss=0.580, global_step=289.0, val_loss=0.352]\n",
      "Epoch 3:  68%|██████▊   | 71/104 [00:15<00:07,  4.50it/s, loss=0.166, v_num=0, reduced_train_loss=0.178, global_step=295.0, val_loss=0.108]\n",
      "Epoch 3:  69%|██████▉   | 72/104 [00:15<00:07,  4.50it/s, loss=0.153, v_num=0, reduced_train_loss=0.0217, global_step=296.0, val_loss=0.108]\n",
      "Epoch 3:  78%|███████▊  | 81/104 [00:17<00:04,  4.66it/s, loss=0.222, v_num=0, reduced_train_loss=0.218, global_step=299.0, val_loss=0.161]\n",
      "Epoch 3:  70%|███████   | 73/104 [00:16<00:06,  4.51it/s, loss=0.163, v_num=0, reduced_train_loss=0.336, global_step=297.0, val_loss=0.108] \n",
      "Epoch 3:  65%|██████▌   | 68/104 [00:15<00:08,  4.35it/s, loss=0.557, v_num=0, reduced_train_loss=0.631, global_step=292.0, val_loss=0.352]\n",
      "Epoch 3:  71%|███████   | 74/104 [00:16<00:06,  4.51it/s, loss=0.178, v_num=0, reduced_train_loss=0.356, global_step=298.0, val_loss=0.108]\n",
      "Epoch 3:  66%|██████▋   | 69/104 [00:15<00:08,  4.35it/s, loss=0.555, v_num=0, reduced_train_loss=0.856, global_step=293.0, val_loss=0.352]\n",
      "Epoch 3:  72%|███████▏  | 75/104 [00:16<00:06,  4.51it/s, loss=0.178, v_num=0, reduced_train_loss=0.0726, global_step=299.0, val_loss=0.108]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3:  67%|██████▋   | 70/104 [00:16<00:07,  4.35it/s, loss=0.546, v_num=0, reduced_train_loss=0.361, global_step=294.0, val_loss=0.352]\n",
      "Epoch 3:  73%|███████▎  | 76/104 [00:16<00:06,  4.52it/s, loss=0.178, v_num=0, reduced_train_loss=0.0726, global_step=299.0, val_loss=0.108]\n",
      "Epoch 3:  85%|████████▍ | 88/104 [00:18<00:03,  4.83it/s, loss=0.222, v_num=0, reduced_train_loss=0.218, global_step=299.0, val_loss=0.161]\n",
      "Epoch 3:  74%|███████▍  | 77/104 [00:16<00:05,  4.55it/s, loss=0.178, v_num=0, reduced_train_loss=0.0726, global_step=299.0, val_loss=0.108]\n",
      "Epoch 3:  68%|██████▊   | 71/104 [00:16<00:07,  4.35it/s, loss=0.515, v_num=0, reduced_train_loss=0.0906, global_step=295.0, val_loss=0.352]\n",
      "Epoch 3:  75%|███████▌  | 78/104 [00:17<00:05,  4.57it/s, loss=0.178, v_num=0, reduced_train_loss=0.0726, global_step=299.0, val_loss=0.108]\n",
      "Epoch 3:  87%|████████▋ | 90/104 [00:18<00:02,  4.87it/s, loss=0.222, v_num=0, reduced_train_loss=0.218, global_step=299.0, val_loss=0.161]\n",
      "Epoch 3:  76%|███████▌  | 79/104 [00:17<00:05,  4.60it/s, loss=0.178, v_num=0, reduced_train_loss=0.0726, global_step=299.0, val_loss=0.108]\n",
      "Epoch 3:  69%|██████▉   | 72/104 [00:16<00:07,  4.35it/s, loss=0.501, v_num=0, reduced_train_loss=0.531, global_step=296.0, val_loss=0.352] \n",
      "Epoch 3:  88%|████████▊ | 92/104 [00:18<00:02,  4.92it/s, loss=0.222, v_num=0, reduced_train_loss=0.218, global_step=299.0, val_loss=0.161]\n",
      "Epoch 3:  77%|███████▋  | 80/104 [00:17<00:05,  4.62it/s, loss=0.178, v_num=0, reduced_train_loss=0.0726, global_step=299.0, val_loss=0.108]\n",
      "Epoch 3:  89%|████████▉ | 93/104 [00:18<00:02,  4.94it/s, loss=0.222, v_num=0, reduced_train_loss=0.218, global_step=299.0, val_loss=0.161]\n",
      "Epoch 3:  70%|███████   | 73/104 [00:16<00:07,  4.35it/s, loss=0.504, v_num=0, reduced_train_loss=0.588, global_step=297.0, val_loss=0.352]]\n",
      "Epoch 3:  90%|█████████ | 94/104 [00:18<00:02,  4.97it/s, loss=0.222, v_num=0, reduced_train_loss=0.218, global_step=299.0, val_loss=0.161]\n",
      "Epoch 3:  79%|███████▉  | 82/104 [00:17<00:04,  4.67it/s, loss=0.178, v_num=0, reduced_train_loss=0.0726, global_step=299.0, val_loss=0.108]\n",
      "Epoch 3:  91%|█████████▏| 95/104 [00:19<00:01,  4.99it/s, loss=0.222, v_num=0, reduced_train_loss=0.218, global_step=299.0, val_loss=0.161]\n",
      "Epoch 3:  71%|███████   | 74/104 [00:17<00:06,  4.35it/s, loss=0.498, v_num=0, reduced_train_loss=0.594, global_step=298.0, val_loss=0.352]]\n",
      "Epoch 3:  92%|█████████▏| 96/104 [00:19<00:01,  5.01it/s, loss=0.222, v_num=0, reduced_train_loss=0.218, global_step=299.0, val_loss=0.161]\n",
      "Epoch 3:  81%|████████  | 84/104 [00:17<00:04,  4.72it/s, loss=0.178, v_num=0, reduced_train_loss=0.0726, global_step=299.0, val_loss=0.108]\n",
      "Epoch 3:  93%|█████████▎| 97/104 [00:19<00:01,  5.03it/s, loss=0.222, v_num=0, reduced_train_loss=0.218, global_step=299.0, val_loss=0.161]\n",
      "Epoch 3:  72%|███████▏  | 75/104 [00:17<00:06,  4.35it/s, loss=0.491, v_num=0, reduced_train_loss=0.454, global_step=299.0, val_loss=0.352]]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3:  94%|█████████▍| 98/104 [00:19<00:01,  5.05it/s, loss=0.222, v_num=0, reduced_train_loss=0.218, global_step=299.0, val_loss=0.161]\n",
      "Epoch 3:  83%|████████▎ | 86/104 [00:18<00:03,  4.77it/s, loss=0.178, v_num=0, reduced_train_loss=0.0726, global_step=299.0, val_loss=0.108]\n",
      "Epoch 3:  73%|███████▎  | 76/104 [00:17<00:06,  4.36it/s, loss=0.491, v_num=0, reduced_train_loss=0.454, global_step=299.0, val_loss=0.352]\n",
      "Epoch 3:  95%|█████████▌| 99/104 [00:19<00:00,  5.07it/s, loss=0.222, v_num=0, reduced_train_loss=0.218, global_step=299.0, val_loss=0.161]\n",
      "Epoch 3:  84%|████████▎ | 87/104 [00:18<00:03,  4.79it/s, loss=0.178, v_num=0, reduced_train_loss=0.0726, global_step=299.0, val_loss=0.108]\n",
      "Epoch 3:  74%|███████▍  | 77/104 [00:17<00:06,  4.39it/s, loss=0.491, v_num=0, reduced_train_loss=0.454, global_step=299.0, val_loss=0.352]\n",
      "Epoch 3:  96%|█████████▌| 100/104 [00:19<00:00,  5.09it/s, loss=0.222, v_num=0, reduced_train_loss=0.218, global_step=299.0, val_loss=0.161]\n",
      "Epoch 3:  85%|████████▍ | 88/104 [00:18<00:03,  4.81it/s, loss=0.178, v_num=0, reduced_train_loss=0.0726, global_step=299.0, val_loss=0.108]\n",
      "Epoch 3:  75%|███████▌  | 78/104 [00:17<00:05,  4.42it/s, loss=0.491, v_num=0, reduced_train_loss=0.454, global_step=299.0, val_loss=0.352]\n",
      "Epoch 3:  97%|█████████▋| 101/104 [00:19<00:00,  5.11it/s, loss=0.222, v_num=0, reduced_train_loss=0.218, global_step=299.0, val_loss=0.161]\n",
      "Epoch 3:  86%|████████▌ | 89/104 [00:18<00:03,  4.84it/s, loss=0.178, v_num=0, reduced_train_loss=0.0726, global_step=299.0, val_loss=0.108]\n",
      "Epoch 3:  76%|███████▌  | 79/104 [00:17<00:05,  4.44it/s, loss=0.491, v_num=0, reduced_train_loss=0.454, global_step=299.0, val_loss=0.352]\n",
      "Epoch 3:  98%|█████████▊| 102/104 [00:19<00:00,  5.13it/s, loss=0.222, v_num=0, reduced_train_loss=0.218, global_step=299.0, val_loss=0.161]\n",
      "Epoch 3:  87%|████████▋ | 90/104 [00:18<00:02,  4.86it/s, loss=0.178, v_num=0, reduced_train_loss=0.0726, global_step=299.0, val_loss=0.108]\n",
      "Epoch 3:  77%|███████▋  | 80/104 [00:17<00:05,  4.47it/s, loss=0.491, v_num=0, reduced_train_loss=0.454, global_step=299.0, val_loss=0.352]\n",
      "Epoch 3:  99%|█████████▉| 103/104 [00:20<00:00,  5.15it/s, loss=0.222, v_num=0, reduced_train_loss=0.218, global_step=299.0, val_loss=0.161]\n",
      "Epoch 3:  88%|████████▊ | 91/104 [00:18<00:02,  4.88it/s, loss=0.178, v_num=0, reduced_train_loss=0.0726, global_step=299.0, val_loss=0.108]\n",
      "Epoch 3: 100%|██████████| 104/104 [00:20<00:00,  5.18it/s, loss=0.222, v_num=0, reduced_train_loss=0.218, global_step=299.0, val_loss=0.161]2023-05-03 21:57:27,432 - root - INFO - val_loss: 0.11162658035755157\n",
      "Epoch 3: 100%|██████████| 104/104 [00:20<00:00,  5.18it/s, loss=0.222, v_num=0, reduced_train_loss=0.218, global_step=299.0, val_loss=0.112]\n",
      "Epoch 4:   0%|          | 0/104 [00:00<?, ?it/s, loss=0.222, v_num=0, reduced_train_loss=0.218, global_step=299.0, val_loss=0.112]          \n",
      "Epoch 3:  78%|███████▊  | 81/104 [00:18<00:05,  4.49it/s, loss=0.491, v_num=0, reduced_train_loss=0.454, global_step=299.0, val_loss=0.352]\n",
      "Epoch 3:  88%|████████▊ | 92/104 [00:18<00:02,  4.90it/s, loss=0.178, v_num=0, reduced_train_loss=0.0726, global_step=299.0, val_loss=0.108]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 21:57:27.432579 140522389899072 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.11162658035755157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3:  79%|███████▉  | 82/104 [00:18<00:04,  4.52it/s, loss=0.491, v_num=0, reduced_train_loss=0.454, global_step=299.0, val_loss=0.352]\n",
      "Epoch 4:   1%|          | 1/104 [00:00<00:27,  3.71it/s, loss=0.219, v_num=0, reduced_train_loss=0.156, global_step=300.0, val_loss=0.112]8]\n",
      "Epoch 3:  80%|███████▉  | 83/104 [00:18<00:04,  4.54it/s, loss=0.491, v_num=0, reduced_train_loss=0.454, global_step=299.0, val_loss=0.352]\n",
      "Epoch 3:  90%|█████████ | 94/104 [00:19<00:02,  4.95it/s, loss=0.178, v_num=0, reduced_train_loss=0.0726, global_step=299.0, val_loss=0.108]\n",
      "Epoch 3:  81%|████████  | 84/104 [00:18<00:04,  4.57it/s, loss=0.491, v_num=0, reduced_train_loss=0.454, global_step=299.0, val_loss=0.352]\n",
      "Epoch 4:   2%|▏         | 2/104 [00:00<00:25,  4.03it/s, loss=0.234, v_num=0, reduced_train_loss=0.439, global_step=301.0, val_loss=0.112]8]\n",
      "Epoch 3:  82%|████████▏ | 85/104 [00:18<00:04,  4.59it/s, loss=0.491, v_num=0, reduced_train_loss=0.454, global_step=299.0, val_loss=0.352]\n",
      "Epoch 3:  92%|█████████▏| 96/104 [00:19<00:01,  4.99it/s, loss=0.178, v_num=0, reduced_train_loss=0.0726, global_step=299.0, val_loss=0.108]\n",
      "Epoch 3:  83%|████████▎ | 86/104 [00:18<00:03,  4.61it/s, loss=0.491, v_num=0, reduced_train_loss=0.454, global_step=299.0, val_loss=0.352]\n",
      "Epoch 4:   3%|▎         | 3/104 [00:00<00:24,  4.15it/s, loss=0.234, v_num=0, reduced_train_loss=0.0601, global_step=302.0, val_loss=0.112]]\n",
      "Epoch 3:  84%|████████▎ | 87/104 [00:18<00:03,  4.64it/s, loss=0.491, v_num=0, reduced_train_loss=0.454, global_step=299.0, val_loss=0.352]\n",
      "Epoch 3:  94%|█████████▍| 98/104 [00:19<00:01,  5.03it/s, loss=0.178, v_num=0, reduced_train_loss=0.0726, global_step=299.0, val_loss=0.108]\n",
      "Epoch 3:  85%|████████▍ | 88/104 [00:18<00:03,  4.66it/s, loss=0.491, v_num=0, reduced_train_loss=0.454, global_step=299.0, val_loss=0.352]\n",
      "Epoch 4:   4%|▍         | 4/104 [00:00<00:23,  4.21it/s, loss=0.225, v_num=0, reduced_train_loss=0.0414, global_step=303.0, val_loss=0.112]]\n",
      "Epoch 3:  86%|████████▌ | 89/104 [00:19<00:03,  4.68it/s, loss=0.491, v_num=0, reduced_train_loss=0.454, global_step=299.0, val_loss=0.352]\n",
      "Epoch 3:  96%|█████████▌| 100/104 [00:19<00:00,  5.07it/s, loss=0.178, v_num=0, reduced_train_loss=0.0726, global_step=299.0, val_loss=0.108]\n",
      "Epoch 4:   5%|▍         | 5/104 [00:01<00:23,  4.27it/s, loss=0.227, v_num=0, reduced_train_loss=0.150, global_step=304.0, val_loss=0.112] \n",
      "Epoch 3:  97%|█████████▋| 101/104 [00:19<00:00,  5.09it/s, loss=0.178, v_num=0, reduced_train_loss=0.0726, global_step=299.0, val_loss=0.108]\n",
      "Epoch 3:  88%|████████▊ | 91/104 [00:19<00:02,  4.73it/s, loss=0.491, v_num=0, reduced_train_loss=0.454, global_step=299.0, val_loss=0.352]\n",
      "Epoch 3:  98%|█████████▊| 102/104 [00:19<00:00,  5.11it/s, loss=0.178, v_num=0, reduced_train_loss=0.0726, global_step=299.0, val_loss=0.108]\n",
      "Epoch 4:   6%|▌         | 6/104 [00:01<00:22,  4.31it/s, loss=0.203, v_num=0, reduced_train_loss=0.198, global_step=305.0, val_loss=0.112]]\n",
      "Epoch 3:  99%|█████████▉| 103/104 [00:20<00:00,  5.13it/s, loss=0.178, v_num=0, reduced_train_loss=0.0726, global_step=299.0, val_loss=0.108]\n",
      "Epoch 3: 100%|██████████| 104/104 [00:20<00:00,  5.16it/s, loss=0.178, v_num=0, reduced_train_loss=0.0726, global_step=299.0, val_loss=0.108]2023-05-03 21:57:28,917 - root - INFO - val_loss: 0.10436980426311493\n",
      "Epoch 3: 100%|██████████| 104/104 [00:20<00:00,  5.16it/s, loss=0.178, v_num=0, reduced_train_loss=0.0726, global_step=299.0, val_loss=0.104]\n",
      "Epoch 4:   0%|          | 0/104 [00:00<?, ?it/s, loss=0.178, v_num=0, reduced_train_loss=0.0726, global_step=299.0, val_loss=0.104]          \n",
      "Epoch 3:  89%|████████▉ | 93/104 [00:19<00:02,  4.77it/s, loss=0.491, v_num=0, reduced_train_loss=0.454, global_step=299.0, val_loss=0.352]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 21:57:28.917787 140649257482048 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.10436980426311493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:   7%|▋         | 7/104 [00:01<00:22,  4.34it/s, loss=0.204, v_num=0, reduced_train_loss=0.0288, global_step=306.0, val_loss=0.112]\n",
      "Epoch 3:  90%|█████████ | 94/104 [00:19<00:02,  4.80it/s, loss=0.491, v_num=0, reduced_train_loss=0.454, global_step=299.0, val_loss=0.352]\n",
      "Epoch 4:   8%|▊         | 8/104 [00:01<00:21,  4.37it/s, loss=0.204, v_num=0, reduced_train_loss=0.174, global_step=307.0, val_loss=0.112] \n",
      "Epoch 4:   2%|▏         | 2/104 [00:00<00:24,  4.18it/s, loss=0.185, v_num=0, reduced_train_loss=0.0523, global_step=301.0, val_loss=0.104]\n",
      "Epoch 4:   9%|▊         | 9/104 [00:02<00:21,  4.39it/s, loss=0.173, v_num=0, reduced_train_loss=0.043, global_step=308.0, val_loss=0.112]]\n",
      "Epoch 4:   3%|▎         | 3/104 [00:00<00:23,  4.29it/s, loss=0.176, v_num=0, reduced_train_loss=0.125, global_step=302.0, val_loss=0.104] \n",
      "Epoch 4:  10%|▉         | 10/104 [00:02<00:21,  4.39it/s, loss=0.171, v_num=0, reduced_train_loss=0.153, global_step=309.0, val_loss=0.112]\n",
      "Epoch 4:   4%|▍         | 4/104 [00:00<00:23,  4.34it/s, loss=0.19, v_num=0, reduced_train_loss=0.324, global_step=303.0, val_loss=0.104] 2]\n",
      "Epoch 4:  11%|█         | 11/104 [00:02<00:21,  4.40it/s, loss=0.188, v_num=0, reduced_train_loss=0.606, global_step=310.0, val_loss=0.112]]\n",
      "Epoch 4:   5%|▍         | 5/104 [00:01<00:22,  4.37it/s, loss=0.182, v_num=0, reduced_train_loss=0.00952, global_step=304.0, val_loss=0.104]\n",
      "Epoch 4:  12%|█▏        | 12/104 [00:02<00:20,  4.41it/s, loss=0.19, v_num=0, reduced_train_loss=0.0843, global_step=311.0, val_loss=0.112]]\n",
      "Epoch 3: 100%|██████████| 104/104 [00:20<00:00,  5.01it/s, loss=0.491, v_num=0, reduced_train_loss=0.454, global_step=299.0, val_loss=0.352]2023-05-03 21:57:30,213 - root - INFO - val_loss: 0.3288504481315613\n",
      "Epoch 3: 100%|██████████| 104/104 [00:20<00:00,  5.01it/s, loss=0.491, v_num=0, reduced_train_loss=0.454, global_step=299.0, val_loss=0.329]\n",
      "Epoch 4:   6%|▌         | 6/104 [00:01<00:22,  4.39it/s, loss=0.169, v_num=0, reduced_train_loss=0.0233, global_step=305.0, val_loss=0.104] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 21:57:30.213440 139755592410944 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.3288504481315613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:  72%|███████▏  | 75/104 [00:16<00:06,  4.51it/s, loss=0.236, v_num=0, reduced_train_loss=0.677, global_step=374.0, val_loss=0.112]  \n",
      "Epoch 4:  59%|█████▊    | 61/104 [00:13<00:09,  4.40it/s, loss=0.528, v_num=0, reduced_train_loss=0.734, global_step=360.0, val_loss=0.329]\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4:  66%|██████▋   | 69/104 [00:15<00:07,  4.52it/s, loss=0.212, v_num=0, reduced_train_loss=0.0749, global_step=368.0, val_loss=0.104]\n",
      "Epoch 4:  60%|█████▉    | 62/104 [00:14<00:09,  4.40it/s, loss=0.503, v_num=0, reduced_train_loss=0.312, global_step=361.0, val_loss=0.329]\n",
      "Epoch 4:  67%|██████▋   | 70/104 [00:15<00:07,  4.52it/s, loss=0.201, v_num=0, reduced_train_loss=0.0528, global_step=369.0, val_loss=0.104]\n",
      "Epoch 4:  61%|██████    | 63/104 [00:14<00:09,  4.40it/s, loss=0.518, v_num=0, reduced_train_loss=0.559, global_step=362.0, val_loss=0.329]\n",
      "Epoch 4:  68%|██████▊   | 71/104 [00:15<00:07,  4.52it/s, loss=0.199, v_num=0, reduced_train_loss=0.0508, global_step=370.0, val_loss=0.104]\n",
      "Epoch 4:  69%|██████▉   | 72/104 [00:15<00:07,  4.52it/s, loss=0.17, v_num=0, reduced_train_loss=0.0322, global_step=371.0, val_loss=0.104] \n",
      "Epoch 4:  78%|███████▊  | 81/104 [00:17<00:04,  4.65it/s, loss=0.236, v_num=0, reduced_train_loss=0.677, global_step=374.0, val_loss=0.112]\n",
      "Epoch 4:  70%|███████   | 73/104 [00:16<00:06,  4.52it/s, loss=0.167, v_num=0, reduced_train_loss=0.0957, global_step=372.0, val_loss=0.104]\n",
      "Epoch 4:  80%|███████▉  | 83/104 [00:17<00:04,  4.70it/s, loss=0.236, v_num=0, reduced_train_loss=0.677, global_step=374.0, val_loss=0.112]\n",
      "Epoch 4:  71%|███████   | 74/104 [00:16<00:06,  4.53it/s, loss=0.151, v_num=0, reduced_train_loss=0.0221, global_step=373.0, val_loss=0.104]\n",
      "Epoch 4:  64%|██████▍   | 67/104 [00:15<00:08,  4.40it/s, loss=0.494, v_num=0, reduced_train_loss=0.655, global_step=365.0, val_loss=0.329]\n",
      "Epoch 4:  72%|███████▏  | 75/104 [00:16<00:06,  4.53it/s, loss=0.139, v_num=0, reduced_train_loss=0.146, global_step=374.0, val_loss=0.104] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4:  84%|████████▎ | 87/104 [00:18<00:03,  4.80it/s, loss=0.236, v_num=0, reduced_train_loss=0.677, global_step=374.0, val_loss=0.112]\n",
      "Epoch 4:  65%|██████▌   | 68/104 [00:15<00:08,  4.40it/s, loss=0.493, v_num=0, reduced_train_loss=0.593, global_step=367.0, val_loss=0.329]\n",
      "Epoch 4:  85%|████████▍ | 88/104 [00:18<00:03,  4.82it/s, loss=0.236, v_num=0, reduced_train_loss=0.677, global_step=374.0, val_loss=0.112]\n",
      "Epoch 4:  74%|███████▍  | 77/104 [00:16<00:05,  4.57it/s, loss=0.139, v_num=0, reduced_train_loss=0.146, global_step=374.0, val_loss=0.104]\n",
      "Epoch 4:  86%|████████▌ | 89/104 [00:18<00:03,  4.85it/s, loss=0.236, v_num=0, reduced_train_loss=0.677, global_step=374.0, val_loss=0.112]\n",
      "Epoch 4:  75%|███████▌  | 78/104 [00:16<00:05,  4.59it/s, loss=0.139, v_num=0, reduced_train_loss=0.146, global_step=374.0, val_loss=0.104]\n",
      "Epoch 4:  66%|██████▋   | 69/104 [00:15<00:07,  4.39it/s, loss=0.504, v_num=0, reduced_train_loss=0.712, global_step=368.0, val_loss=0.329]\n",
      "Epoch 4:  76%|███████▌  | 79/104 [00:17<00:05,  4.62it/s, loss=0.139, v_num=0, reduced_train_loss=0.146, global_step=374.0, val_loss=0.104]\n",
      "Epoch 4:  88%|████████▊ | 91/104 [00:18<00:02,  4.89it/s, loss=0.236, v_num=0, reduced_train_loss=0.677, global_step=374.0, val_loss=0.112]\n",
      "Epoch 4:  67%|██████▋   | 70/104 [00:15<00:07,  4.39it/s, loss=0.508, v_num=0, reduced_train_loss=0.645, global_step=369.0, val_loss=0.329]\n",
      "Epoch 4:  88%|████████▊ | 92/104 [00:18<00:02,  4.91it/s, loss=0.236, v_num=0, reduced_train_loss=0.677, global_step=374.0, val_loss=0.112]\n",
      "Epoch 4:  78%|███████▊  | 81/104 [00:17<00:04,  4.67it/s, loss=0.139, v_num=0, reduced_train_loss=0.146, global_step=374.0, val_loss=0.104]\n",
      "Epoch 4:  68%|██████▊   | 71/104 [00:16<00:07,  4.39it/s, loss=0.518, v_num=0, reduced_train_loss=0.639, global_step=370.0, val_loss=0.329]\n",
      "Epoch 4:  79%|███████▉  | 82/104 [00:17<00:04,  4.69it/s, loss=0.139, v_num=0, reduced_train_loss=0.146, global_step=374.0, val_loss=0.104]\n",
      "Epoch 4:  90%|█████████ | 94/104 [00:18<00:02,  4.96it/s, loss=0.236, v_num=0, reduced_train_loss=0.677, global_step=374.0, val_loss=0.112]\n",
      "Epoch 4:  91%|█████████▏| 95/104 [00:19<00:01,  4.98it/s, loss=0.236, v_num=0, reduced_train_loss=0.677, global_step=374.0, val_loss=0.112]\n",
      "Epoch 4:  69%|██████▉   | 72/104 [00:16<00:07,  4.39it/s, loss=0.521, v_num=0, reduced_train_loss=0.436, global_step=371.0, val_loss=0.329]\n",
      "Epoch 4:  92%|█████████▏| 96/104 [00:19<00:01,  5.00it/s, loss=0.236, v_num=0, reduced_train_loss=0.677, global_step=374.0, val_loss=0.112]\n",
      "Epoch 4:  81%|████████  | 84/104 [00:17<00:04,  4.74it/s, loss=0.139, v_num=0, reduced_train_loss=0.146, global_step=374.0, val_loss=0.104]\n",
      "Epoch 4:  93%|█████████▎| 97/104 [00:19<00:01,  5.02it/s, loss=0.236, v_num=0, reduced_train_loss=0.677, global_step=374.0, val_loss=0.112]\n",
      "Epoch 4:  70%|███████   | 73/104 [00:16<00:07,  4.40it/s, loss=0.52, v_num=0, reduced_train_loss=0.436, global_step=372.0, val_loss=0.329] \n",
      "Epoch 4:  94%|█████████▍| 98/104 [00:19<00:01,  5.04it/s, loss=0.236, v_num=0, reduced_train_loss=0.677, global_step=374.0, val_loss=0.112]\n",
      "Epoch 4:  83%|████████▎ | 86/104 [00:17<00:03,  4.79it/s, loss=0.139, v_num=0, reduced_train_loss=0.146, global_step=374.0, val_loss=0.104]\n",
      "Epoch 4:  95%|█████████▌| 99/104 [00:19<00:00,  5.06it/s, loss=0.236, v_num=0, reduced_train_loss=0.677, global_step=374.0, val_loss=0.112]\n",
      "Epoch 4:  71%|███████   | 74/104 [00:16<00:06,  4.40it/s, loss=0.521, v_num=0, reduced_train_loss=0.386, global_step=373.0, val_loss=0.329]\n",
      "Epoch 4:  96%|█████████▌| 100/104 [00:19<00:00,  5.08it/s, loss=0.236, v_num=0, reduced_train_loss=0.677, global_step=374.0, val_loss=0.112]\n",
      "Epoch 4:  85%|████████▍ | 88/104 [00:18<00:03,  4.83it/s, loss=0.139, v_num=0, reduced_train_loss=0.146, global_step=374.0, val_loss=0.104]\n",
      "Epoch 4:  97%|█████████▋| 101/104 [00:19<00:00,  5.10it/s, loss=0.236, v_num=0, reduced_train_loss=0.677, global_step=374.0, val_loss=0.112]\n",
      "Epoch 4:  72%|███████▏  | 75/104 [00:17<00:06,  4.40it/s, loss=0.519, v_num=0, reduced_train_loss=0.449, global_step=374.0, val_loss=0.329]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4:  98%|█████████▊| 102/104 [00:19<00:00,  5.13it/s, loss=0.236, v_num=0, reduced_train_loss=0.677, global_step=374.0, val_loss=0.112]\n",
      "Epoch 4:  87%|████████▋ | 90/104 [00:18<00:02,  4.88it/s, loss=0.139, v_num=0, reduced_train_loss=0.146, global_step=374.0, val_loss=0.104]\n",
      "Epoch 4:  73%|███████▎  | 76/104 [00:17<00:06,  4.41it/s, loss=0.519, v_num=0, reduced_train_loss=0.449, global_step=374.0, val_loss=0.329]\n",
      "Epoch 4:  99%|█████████▉| 103/104 [00:20<00:00,  5.14it/s, loss=0.236, v_num=0, reduced_train_loss=0.677, global_step=374.0, val_loss=0.112]\n",
      "Epoch 4:  88%|████████▊ | 91/104 [00:18<00:02,  4.90it/s, loss=0.139, v_num=0, reduced_train_loss=0.146, global_step=374.0, val_loss=0.104]\n",
      "Epoch 4: 100%|██████████| 104/104 [00:20<00:00,  5.18it/s, loss=0.236, v_num=0, reduced_train_loss=0.677, global_step=374.0, val_loss=0.112]2023-05-03 21:57:47,522 - root - INFO - val_loss: 0.1115463525056839\n",
      "Epoch 4: 100%|██████████| 104/104 [00:20<00:00,  5.18it/s, loss=0.236, v_num=0, reduced_train_loss=0.677, global_step=374.0, val_loss=0.112]\n",
      "Epoch 5:   0%|          | 0/104 [00:00<?, ?it/s, loss=0.236, v_num=0, reduced_train_loss=0.677, global_step=374.0, val_loss=0.112]          \n",
      "Epoch 4:  74%|███████▍  | 77/104 [00:17<00:06,  4.44it/s, loss=0.519, v_num=0, reduced_train_loss=0.449, global_step=374.0, val_loss=0.329]\n",
      "Epoch 4:  88%|████████▊ | 92/104 [00:18<00:02,  4.93it/s, loss=0.139, v_num=0, reduced_train_loss=0.146, global_step=374.0, val_loss=0.104]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 21:57:47.522151 140522389899072 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.1115463525056839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4:  75%|███████▌  | 78/104 [00:17<00:05,  4.46it/s, loss=0.519, v_num=0, reduced_train_loss=0.449, global_step=374.0, val_loss=0.329]\n",
      "Epoch 5:   1%|          | 1/104 [00:00<00:27,  3.76it/s, loss=0.239, v_num=0, reduced_train_loss=0.0833, global_step=375.0, val_loss=0.112]\n",
      "Epoch 4:  76%|███████▌  | 79/104 [00:17<00:05,  4.49it/s, loss=0.519, v_num=0, reduced_train_loss=0.449, global_step=374.0, val_loss=0.329]\n",
      "Epoch 4:  90%|█████████ | 94/104 [00:18<00:02,  4.97it/s, loss=0.139, v_num=0, reduced_train_loss=0.146, global_step=374.0, val_loss=0.104]\n",
      "Validation DataLoader 0:  69%|██████▉   | 20/29 [00:02<00:01,  8.17it/s]\u001b[A\n",
      "Epoch 5:   2%|▏         | 2/104 [00:00<00:24,  4.10it/s, loss=0.231, v_num=0, reduced_train_loss=0.0683, global_step=376.0, val_loss=0.112]\n",
      "Epoch 4:  92%|█████████▏| 96/104 [00:19<00:01,  5.01it/s, loss=0.139, v_num=0, reduced_train_loss=0.146, global_step=374.0, val_loss=0.104]\n",
      "Epoch 4:  78%|███████▊  | 81/104 [00:17<00:05,  4.53it/s, loss=0.519, v_num=0, reduced_train_loss=0.449, global_step=374.0, val_loss=0.329]\n",
      "Epoch 4:  93%|█████████▎| 97/104 [00:19<00:01,  5.03it/s, loss=0.139, v_num=0, reduced_train_loss=0.146, global_step=374.0, val_loss=0.104]\n",
      "Epoch 5:   3%|▎         | 3/104 [00:00<00:23,  4.23it/s, loss=0.252, v_num=0, reduced_train_loss=0.444, global_step=377.0, val_loss=0.112] \n",
      "Epoch 4:  94%|█████████▍| 98/104 [00:19<00:01,  5.05it/s, loss=0.139, v_num=0, reduced_train_loss=0.146, global_step=374.0, val_loss=0.104]\n",
      "Epoch 4:  80%|███████▉  | 83/104 [00:18<00:04,  4.58it/s, loss=0.519, v_num=0, reduced_train_loss=0.449, global_step=374.0, val_loss=0.329]\n",
      "Epoch 4:  95%|█████████▌| 99/104 [00:19<00:00,  5.07it/s, loss=0.139, v_num=0, reduced_train_loss=0.146, global_step=374.0, val_loss=0.104]\n",
      "Epoch 5:   4%|▍         | 4/104 [00:00<00:23,  4.29it/s, loss=0.248, v_num=0, reduced_train_loss=0.0298, global_step=378.0, val_loss=0.112]\n",
      "Epoch 4:  96%|█████████▌| 100/104 [00:19<00:00,  5.09it/s, loss=0.139, v_num=0, reduced_train_loss=0.146, global_step=374.0, val_loss=0.104]\n",
      "Epoch 4:  82%|████████▏ | 85/104 [00:18<00:04,  4.63it/s, loss=0.519, v_num=0, reduced_train_loss=0.449, global_step=374.0, val_loss=0.329]\n",
      "Epoch 5:   5%|▍         | 5/104 [00:01<00:22,  4.32it/s, loss=0.24, v_num=0, reduced_train_loss=0.067, global_step=379.0, val_loss=0.112]  ]\n",
      "Epoch 4:  83%|████████▎ | 86/104 [00:18<00:03,  4.65it/s, loss=0.519, v_num=0, reduced_train_loss=0.449, global_step=374.0, val_loss=0.329]\n",
      "Epoch 4:  98%|█████████▊| 102/104 [00:19<00:00,  5.13it/s, loss=0.139, v_num=0, reduced_train_loss=0.146, global_step=374.0, val_loss=0.104]\n",
      "Epoch 5:   6%|▌         | 6/104 [00:01<00:22,  4.35it/s, loss=0.238, v_num=0, reduced_train_loss=0.235, global_step=380.0, val_loss=0.112]]\n",
      "Epoch 4:  99%|█████████▉| 103/104 [00:19<00:00,  5.15it/s, loss=0.139, v_num=0, reduced_train_loss=0.146, global_step=374.0, val_loss=0.104]\n",
      "Epoch 4:  85%|████████▍ | 88/104 [00:18<00:03,  4.70it/s, loss=0.519, v_num=0, reduced_train_loss=0.449, global_step=374.0, val_loss=0.329]\n",
      "Epoch 4: 100%|██████████| 104/104 [00:20<00:00,  5.19it/s, loss=0.139, v_num=0, reduced_train_loss=0.146, global_step=374.0, val_loss=0.104]2023-05-03 21:57:48,981 - root - INFO - val_loss: 0.07740198075771332\n",
      "Epoch 4: 100%|██████████| 104/104 [00:20<00:00,  5.18it/s, loss=0.139, v_num=0, reduced_train_loss=0.146, global_step=374.0, val_loss=0.0774]\n",
      "Epoch 5:   0%|          | 0/104 [00:00<?, ?it/s, loss=0.139, v_num=0, reduced_train_loss=0.146, global_step=374.0, val_loss=0.0774]          \n",
      "Epoch 4:  86%|████████▌ | 89/104 [00:18<00:03,  4.72it/s, loss=0.519, v_num=0, reduced_train_loss=0.449, global_step=374.0, val_loss=0.329]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 21:57:48.981627 140649257482048 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.07740198075771332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:   7%|▋         | 7/104 [00:01<00:22,  4.37it/s, loss=0.239, v_num=0, reduced_train_loss=0.198, global_step=381.0, val_loss=0.112]\n",
      "Epoch 5:   1%|          | 1/104 [00:00<00:27,  3.78it/s, loss=0.13, v_num=0, reduced_train_loss=0.0104, global_step=375.0, val_loss=0.0774]\n",
      "Epoch 5:   8%|▊         | 8/104 [00:01<00:21,  4.40it/s, loss=0.233, v_num=0, reduced_train_loss=0.0201, global_step=382.0, val_loss=0.112]\n",
      "Epoch 5:   2%|▏         | 2/104 [00:00<00:24,  4.15it/s, loss=0.146, v_num=0, reduced_train_loss=0.461, global_step=376.0, val_loss=0.0774]\n",
      "Epoch 5:   9%|▊         | 9/104 [00:02<00:21,  4.42it/s, loss=0.222, v_num=0, reduced_train_loss=0.0342, global_step=383.0, val_loss=0.112]\n",
      "Epoch 5:  10%|▉         | 10/104 [00:02<00:21,  4.44it/s, loss=0.222, v_num=0, reduced_train_loss=0.0342, global_step=383.0, val_loss=0.112]\n",
      "Epoch 5:   4%|▍         | 4/104 [00:00<00:23,  4.32it/s, loss=0.125, v_num=0, reduced_train_loss=0.0405, global_step=378.0, val_loss=0.0774]\n",
      "Epoch 5:  11%|█         | 11/104 [00:02<00:20,  4.45it/s, loss=0.225, v_num=0, reduced_train_loss=0.135, global_step=385.0, val_loss=0.112] \n",
      "Epoch 5:   5%|▍         | 5/104 [00:01<00:22,  4.37it/s, loss=0.131, v_num=0, reduced_train_loss=0.172, global_step=379.0, val_loss=0.0774] \n",
      "Epoch 5:  12%|█▏        | 12/104 [00:02<00:20,  4.45it/s, loss=0.22, v_num=0, reduced_train_loss=0.496, global_step=386.0, val_loss=0.112] \n",
      "Epoch 5:   6%|▌         | 6/104 [00:01<00:22,  4.38it/s, loss=0.133, v_num=0, reduced_train_loss=0.226, global_step=380.0, val_loss=0.0774]\n",
      "Epoch 5:  12%|█▎        | 13/104 [00:02<00:20,  4.45it/s, loss=0.235, v_num=0, reduced_train_loss=0.309, global_step=387.0, val_loss=0.112]]\n",
      "Epoch 5:   7%|▋         | 7/104 [00:01<00:22,  4.34it/s, loss=0.127, v_num=0, reduced_train_loss=0.208, global_step=381.0, val_loss=0.0774]]\n",
      "Epoch 5:  13%|█▎        | 14/104 [00:03<00:20,  4.44it/s, loss=0.216, v_num=0, reduced_train_loss=0.0619, global_step=388.0, val_loss=0.112]\n",
      "Epoch 5:   8%|▊         | 8/104 [00:01<00:22,  4.33it/s, loss=0.138, v_num=0, reduced_train_loss=0.359, global_step=382.0, val_loss=0.0774]]\n",
      "Epoch 4: 100%|██████████| 104/104 [00:20<00:00,  5.04it/s, loss=0.519, v_num=0, reduced_train_loss=0.449, global_step=374.0, val_loss=0.329]2023-05-03 21:57:50,845 - root - INFO - val_loss: 0.24179112911224365\n",
      "Epoch 4: 100%|██████████| 104/104 [00:20<00:00,  5.04it/s, loss=0.519, v_num=0, reduced_train_loss=0.449, global_step=374.0, val_loss=0.242]\n",
      "Epoch 5:   0%|          | 0/104 [00:00<?, ?it/s, loss=0.519, v_num=0, reduced_train_loss=0.449, global_step=374.0, val_loss=0.242]          "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 21:57:50.845316 139755592410944 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.24179112911224365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:  72%|███████▏  | 75/104 [00:16<00:06,  4.50it/s, loss=0.156, v_num=0, reduced_train_loss=0.588, global_step=449.0, val_loss=0.112]4] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5:  57%|█████▋    | 59/104 [00:13<00:10,  4.39it/s, loss=0.404, v_num=0, reduced_train_loss=0.402, global_step=433.0, val_loss=0.242]\n",
      "Epoch 5:  66%|██████▋   | 69/104 [00:15<00:07,  4.49it/s, loss=0.171, v_num=0, reduced_train_loss=0.0223, global_step=443.0, val_loss=0.0774]\n",
      "Epoch 5:  67%|██████▋   | 70/104 [00:15<00:07,  4.49it/s, loss=0.171, v_num=0, reduced_train_loss=0.0223, global_step=443.0, val_loss=0.0774]\n",
      "Epoch 5:  67%|██████▋   | 70/104 [00:15<00:07,  4.49it/s, loss=0.157, v_num=0, reduced_train_loss=0.114, global_step=444.0, val_loss=0.0774] \n",
      "Epoch 5:  68%|██████▊   | 71/104 [00:15<00:07,  4.49it/s, loss=0.131, v_num=0, reduced_train_loss=0.0158, global_step=445.0, val_loss=0.0774]\n",
      "Epoch 5:  77%|███████▋  | 80/104 [00:17<00:05,  4.63it/s, loss=0.156, v_num=0, reduced_train_loss=0.588, global_step=449.0, val_loss=0.112]\n",
      "Epoch 5:  69%|██████▉   | 72/104 [00:16<00:07,  4.49it/s, loss=0.136, v_num=0, reduced_train_loss=0.103, global_step=446.0, val_loss=0.0774] \n",
      "Epoch 5:  79%|███████▉  | 82/104 [00:17<00:04,  4.68it/s, loss=0.156, v_num=0, reduced_train_loss=0.588, global_step=449.0, val_loss=0.112]\n",
      "Epoch 5:  70%|███████   | 73/104 [00:16<00:06,  4.49it/s, loss=0.122, v_num=0, reduced_train_loss=0.268, global_step=447.0, val_loss=0.0774]\n",
      "Epoch 5:  62%|██████▏   | 64/104 [00:14<00:09,  4.40it/s, loss=0.438, v_num=0, reduced_train_loss=0.513, global_step=438.0, val_loss=0.242]\n",
      "Epoch 5:  71%|███████   | 74/104 [00:16<00:06,  4.49it/s, loss=0.12, v_num=0, reduced_train_loss=0.115, global_step=448.0, val_loss=0.0774] \n",
      "Epoch 5:  62%|██████▎   | 65/104 [00:14<00:08,  4.40it/s, loss=0.452, v_num=0, reduced_train_loss=0.493, global_step=439.0, val_loss=0.242]\n",
      "Epoch 5:  72%|███████▏  | 75/104 [00:16<00:06,  4.49it/s, loss=0.117, v_num=0, reduced_train_loss=0.00904, global_step=449.0, val_loss=0.0774]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5:  85%|████████▍ | 88/104 [00:18<00:03,  4.82it/s, loss=0.156, v_num=0, reduced_train_loss=0.588, global_step=449.0, val_loss=0.112]\n",
      "Epoch 5:  63%|██████▎   | 66/104 [00:15<00:08,  4.40it/s, loss=0.455, v_num=0, reduced_train_loss=0.317, global_step=440.0, val_loss=0.242]74]\n",
      "Epoch 5:  86%|████████▌ | 89/104 [00:18<00:03,  4.84it/s, loss=0.156, v_num=0, reduced_train_loss=0.588, global_step=449.0, val_loss=0.112]\n",
      "Epoch 5:  74%|███████▍  | 77/104 [00:16<00:05,  4.53it/s, loss=0.117, v_num=0, reduced_train_loss=0.00904, global_step=449.0, val_loss=0.0774]\n",
      "Epoch 5:  64%|██████▍   | 67/104 [00:15<00:08,  4.40it/s, loss=0.447, v_num=0, reduced_train_loss=0.398, global_step=441.0, val_loss=0.242]\n",
      "Epoch 5:  75%|███████▌  | 78/104 [00:17<00:05,  4.56it/s, loss=0.117, v_num=0, reduced_train_loss=0.00904, global_step=449.0, val_loss=0.0774]\n",
      "Epoch 5:  88%|████████▊ | 91/104 [00:18<00:02,  4.89it/s, loss=0.156, v_num=0, reduced_train_loss=0.588, global_step=449.0, val_loss=0.112]\n",
      "Epoch 5:  76%|███████▌  | 79/104 [00:17<00:05,  4.58it/s, loss=0.117, v_num=0, reduced_train_loss=0.00904, global_step=449.0, val_loss=0.0774]\n",
      "Epoch 5:  65%|██████▌   | 68/104 [00:15<00:08,  4.40it/s, loss=0.431, v_num=0, reduced_train_loss=0.273, global_step=442.0, val_loss=0.242]\n",
      "Epoch 5:  77%|███████▋  | 80/104 [00:17<00:05,  4.61it/s, loss=0.117, v_num=0, reduced_train_loss=0.00904, global_step=449.0, val_loss=0.0774]\n",
      "Epoch 5:  89%|████████▉ | 93/104 [00:18<00:02,  4.94it/s, loss=0.156, v_num=0, reduced_train_loss=0.588, global_step=449.0, val_loss=0.112]\n",
      "Epoch 5:  78%|███████▊  | 81/104 [00:17<00:04,  4.63it/s, loss=0.117, v_num=0, reduced_train_loss=0.00904, global_step=449.0, val_loss=0.0774]\n",
      "Epoch 5:  66%|██████▋   | 69/104 [00:15<00:07,  4.40it/s, loss=0.445, v_num=0, reduced_train_loss=0.564, global_step=443.0, val_loss=0.242]\n",
      "Epoch 5:  79%|███████▉  | 82/104 [00:17<00:04,  4.66it/s, loss=0.117, v_num=0, reduced_train_loss=0.00904, global_step=449.0, val_loss=0.0774]\n",
      "Epoch 5:  91%|█████████▏| 95/104 [00:19<00:01,  4.98it/s, loss=0.156, v_num=0, reduced_train_loss=0.588, global_step=449.0, val_loss=0.112]\n",
      "Epoch 5:  92%|█████████▏| 96/104 [00:19<00:01,  5.00it/s, loss=0.156, v_num=0, reduced_train_loss=0.588, global_step=449.0, val_loss=0.112]\n",
      "Epoch 5:  67%|██████▋   | 70/104 [00:15<00:07,  4.40it/s, loss=0.439, v_num=0, reduced_train_loss=0.352, global_step=444.0, val_loss=0.242]74]\n",
      "Epoch 5:  93%|█████████▎| 97/104 [00:19<00:01,  5.02it/s, loss=0.156, v_num=0, reduced_train_loss=0.588, global_step=449.0, val_loss=0.112]\n",
      "Epoch 5:  81%|████████  | 84/104 [00:17<00:04,  4.70it/s, loss=0.117, v_num=0, reduced_train_loss=0.00904, global_step=449.0, val_loss=0.0774]\n",
      "Epoch 5:  94%|█████████▍| 98/104 [00:19<00:01,  5.05it/s, loss=0.156, v_num=0, reduced_train_loss=0.588, global_step=449.0, val_loss=0.112]\n",
      "Epoch 5:  68%|██████▊   | 71/104 [00:16<00:07,  4.40it/s, loss=0.444, v_num=0, reduced_train_loss=0.506, global_step=445.0, val_loss=0.242]74]\n",
      "Epoch 5:  95%|█████████▌| 99/104 [00:19<00:00,  5.07it/s, loss=0.156, v_num=0, reduced_train_loss=0.588, global_step=449.0, val_loss=0.112]\n",
      "Epoch 5:  83%|████████▎ | 86/104 [00:18<00:03,  4.75it/s, loss=0.117, v_num=0, reduced_train_loss=0.00904, global_step=449.0, val_loss=0.0774]\n",
      "Epoch 5:  96%|█████████▌| 100/104 [00:19<00:00,  5.09it/s, loss=0.156, v_num=0, reduced_train_loss=0.588, global_step=449.0, val_loss=0.112]\n",
      "Epoch 5:  69%|██████▉   | 72/104 [00:16<00:07,  4.40it/s, loss=0.429, v_num=0, reduced_train_loss=0.344, global_step=446.0, val_loss=0.242]74]\n",
      "Epoch 5:  97%|█████████▋| 101/104 [00:19<00:00,  5.11it/s, loss=0.156, v_num=0, reduced_train_loss=0.588, global_step=449.0, val_loss=0.112]\n",
      "Epoch 5:  85%|████████▍ | 88/104 [00:18<00:03,  4.79it/s, loss=0.117, v_num=0, reduced_train_loss=0.00904, global_step=449.0, val_loss=0.0774]\n",
      "Epoch 5:  70%|███████   | 73/104 [00:16<00:07,  4.40it/s, loss=0.408, v_num=0, reduced_train_loss=0.176, global_step=447.0, val_loss=0.242]]\n",
      "Epoch 5:  86%|████████▌ | 89/104 [00:18<00:03,  4.82it/s, loss=0.117, v_num=0, reduced_train_loss=0.00904, global_step=449.0, val_loss=0.0774]\n",
      "Epoch 5:  99%|█████████▉| 103/104 [00:20<00:00,  5.15it/s, loss=0.156, v_num=0, reduced_train_loss=0.588, global_step=449.0, val_loss=0.112]\n",
      "Epoch 5:  87%|████████▋ | 90/104 [00:18<00:02,  4.84it/s, loss=0.117, v_num=0, reduced_train_loss=0.00904, global_step=449.0, val_loss=0.0774]\n",
      "Epoch 5: 100%|██████████| 104/104 [00:20<00:00,  5.18it/s, loss=0.156, v_num=0, reduced_train_loss=0.588, global_step=449.0, val_loss=0.112]2023-05-03 21:58:07,600 - root - INFO - val_loss: 0.09811785817146301\n",
      "Epoch 5: 100%|██████████| 104/104 [00:20<00:00,  5.18it/s, loss=0.156, v_num=0, reduced_train_loss=0.588, global_step=449.0, val_loss=0.0981]\n",
      "Epoch 6:   0%|          | 0/104 [00:00<?, ?it/s, loss=0.156, v_num=0, reduced_train_loss=0.588, global_step=449.0, val_loss=0.0981]          "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 21:58:07.600023 140522389899072 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.09811785817146301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:  71%|███████   | 74/104 [00:16<00:06,  4.40it/s, loss=0.404, v_num=0, reduced_train_loss=0.165, global_step=448.0, val_loss=0.242]\n",
      "Epoch 5:  88%|████████▊ | 91/104 [00:18<00:02,  4.86it/s, loss=0.117, v_num=0, reduced_train_loss=0.00904, global_step=449.0, val_loss=0.0774]\n",
      "Epoch 5:  72%|███████▏  | 75/104 [00:17<00:06,  4.40it/s, loss=0.406, v_num=0, reduced_train_loss=0.389, global_step=449.0, val_loss=0.242]74]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5:  89%|████████▉ | 93/104 [00:18<00:02,  4.91it/s, loss=0.117, v_num=0, reduced_train_loss=0.00904, global_step=449.0, val_loss=0.0774]\n",
      "Epoch 6:   2%|▏         | 2/104 [00:00<00:24,  4.17it/s, loss=0.149, v_num=0, reduced_train_loss=0.0163, global_step=451.0, val_loss=0.0981]4]\n",
      "Epoch 5:  73%|███████▎  | 76/104 [00:17<00:06,  4.41it/s, loss=0.406, v_num=0, reduced_train_loss=0.389, global_step=449.0, val_loss=0.242]\n",
      "Epoch 5:  91%|█████████▏| 95/104 [00:19<00:01,  4.95it/s, loss=0.117, v_num=0, reduced_train_loss=0.00904, global_step=449.0, val_loss=0.0774]\n",
      "Epoch 5:  74%|███████▍  | 77/104 [00:17<00:06,  4.43it/s, loss=0.406, v_num=0, reduced_train_loss=0.389, global_step=449.0, val_loss=0.242]\n",
      "Epoch 6:   3%|▎         | 3/104 [00:00<00:23,  4.31it/s, loss=0.138, v_num=0, reduced_train_loss=0.00239, global_step=452.0, val_loss=0.0981]]\n",
      "Epoch 5:  75%|███████▌  | 78/104 [00:17<00:05,  4.46it/s, loss=0.406, v_num=0, reduced_train_loss=0.389, global_step=449.0, val_loss=0.242]\n",
      "Epoch 5:  93%|█████████▎| 97/104 [00:19<00:01,  4.99it/s, loss=0.117, v_num=0, reduced_train_loss=0.00904, global_step=449.0, val_loss=0.0774]\n",
      "Epoch 6:   4%|▍         | 4/104 [00:00<00:22,  4.38it/s, loss=0.136, v_num=0, reduced_train_loss=0.178, global_step=453.0, val_loss=0.0981]  \n",
      "Epoch 5:  94%|█████████▍| 98/104 [00:19<00:01,  5.01it/s, loss=0.117, v_num=0, reduced_train_loss=0.00904, global_step=449.0, val_loss=0.0774]\n",
      "Epoch 5:  77%|███████▋  | 80/104 [00:17<00:05,  4.51it/s, loss=0.406, v_num=0, reduced_train_loss=0.389, global_step=449.0, val_loss=0.242]\n",
      "Epoch 5:  95%|█████████▌| 99/104 [00:19<00:00,  5.03it/s, loss=0.117, v_num=0, reduced_train_loss=0.00904, global_step=449.0, val_loss=0.0774]\n",
      "Epoch 6:   5%|▍         | 5/104 [00:01<00:22,  4.41it/s, loss=0.138, v_num=0, reduced_train_loss=0.180, global_step=454.0, val_loss=0.0981]\n",
      "Epoch 5:  96%|█████████▌| 100/104 [00:19<00:00,  5.05it/s, loss=0.117, v_num=0, reduced_train_loss=0.00904, global_step=449.0, val_loss=0.0774]\n",
      "Epoch 5:  79%|███████▉  | 82/104 [00:18<00:04,  4.55it/s, loss=0.406, v_num=0, reduced_train_loss=0.389, global_step=449.0, val_loss=0.242]\n",
      "Epoch 6:   6%|▌         | 6/104 [00:01<00:22,  4.44it/s, loss=0.157, v_num=0, reduced_train_loss=0.408, global_step=455.0, val_loss=0.0981]774]\n",
      "Epoch 5:  80%|███████▉  | 83/104 [00:18<00:04,  4.58it/s, loss=0.406, v_num=0, reduced_train_loss=0.389, global_step=449.0, val_loss=0.242]\n",
      "Epoch 5:  98%|█████████▊| 102/104 [00:20<00:00,  5.09it/s, loss=0.117, v_num=0, reduced_train_loss=0.00904, global_step=449.0, val_loss=0.0774]\n",
      "Epoch 5:  81%|████████  | 84/104 [00:18<00:04,  4.60it/s, loss=0.406, v_num=0, reduced_train_loss=0.389, global_step=449.0, val_loss=0.242]\n",
      "Epoch 6:   7%|▋         | 7/104 [00:01<00:21,  4.47it/s, loss=0.134, v_num=0, reduced_train_loss=0.0072, global_step=456.0, val_loss=0.0981]74]\n",
      "Epoch 5: 100%|██████████| 104/104 [00:20<00:00,  5.15it/s, loss=0.117, v_num=0, reduced_train_loss=0.00904, global_step=449.0, val_loss=0.0774]2023-05-03 21:58:09,201 - root - INFO - val_loss: 0.08989561349153519\n",
      "Epoch 5: 100%|██████████| 104/104 [00:20<00:00,  5.14it/s, loss=0.117, v_num=0, reduced_train_loss=0.00904, global_step=449.0, val_loss=0.0899]\n",
      "Epoch 6:   0%|          | 0/104 [00:00<?, ?it/s, loss=0.117, v_num=0, reduced_train_loss=0.00904, global_step=449.0, val_loss=0.0899]          \n",
      "Epoch 5:  82%|████████▏ | 85/104 [00:18<00:04,  4.62it/s, loss=0.406, v_num=0, reduced_train_loss=0.389, global_step=449.0, val_loss=0.242]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 21:58:09.201494 140649257482048 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.08989561349153519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6:   1%|          | 1/104 [00:00<00:27,  3.71it/s, loss=0.114, v_num=0, reduced_train_loss=0.00804, global_step=450.0, val_loss=0.0899]\n",
      "Epoch 6:   9%|▊         | 9/104 [00:02<00:21,  4.48it/s, loss=0.12, v_num=0, reduced_train_loss=0.00215, global_step=458.0, val_loss=0.0981] \n",
      "Epoch 6:   2%|▏         | 2/104 [00:00<00:25,  4.01it/s, loss=0.117, v_num=0, reduced_train_loss=0.110, global_step=451.0, val_loss=0.0899]  \n",
      "Epoch 6:  10%|▉         | 10/104 [00:02<00:20,  4.49it/s, loss=0.12, v_num=0, reduced_train_loss=0.00908, global_step=459.0, val_loss=0.0981]\n",
      "Epoch 6:   3%|▎         | 3/104 [00:00<00:24,  4.15it/s, loss=0.103, v_num=0, reduced_train_loss=0.0734, global_step=452.0, val_loss=0.0899]\n",
      "Epoch 6:  11%|█         | 11/104 [00:02<00:20,  4.49it/s, loss=0.128, v_num=0, reduced_train_loss=0.248, global_step=460.0, val_loss=0.0981] \n",
      "Epoch 6:   4%|▍         | 4/104 [00:00<00:23,  4.23it/s, loss=0.114, v_num=0, reduced_train_loss=0.291, global_step=453.0, val_loss=0.0899] \n",
      "Epoch 6:   5%|▍         | 5/104 [00:01<00:23,  4.28it/s, loss=0.114, v_num=0, reduced_train_loss=0.291, global_step=453.0, val_loss=0.0899]1]\n",
      "Epoch 6:  12%|█▎        | 13/104 [00:02<00:20,  4.51it/s, loss=0.127, v_num=0, reduced_train_loss=0.187, global_step=462.0, val_loss=0.0981] \n",
      "Epoch 6:   6%|▌         | 6/104 [00:01<00:22,  4.29it/s, loss=0.103, v_num=0, reduced_train_loss=0.00477, global_step=455.0, val_loss=0.0899]\n",
      "Epoch 6:  13%|█▎        | 14/104 [00:03<00:19,  4.51it/s, loss=0.132, v_num=0, reduced_train_loss=0.0919, global_step=463.0, val_loss=0.0981]\n",
      "Epoch 6:   7%|▋         | 7/104 [00:01<00:22,  4.32it/s, loss=0.109, v_num=0, reduced_train_loss=0.232, global_step=456.0, val_loss=0.0899]  \n",
      "Epoch 6:  14%|█▍        | 15/104 [00:03<00:19,  4.52it/s, loss=0.12, v_num=0, reduced_train_loss=0.0935, global_step=464.0, val_loss=0.0981] \n",
      "Epoch 6:   8%|▊         | 8/104 [00:01<00:22,  4.34it/s, loss=0.109, v_num=0, reduced_train_loss=0.0331, global_step=457.0, val_loss=0.0899]\n",
      "Epoch 6:  15%|█▌        | 16/104 [00:03<00:19,  4.52it/s, loss=0.127, v_num=0, reduced_train_loss=0.152, global_step=465.0, val_loss=0.0981]\n",
      "Epoch 6:  16%|█▋        | 17/104 [00:03<00:19,  4.53it/s, loss=0.127, v_num=0, reduced_train_loss=0.00218, global_step=466.0, val_loss=0.0981]\n",
      "Epoch 6:  10%|▉         | 10/104 [00:02<00:21,  4.37it/s, loss=0.0994, v_num=0, reduced_train_loss=0.0303, global_step=459.0, val_loss=0.0899]\n",
      "Epoch 5:  99%|█████████▉| 103/104 [00:20<00:00,  4.99it/s, loss=0.406, v_num=0, reduced_train_loss=0.389, global_step=449.0, val_loss=0.242]\n",
      "Epoch 5: 100%|██████████| 104/104 [00:20<00:00,  5.02it/s, loss=0.406, v_num=0, reduced_train_loss=0.389, global_step=449.0, val_loss=0.242]2023-05-03 21:58:11,569 - root - INFO - val_loss: 0.23421069979667664\n",
      "Epoch 5: 100%|██████████| 104/104 [00:20<00:00,  5.02it/s, loss=0.406, v_num=0, reduced_train_loss=0.389, global_step=449.0, val_loss=0.234]\n",
      "Epoch 6:  11%|█         | 11/104 [00:02<00:21,  4.37it/s, loss=0.104, v_num=0, reduced_train_loss=0.158, global_step=460.0, val_loss=0.0899]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 21:58:11.569724 139755592410944 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.23421069979667664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6:  68%|██████▊   | 71/104 [00:15<00:07,  4.48it/s, loss=0.209, v_num=0, reduced_train_loss=0.0167, global_step=520.0, val_loss=0.0899]  \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6:  69%|██████▉   | 72/104 [00:16<00:07,  4.48it/s, loss=0.208, v_num=0, reduced_train_loss=0.375, global_step=521.0, val_loss=0.0899] \n",
      "Epoch 6:  58%|█████▊    | 60/104 [00:13<00:10,  4.32it/s, loss=0.232, v_num=0, reduced_train_loss=0.128, global_step=509.0, val_loss=0.234]]\n",
      "Epoch 6:  70%|███████   | 73/104 [00:16<00:06,  4.48it/s, loss=0.183, v_num=0, reduced_train_loss=0.00416, global_step=522.0, val_loss=0.0899]\n",
      "Epoch 6:  59%|█████▊    | 61/104 [00:14<00:09,  4.32it/s, loss=0.234, v_num=0, reduced_train_loss=0.159, global_step=510.0, val_loss=0.234]]\n",
      "Epoch 6:  71%|███████   | 74/104 [00:16<00:06,  4.48it/s, loss=0.186, v_num=0, reduced_train_loss=0.0563, global_step=523.0, val_loss=0.0899] \n",
      "Epoch 6:  72%|███████▏  | 75/104 [00:16<00:06,  4.48it/s, loss=0.179, v_num=0, reduced_train_loss=0.0785, global_step=524.0, val_loss=0.0899]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6:  79%|███████▉  | 82/104 [00:18<00:04,  4.47it/s, loss=0.144, v_num=0, reduced_train_loss=0.186, global_step=524.0, val_loss=0.0981]\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6:  80%|███████▉  | 83/104 [00:18<00:04,  4.50it/s, loss=0.144, v_num=0, reduced_train_loss=0.186, global_step=524.0, val_loss=0.0981]\n",
      "Epoch 6:  61%|██████    | 63/104 [00:14<00:09,  4.33it/s, loss=0.24, v_num=0, reduced_train_loss=0.230, global_step=512.0, val_loss=0.234] 9]\n",
      "Epoch 6:  81%|████████  | 84/104 [00:18<00:04,  4.52it/s, loss=0.144, v_num=0, reduced_train_loss=0.186, global_step=524.0, val_loss=0.0981]\n",
      "Epoch 6:  74%|███████▍  | 77/104 [00:17<00:05,  4.52it/s, loss=0.179, v_num=0, reduced_train_loss=0.0785, global_step=524.0, val_loss=0.0899]\n",
      "Epoch 6:  82%|████████▏ | 85/104 [00:18<00:04,  4.55it/s, loss=0.144, v_num=0, reduced_train_loss=0.186, global_step=524.0, val_loss=0.0981]\n",
      "Epoch 6:  62%|██████▏   | 64/104 [00:14<00:09,  4.33it/s, loss=0.243, v_num=0, reduced_train_loss=0.322, global_step=513.0, val_loss=0.234]9]\n",
      "Epoch 6:  83%|████████▎ | 86/104 [00:18<00:03,  4.57it/s, loss=0.144, v_num=0, reduced_train_loss=0.186, global_step=524.0, val_loss=0.0981]\n",
      "Epoch 6:  76%|███████▌  | 79/104 [00:17<00:05,  4.57it/s, loss=0.179, v_num=0, reduced_train_loss=0.0785, global_step=524.0, val_loss=0.0899]\n",
      "Epoch 6:  62%|██████▎   | 65/104 [00:15<00:09,  4.33it/s, loss=0.238, v_num=0, reduced_train_loss=0.0746, global_step=514.0, val_loss=0.234]\n",
      "Epoch 6:  77%|███████▋  | 80/104 [00:17<00:05,  4.60it/s, loss=0.179, v_num=0, reduced_train_loss=0.0785, global_step=524.0, val_loss=0.0899]\n",
      "Epoch 6:  85%|████████▍ | 88/104 [00:19<00:03,  4.62it/s, loss=0.144, v_num=0, reduced_train_loss=0.186, global_step=524.0, val_loss=0.0981]\n",
      "Epoch 6:  78%|███████▊  | 81/104 [00:17<00:04,  4.62it/s, loss=0.179, v_num=0, reduced_train_loss=0.0785, global_step=524.0, val_loss=0.0899]\n",
      "Epoch 6:  63%|██████▎   | 66/104 [00:15<00:08,  4.33it/s, loss=0.223, v_num=0, reduced_train_loss=0.0336, global_step=515.0, val_loss=0.234]\n",
      "Epoch 6:  79%|███████▉  | 82/104 [00:17<00:04,  4.65it/s, loss=0.179, v_num=0, reduced_train_loss=0.0785, global_step=524.0, val_loss=0.0899]\n",
      "Epoch 6:  87%|████████▋ | 90/104 [00:19<00:03,  4.66it/s, loss=0.144, v_num=0, reduced_train_loss=0.186, global_step=524.0, val_loss=0.0981]\n",
      "Epoch 6:  80%|███████▉  | 83/104 [00:17<00:04,  4.67it/s, loss=0.179, v_num=0, reduced_train_loss=0.0785, global_step=524.0, val_loss=0.0899]\n",
      "Epoch 6:  64%|██████▍   | 67/104 [00:15<00:08,  4.33it/s, loss=0.229, v_num=0, reduced_train_loss=0.357, global_step=516.0, val_loss=0.234] \n",
      "Epoch 6:  81%|████████  | 84/104 [00:17<00:04,  4.70it/s, loss=0.179, v_num=0, reduced_train_loss=0.0785, global_step=524.0, val_loss=0.0899]\n",
      "Epoch 6:  88%|████████▊ | 92/104 [00:19<00:02,  4.71it/s, loss=0.144, v_num=0, reduced_train_loss=0.186, global_step=524.0, val_loss=0.0981]\n",
      "Epoch 6:  82%|████████▏ | 85/104 [00:18<00:04,  4.72it/s, loss=0.179, v_num=0, reduced_train_loss=0.0785, global_step=524.0, val_loss=0.0899]\n",
      "Epoch 6:  65%|██████▌   | 68/104 [00:15<00:08,  4.33it/s, loss=0.216, v_num=0, reduced_train_loss=0.084, global_step=517.0, val_loss=0.234]]\n",
      "Epoch 6:  83%|████████▎ | 86/104 [00:18<00:03,  4.74it/s, loss=0.179, v_num=0, reduced_train_loss=0.0785, global_step=524.0, val_loss=0.0899]\n",
      "Epoch 6:  90%|█████████ | 94/104 [00:19<00:02,  4.76it/s, loss=0.144, v_num=0, reduced_train_loss=0.186, global_step=524.0, val_loss=0.0981]\n",
      "Epoch 6:  84%|████████▎ | 87/104 [00:18<00:03,  4.77it/s, loss=0.179, v_num=0, reduced_train_loss=0.0785, global_step=524.0, val_loss=0.0899]\n",
      "Epoch 6:  66%|██████▋   | 69/104 [00:15<00:08,  4.33it/s, loss=0.214, v_num=0, reduced_train_loss=0.0691, global_step=518.0, val_loss=0.234]\n",
      "Epoch 6:  85%|████████▍ | 88/104 [00:18<00:03,  4.79it/s, loss=0.179, v_num=0, reduced_train_loss=0.0785, global_step=524.0, val_loss=0.0899]\n",
      "Epoch 6:  92%|█████████▏| 96/104 [00:20<00:01,  4.80it/s, loss=0.144, v_num=0, reduced_train_loss=0.186, global_step=524.0, val_loss=0.0981]\n",
      "Epoch 6:  86%|████████▌ | 89/104 [00:18<00:03,  4.81it/s, loss=0.179, v_num=0, reduced_train_loss=0.0785, global_step=524.0, val_loss=0.0899]\n",
      "Epoch 6:  67%|██████▋   | 70/104 [00:16<00:07,  4.33it/s, loss=0.208, v_num=0, reduced_train_loss=0.197, global_step=519.0, val_loss=0.234] \n",
      "Epoch 6:  87%|████████▋ | 90/104 [00:18<00:02,  4.84it/s, loss=0.179, v_num=0, reduced_train_loss=0.0785, global_step=524.0, val_loss=0.0899]\n",
      "Epoch 6:  94%|█████████▍| 98/104 [00:20<00:01,  4.84it/s, loss=0.144, v_num=0, reduced_train_loss=0.186, global_step=524.0, val_loss=0.0981]\n",
      "Epoch 6:  88%|████████▊ | 91/104 [00:18<00:02,  4.86it/s, loss=0.179, v_num=0, reduced_train_loss=0.0785, global_step=524.0, val_loss=0.0899]\n",
      "Epoch 6:  68%|██████▊   | 71/104 [00:16<00:07,  4.33it/s, loss=0.205, v_num=0, reduced_train_loss=0.261, global_step=520.0, val_loss=0.234]]\n",
      "Epoch 6:  88%|████████▊ | 92/104 [00:18<00:02,  4.88it/s, loss=0.179, v_num=0, reduced_train_loss=0.0785, global_step=524.0, val_loss=0.0899]\n",
      "Epoch 6:  96%|█████████▌| 100/104 [00:20<00:00,  4.88it/s, loss=0.144, v_num=0, reduced_train_loss=0.186, global_step=524.0, val_loss=0.0981]\n",
      "Epoch 6:  69%|██████▉   | 72/104 [00:16<00:07,  4.33it/s, loss=0.202, v_num=0, reduced_train_loss=0.328, global_step=521.0, val_loss=0.234]9]\n",
      "Epoch 6:  97%|█████████▋| 101/104 [00:20<00:00,  4.91it/s, loss=0.144, v_num=0, reduced_train_loss=0.186, global_step=524.0, val_loss=0.0981]\n",
      "Epoch 6:  90%|█████████ | 94/104 [00:19<00:02,  4.92it/s, loss=0.179, v_num=0, reduced_train_loss=0.0785, global_step=524.0, val_loss=0.0899]\n",
      "Epoch 6:  70%|███████   | 73/104 [00:16<00:07,  4.34it/s, loss=0.207, v_num=0, reduced_train_loss=0.262, global_step=522.0, val_loss=0.234]1]\n",
      "Epoch 6:  91%|█████████▏| 95/104 [00:19<00:01,  4.95it/s, loss=0.179, v_num=0, reduced_train_loss=0.0785, global_step=524.0, val_loss=0.0899]\n",
      "Epoch 6:  99%|█████████▉| 103/104 [00:20<00:00,  4.95it/s, loss=0.144, v_num=0, reduced_train_loss=0.186, global_step=524.0, val_loss=0.0981]\n",
      "Epoch 6: 100%|██████████| 104/104 [00:20<00:00,  4.98it/s, loss=0.144, v_num=0, reduced_train_loss=0.186, global_step=524.0, val_loss=0.0981]2023-05-03 21:58:28,492 - root - INFO - val_loss: 0.06033783778548241\n",
      "Epoch 6: 100%|██████████| 104/104 [00:20<00:00,  4.98it/s, loss=0.144, v_num=0, reduced_train_loss=0.186, global_step=524.0, val_loss=0.0603]\n",
      "Epoch 7:   0%|          | 0/104 [00:00<?, ?it/s, loss=0.144, v_num=0, reduced_train_loss=0.186, global_step=524.0, val_loss=0.0603]          \n",
      "Epoch 6:  92%|█████████▏| 96/104 [00:19<00:01,  4.97it/s, loss=0.179, v_num=0, reduced_train_loss=0.0785, global_step=524.0, val_loss=0.0899]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 21:58:28.492641 140522389899072 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.06033783778548241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6:  71%|███████   | 74/104 [00:17<00:06,  4.34it/s, loss=0.206, v_num=0, reduced_train_loss=0.247, global_step=523.0, val_loss=0.234]\n",
      "Epoch 6:  93%|█████████▎| 97/104 [00:19<00:01,  4.99it/s, loss=0.179, v_num=0, reduced_train_loss=0.0785, global_step=524.0, val_loss=0.0899]\n",
      "Epoch 6:  72%|███████▏  | 75/104 [00:17<00:06,  4.34it/s, loss=0.2, v_num=0, reduced_train_loss=0.274, global_step=524.0, val_loss=0.234]  ]]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6:  95%|█████████▌| 99/104 [00:19<00:00,  5.03it/s, loss=0.179, v_num=0, reduced_train_loss=0.0785, global_step=524.0, val_loss=0.0899]\n",
      "Epoch 7:   2%|▏         | 2/104 [00:00<00:26,  3.80it/s, loss=0.125, v_num=0, reduced_train_loss=0.0141, global_step=526.0, val_loss=0.0603]9]\n",
      "Epoch 6:  73%|███████▎  | 76/104 [00:17<00:06,  4.35it/s, loss=0.2, v_num=0, reduced_train_loss=0.274, global_step=524.0, val_loss=0.234]\n",
      "Epoch 6:  97%|█████████▋| 101/104 [00:19<00:00,  5.07it/s, loss=0.179, v_num=0, reduced_train_loss=0.0785, global_step=524.0, val_loss=0.0899]\n",
      "Epoch 6:  74%|███████▍  | 77/104 [00:17<00:06,  4.37it/s, loss=0.2, v_num=0, reduced_train_loss=0.274, global_step=524.0, val_loss=0.234]\n",
      "Epoch 7:   3%|▎         | 3/104 [00:00<00:25,  3.92it/s, loss=0.11, v_num=0, reduced_train_loss=0.0177, global_step=527.0, val_loss=0.0603] 9]\n",
      "Epoch 6:  75%|███████▌  | 78/104 [00:17<00:05,  4.40it/s, loss=0.2, v_num=0, reduced_train_loss=0.274, global_step=524.0, val_loss=0.234]\n",
      "Epoch 6:  99%|█████████▉| 103/104 [00:20<00:00,  5.11it/s, loss=0.179, v_num=0, reduced_train_loss=0.0785, global_step=524.0, val_loss=0.0899]\n",
      "Epoch 6:  76%|███████▌  | 79/104 [00:17<00:05,  4.43it/s, loss=0.2, v_num=0, reduced_train_loss=0.274, global_step=524.0, val_loss=0.234]\n",
      "Epoch 6: 100%|██████████| 104/104 [00:20<00:00,  5.14it/s, loss=0.179, v_num=0, reduced_train_loss=0.0785, global_step=524.0, val_loss=0.0899]2023-05-03 21:58:29,437 - root - INFO - val_loss: 0.07053378224372864\n",
      "Epoch 6: 100%|██████████| 104/104 [00:20<00:00,  5.14it/s, loss=0.179, v_num=0, reduced_train_loss=0.0785, global_step=524.0, val_loss=0.0705]\n",
      "Epoch 7:   4%|▍         | 4/104 [00:01<00:25,  3.94it/s, loss=0.105, v_num=0, reduced_train_loss=0.123, global_step=528.0, val_loss=0.0603]   \n",
      "Epoch 6:  77%|███████▋  | 80/104 [00:17<00:05,  4.45it/s, loss=0.2, v_num=0, reduced_train_loss=0.274, global_step=524.0, val_loss=0.234]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 21:58:29.437052 140649257482048 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.07053378224372864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:   5%|▍         | 5/104 [00:01<00:25,  3.96it/s, loss=0.101, v_num=0, reduced_train_loss=0.113, global_step=529.0, val_loss=0.0603]]\n",
      "Epoch 7:   2%|▏         | 2/104 [00:00<00:25,  4.08it/s, loss=0.167, v_num=0, reduced_train_loss=0.293, global_step=526.0, val_loss=0.0705] \n",
      "Epoch 7:   6%|▌         | 6/104 [00:01<00:24,  3.96it/s, loss=0.104, v_num=0, reduced_train_loss=0.0709, global_step=530.0, val_loss=0.0603]\n",
      "Epoch 7:   3%|▎         | 3/104 [00:00<00:24,  4.21it/s, loss=0.151, v_num=0, reduced_train_loss=0.0156, global_step=527.0, val_loss=0.0705]\n",
      "Epoch 7:   7%|▋         | 7/104 [00:01<00:24,  3.97it/s, loss=0.107, v_num=0, reduced_train_loss=0.098, global_step=531.0, val_loss=0.0603] \n",
      "Epoch 7:   4%|▍         | 4/104 [00:00<00:23,  4.27it/s, loss=0.163, v_num=0, reduced_train_loss=0.321, global_step=528.0, val_loss=0.0705] \n",
      "Epoch 7:   8%|▊         | 8/104 [00:02<00:24,  3.97it/s, loss=0.113, v_num=0, reduced_train_loss=0.322, global_step=532.0, val_loss=0.0603]\n",
      "Epoch 7:   5%|▍         | 5/104 [00:01<00:22,  4.32it/s, loss=0.147, v_num=0, reduced_train_loss=0.0144, global_step=529.0, val_loss=0.0705]\n",
      "Epoch 7:   9%|▊         | 9/104 [00:02<00:23,  3.98it/s, loss=0.112, v_num=0, reduced_train_loss=0.370, global_step=533.0, val_loss=0.0603]\n",
      "Epoch 7:   6%|▌         | 6/104 [00:01<00:22,  4.34it/s, loss=0.144, v_num=0, reduced_train_loss=0.103, global_step=530.0, val_loss=0.0705] \n",
      "Epoch 7:  10%|▉         | 10/104 [00:02<00:23,  3.99it/s, loss=0.107, v_num=0, reduced_train_loss=0.021, global_step=534.0, val_loss=0.0603]\n",
      "Epoch 7:   7%|▋         | 7/104 [00:01<00:22,  4.37it/s, loss=0.147, v_num=0, reduced_train_loss=0.220, global_step=531.0, val_loss=0.0705]\n",
      "Epoch 7:   8%|▊         | 8/104 [00:01<00:21,  4.39it/s, loss=0.143, v_num=0, reduced_train_loss=0.0253, global_step=532.0, val_loss=0.0705]3]\n",
      "Epoch 6:  90%|█████████ | 94/104 [00:19<00:02,  4.77it/s, loss=0.2, v_num=0, reduced_train_loss=0.274, global_step=524.0, val_loss=0.234]\n",
      "Epoch 7:  12%|█▏        | 12/104 [00:03<00:23,  4.00it/s, loss=0.104, v_num=0, reduced_train_loss=0.00618, global_step=536.0, val_loss=0.0603]\n",
      "Epoch 6:  92%|█████████▏| 96/104 [00:19<00:01,  4.82it/s, loss=0.2, v_num=0, reduced_train_loss=0.274, global_step=524.0, val_loss=0.234]\n",
      "Epoch 7:  12%|█▎        | 13/104 [00:03<00:22,  4.00it/s, loss=0.0837, v_num=0, reduced_train_loss=0.0307, global_step=537.0, val_loss=0.0603]\n",
      "Epoch 6:  94%|█████████▍| 98/104 [00:20<00:01,  4.86it/s, loss=0.2, v_num=0, reduced_train_loss=0.274, global_step=524.0, val_loss=0.234]\n",
      "Epoch 7:  11%|█         | 11/104 [00:02<00:21,  4.42it/s, loss=0.142, v_num=0, reduced_train_loss=0.147, global_step=535.0, val_loss=0.0705]\n",
      "Epoch 7:  13%|█▎        | 14/104 [00:03<00:22,  4.00it/s, loss=0.0829, v_num=0, reduced_train_loss=0.0125, global_step=538.0, val_loss=0.0603]\n",
      "Epoch 7:  12%|█▏        | 12/104 [00:02<00:20,  4.43it/s, loss=0.122, v_num=0, reduced_train_loss=0.0236, global_step=536.0, val_loss=0.0705]\n",
      "Epoch 7:  14%|█▍        | 15/104 [00:03<00:22,  4.01it/s, loss=0.0798, v_num=0, reduced_train_loss=0.00237, global_step=539.0, val_loss=0.0603]\n",
      "Epoch 7:  12%|█▎        | 13/104 [00:02<00:20,  4.44it/s, loss=0.122, v_num=0, reduced_train_loss=0.0652, global_step=537.0, val_loss=0.0705]\n",
      "Epoch 6: 100%|██████████| 104/104 [00:20<00:00,  4.99it/s, loss=0.2, v_num=0, reduced_train_loss=0.274, global_step=524.0, val_loss=0.234]2023-05-03 21:58:32,413 - root - INFO - val_loss: 0.2053402066230774\n",
      "Epoch 6: 100%|██████████| 104/104 [00:20<00:00,  4.99it/s, loss=0.2, v_num=0, reduced_train_loss=0.274, global_step=524.0, val_loss=0.205]\n",
      "Epoch 7:  15%|█▌        | 16/104 [00:03<00:21,  4.01it/s, loss=0.0792, v_num=0, reduced_train_loss=0.00197, global_step=540.0, val_loss=0.0603]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 21:58:32.413452 139755592410944 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.2053402066230774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7:  72%|███████▏  | 75/104 [00:16<00:06,  4.47it/s, loss=0.0647, v_num=0, reduced_train_loss=0.0173, global_step=599.0, val_loss=0.0705]]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7:  69%|██████▉   | 72/104 [00:17<00:07,  4.01it/s, loss=0.174, v_num=0, reduced_train_loss=0.0425, global_step=596.0, val_loss=0.0603]  \n",
      "Epoch 7:  59%|█████▊    | 61/104 [00:14<00:10,  4.29it/s, loss=0.349, v_num=0, reduced_train_loss=0.408, global_step=585.0, val_loss=0.205] 5]\n",
      "Epoch 7:  70%|███████   | 73/104 [00:18<00:07,  4.01it/s, loss=0.17, v_num=0, reduced_train_loss=0.114, global_step=597.0, val_loss=0.0603]  ]\n",
      "Epoch 7:  60%|█████▉    | 62/104 [00:14<00:09,  4.30it/s, loss=0.349, v_num=0, reduced_train_loss=0.211, global_step=586.0, val_loss=0.205]05]\n",
      "Epoch 7:  71%|███████   | 74/104 [00:18<00:07,  4.01it/s, loss=0.168, v_num=0, reduced_train_loss=0.0497, global_step=598.0, val_loss=0.0603]]\n",
      "Epoch 7:  61%|██████    | 63/104 [00:14<00:09,  4.30it/s, loss=0.337, v_num=0, reduced_train_loss=0.263, global_step=587.0, val_loss=0.205]05]\n",
      "Epoch 7:  72%|███████▏  | 75/104 [00:18<00:07,  4.01it/s, loss=0.161, v_num=0, reduced_train_loss=0.00395, global_step=599.0, val_loss=0.0603]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7:  62%|██████▏   | 64/104 [00:14<00:09,  4.30it/s, loss=0.334, v_num=0, reduced_train_loss=0.219, global_step=588.0, val_loss=0.205]05]\n",
      "Epoch 7:  73%|███████▎  | 76/104 [00:18<00:06,  4.03it/s, loss=0.161, v_num=0, reduced_train_loss=0.00395, global_step=599.0, val_loss=0.0603]\n",
      "Epoch 7:  81%|████████  | 84/104 [00:17<00:04,  4.69it/s, loss=0.0647, v_num=0, reduced_train_loss=0.0173, global_step=599.0, val_loss=0.0705]\n",
      "Epoch 7:  74%|███████▍  | 77/104 [00:18<00:06,  4.05it/s, loss=0.161, v_num=0, reduced_train_loss=0.00395, global_step=599.0, val_loss=0.0603]\n",
      "Epoch 7:  62%|██████▎   | 65/104 [00:15<00:09,  4.30it/s, loss=0.321, v_num=0, reduced_train_loss=0.119, global_step=589.0, val_loss=0.205]05]\n",
      "Epoch 7:  75%|███████▌  | 78/104 [00:19<00:06,  4.08it/s, loss=0.161, v_num=0, reduced_train_loss=0.00395, global_step=599.0, val_loss=0.0603]\n",
      "Epoch 7:  83%|████████▎ | 86/104 [00:18<00:03,  4.73it/s, loss=0.0647, v_num=0, reduced_train_loss=0.0173, global_step=599.0, val_loss=0.0705]\n",
      "Epoch 7:  76%|███████▌  | 79/104 [00:19<00:06,  4.11it/s, loss=0.161, v_num=0, reduced_train_loss=0.00395, global_step=599.0, val_loss=0.0603]\n",
      "Epoch 7:  63%|██████▎   | 66/104 [00:15<00:08,  4.30it/s, loss=0.3, v_num=0, reduced_train_loss=0.0468, global_step=590.0, val_loss=0.205] 05]\n",
      "Epoch 7:  77%|███████▋  | 80/104 [00:19<00:05,  4.13it/s, loss=0.161, v_num=0, reduced_train_loss=0.00395, global_step=599.0, val_loss=0.0603]\n",
      "Epoch 7:  85%|████████▍ | 88/104 [00:18<00:03,  4.78it/s, loss=0.0647, v_num=0, reduced_train_loss=0.0173, global_step=599.0, val_loss=0.0705]\n",
      "Epoch 7:  78%|███████▊  | 81/104 [00:19<00:05,  4.16it/s, loss=0.161, v_num=0, reduced_train_loss=0.00395, global_step=599.0, val_loss=0.0603]\n",
      "Epoch 7:  64%|██████▍   | 67/104 [00:15<00:08,  4.30it/s, loss=0.283, v_num=0, reduced_train_loss=0.149, global_step=591.0, val_loss=0.205]05]\n",
      "Epoch 7:  79%|███████▉  | 82/104 [00:19<00:05,  4.19it/s, loss=0.161, v_num=0, reduced_train_loss=0.00395, global_step=599.0, val_loss=0.0603]\n",
      "Epoch 7:  65%|██████▌   | 68/104 [00:15<00:08,  4.31it/s, loss=0.28, v_num=0, reduced_train_loss=0.139, global_step=592.0, val_loss=0.205] 05]\n",
      "Epoch 7:  80%|███████▉  | 83/104 [00:19<00:04,  4.21it/s, loss=0.161, v_num=0, reduced_train_loss=0.00395, global_step=599.0, val_loss=0.0603]\n",
      "Epoch 7:  88%|████████▊ | 91/104 [00:18<00:02,  4.85it/s, loss=0.0647, v_num=0, reduced_train_loss=0.0173, global_step=599.0, val_loss=0.0705]\n",
      "Epoch 7:  81%|████████  | 84/104 [00:19<00:04,  4.24it/s, loss=0.161, v_num=0, reduced_train_loss=0.00395, global_step=599.0, val_loss=0.0603]\n",
      "Epoch 7:  66%|██████▋   | 69/104 [00:16<00:08,  4.31it/s, loss=0.281, v_num=0, reduced_train_loss=0.177, global_step=593.0, val_loss=0.205]05]\n",
      "Epoch 7:  82%|████████▏ | 85/104 [00:19<00:04,  4.26it/s, loss=0.161, v_num=0, reduced_train_loss=0.00395, global_step=599.0, val_loss=0.0603]\n",
      "Epoch 7:  89%|████████▉ | 93/104 [00:19<00:02,  4.89it/s, loss=0.0647, v_num=0, reduced_train_loss=0.0173, global_step=599.0, val_loss=0.0705]\n",
      "Epoch 7:  83%|████████▎ | 86/104 [00:20<00:04,  4.28it/s, loss=0.161, v_num=0, reduced_train_loss=0.00395, global_step=599.0, val_loss=0.0603]\n",
      "Epoch 7:  67%|██████▋   | 70/104 [00:16<00:07,  4.31it/s, loss=0.277, v_num=0, reduced_train_loss=0.0738, global_step=594.0, val_loss=0.205]5]\n",
      "Epoch 7:  84%|████████▎ | 87/104 [00:20<00:03,  4.31it/s, loss=0.161, v_num=0, reduced_train_loss=0.00395, global_step=599.0, val_loss=0.0603]\n",
      "Epoch 7:  91%|█████████▏| 95/104 [00:19<00:01,  4.93it/s, loss=0.0647, v_num=0, reduced_train_loss=0.0173, global_step=599.0, val_loss=0.0705]\n",
      "Epoch 7:  85%|████████▍ | 88/104 [00:20<00:03,  4.33it/s, loss=0.161, v_num=0, reduced_train_loss=0.00395, global_step=599.0, val_loss=0.0603]\n",
      "Epoch 7:  68%|██████▊   | 71/104 [00:16<00:07,  4.31it/s, loss=0.254, v_num=0, reduced_train_loss=0.0785, global_step=595.0, val_loss=0.205]5]\n",
      "Epoch 7:  86%|████████▌ | 89/104 [00:20<00:03,  4.36it/s, loss=0.161, v_num=0, reduced_train_loss=0.00395, global_step=599.0, val_loss=0.0603]\n",
      "Epoch 7:  93%|█████████▎| 97/104 [00:19<00:01,  4.97it/s, loss=0.0647, v_num=0, reduced_train_loss=0.0173, global_step=599.0, val_loss=0.0705]\n",
      "Epoch 7:  87%|████████▋ | 90/104 [00:20<00:03,  4.38it/s, loss=0.161, v_num=0, reduced_train_loss=0.00395, global_step=599.0, val_loss=0.0603]\n",
      "Epoch 7:  69%|██████▉   | 72/104 [00:16<00:07,  4.31it/s, loss=0.233, v_num=0, reduced_train_loss=0.0605, global_step=596.0, val_loss=0.205]5]\n",
      "Epoch 7:  88%|████████▊ | 91/104 [00:20<00:02,  4.40it/s, loss=0.161, v_num=0, reduced_train_loss=0.00395, global_step=599.0, val_loss=0.0603]\n",
      "Epoch 7:  95%|█████████▌| 99/104 [00:19<00:00,  5.02it/s, loss=0.0647, v_num=0, reduced_train_loss=0.0173, global_step=599.0, val_loss=0.0705]\n",
      "Epoch 7:  88%|████████▊ | 92/104 [00:20<00:02,  4.43it/s, loss=0.161, v_num=0, reduced_train_loss=0.00395, global_step=599.0, val_loss=0.0603]\n",
      "Epoch 7:  70%|███████   | 73/104 [00:16<00:07,  4.31it/s, loss=0.221, v_num=0, reduced_train_loss=0.149, global_step=597.0, val_loss=0.205] 05]\n",
      "Epoch 7:  89%|████████▉ | 93/104 [00:20<00:02,  4.45it/s, loss=0.161, v_num=0, reduced_train_loss=0.00395, global_step=599.0, val_loss=0.0603]\n",
      "Epoch 7:  97%|█████████▋| 101/104 [00:19<00:00,  5.06it/s, loss=0.0647, v_num=0, reduced_train_loss=0.0173, global_step=599.0, val_loss=0.0705]\n",
      "Epoch 7:  90%|█████████ | 94/104 [00:21<00:02,  4.47it/s, loss=0.161, v_num=0, reduced_train_loss=0.00395, global_step=599.0, val_loss=0.0603]\n",
      "Epoch 7:  71%|███████   | 74/104 [00:17<00:06,  4.31it/s, loss=0.205, v_num=0, reduced_train_loss=0.0104, global_step=598.0, val_loss=0.205]05]\n",
      "Epoch 7:  91%|█████████▏| 95/104 [00:21<00:02,  4.49it/s, loss=0.161, v_num=0, reduced_train_loss=0.00395, global_step=599.0, val_loss=0.0603]\n",
      "Epoch 7:  99%|█████████▉| 103/104 [00:20<00:00,  5.10it/s, loss=0.0647, v_num=0, reduced_train_loss=0.0173, global_step=599.0, val_loss=0.0705]\n",
      "Epoch 7: 100%|██████████| 104/104 [00:20<00:00,  5.13it/s, loss=0.0647, v_num=0, reduced_train_loss=0.0173, global_step=599.0, val_loss=0.0705]2023-05-03 21:58:49,720 - root - INFO - val_loss: 0.061218589544296265\n",
      "Epoch 7: 100%|██████████| 104/104 [00:20<00:00,  5.13it/s, loss=0.0647, v_num=0, reduced_train_loss=0.0173, global_step=599.0, val_loss=0.0612]\n",
      "Epoch 8:   0%|          | 0/104 [00:00<?, ?it/s, loss=0.0647, v_num=0, reduced_train_loss=0.0173, global_step=599.0, val_loss=0.0612]          \n",
      "Epoch 7:  92%|█████████▏| 96/104 [00:21<00:01,  4.52it/s, loss=0.161, v_num=0, reduced_train_loss=0.00395, global_step=599.0, val_loss=0.0603]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 21:58:49.720328 140649257482048 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.061218589544296265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7:  72%|███████▏  | 75/104 [00:17<00:06,  4.31it/s, loss=0.188, v_num=0, reduced_train_loss=0.148, global_step=599.0, val_loss=0.205] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7:  93%|█████████▎| 97/104 [00:21<00:01,  4.54it/s, loss=0.161, v_num=0, reduced_train_loss=0.00395, global_step=599.0, val_loss=0.0603]\n",
      "Epoch 8:   1%|          | 1/104 [00:00<00:27,  3.80it/s, loss=0.0688, v_num=0, reduced_train_loss=0.116, global_step=600.0, val_loss=0.0612] ]\n",
      "Epoch 7:  73%|███████▎  | 76/104 [00:17<00:06,  4.33it/s, loss=0.188, v_num=0, reduced_train_loss=0.148, global_step=599.0, val_loss=0.205]\n",
      "Epoch 7:  95%|█████████▌| 99/104 [00:21<00:01,  4.58it/s, loss=0.161, v_num=0, reduced_train_loss=0.00395, global_step=599.0, val_loss=0.0603]\n",
      "Epoch 8:   2%|▏         | 2/104 [00:00<00:24,  4.14it/s, loss=0.0693, v_num=0, reduced_train_loss=0.0354, global_step=601.0, val_loss=0.0612]\n",
      "Epoch 7:  96%|█████████▌| 100/104 [00:21<00:00,  4.60it/s, loss=0.161, v_num=0, reduced_train_loss=0.00395, global_step=599.0, val_loss=0.0603]\n",
      "Epoch 7:  75%|███████▌  | 78/104 [00:17<00:05,  4.38it/s, loss=0.188, v_num=0, reduced_train_loss=0.148, global_step=599.0, val_loss=0.205]\n",
      "Epoch 7:  97%|█████████▋| 101/104 [00:21<00:00,  4.62it/s, loss=0.161, v_num=0, reduced_train_loss=0.00395, global_step=599.0, val_loss=0.0603]\n",
      "Epoch 8:   3%|▎         | 3/104 [00:00<00:23,  4.25it/s, loss=0.079, v_num=0, reduced_train_loss=0.222, global_step=602.0, val_loss=0.0612]  \n",
      "Epoch 7:  98%|█████████▊| 102/104 [00:21<00:00,  4.64it/s, loss=0.161, v_num=0, reduced_train_loss=0.00395, global_step=599.0, val_loss=0.0603]\n",
      "Epoch 7:  77%|███████▋  | 80/104 [00:18<00:05,  4.43it/s, loss=0.188, v_num=0, reduced_train_loss=0.148, global_step=599.0, val_loss=0.205]\n",
      "Epoch 7:  99%|█████████▉| 103/104 [00:22<00:00,  4.66it/s, loss=0.161, v_num=0, reduced_train_loss=0.00395, global_step=599.0, val_loss=0.0603]\n",
      "Epoch 7:  78%|███████▊  | 81/104 [00:18<00:05,  4.45it/s, loss=0.188, v_num=0, reduced_train_loss=0.148, global_step=599.0, val_loss=0.205]\n",
      "Epoch 7: 100%|██████████| 104/104 [00:22<00:00,  4.70it/s, loss=0.161, v_num=0, reduced_train_loss=0.00395, global_step=599.0, val_loss=0.0603]2023-05-03 21:58:50,643 - root - INFO - val_loss: 0.12819087505340576\n",
      "Epoch 7: 100%|██████████| 104/104 [00:22<00:00,  4.70it/s, loss=0.161, v_num=0, reduced_train_loss=0.00395, global_step=599.0, val_loss=0.128] \n",
      "Epoch 8:   4%|▍         | 4/104 [00:00<00:23,  4.30it/s, loss=0.0739, v_num=0, reduced_train_loss=0.0494, global_step=603.0, val_loss=0.0612] \n",
      "Epoch 7:  79%|███████▉  | 82/104 [00:18<00:04,  4.47it/s, loss=0.188, v_num=0, reduced_train_loss=0.148, global_step=599.0, val_loss=0.205]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 21:58:50.643182 140522389899072 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.12819087505340576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8:   1%|          | 1/104 [00:00<00:27,  3.78it/s, loss=0.161, v_num=0, reduced_train_loss=0.0204, global_step=600.0, val_loss=0.128]  \n",
      "Epoch 8:   6%|▌         | 6/104 [00:01<00:22,  4.38it/s, loss=0.0695, v_num=0, reduced_train_loss=0.0274, global_step=605.0, val_loss=0.0612]\n",
      "Epoch 8:   2%|▏         | 2/104 [00:00<00:24,  4.12it/s, loss=0.161, v_num=0, reduced_train_loss=0.00965, global_step=601.0, val_loss=0.128]\n",
      "Epoch 8:   3%|▎         | 3/104 [00:00<00:23,  4.25it/s, loss=0.147, v_num=0, reduced_train_loss=0.0137, global_step=602.0, val_loss=0.128]  \n",
      "Epoch 7:  84%|████████▎ | 87/104 [00:18<00:03,  4.59it/s, loss=0.188, v_num=0, reduced_train_loss=0.148, global_step=599.0, val_loss=0.205]\n",
      "Epoch 8:   4%|▍         | 4/104 [00:00<00:23,  4.31it/s, loss=0.146, v_num=0, reduced_train_loss=0.0208, global_step=603.0, val_loss=0.128]]\n",
      "Epoch 7:  86%|████████▌ | 89/104 [00:19<00:03,  4.63it/s, loss=0.188, v_num=0, reduced_train_loss=0.148, global_step=599.0, val_loss=0.205]\n",
      "Epoch 8:   5%|▍         | 5/104 [00:01<00:22,  4.35it/s, loss=0.11, v_num=0, reduced_train_loss=0.0322, global_step=604.0, val_loss=0.128] ]\n",
      "Epoch 8:  10%|▉         | 10/104 [00:02<00:21,  4.43it/s, loss=0.0835, v_num=0, reduced_train_loss=0.00284, global_step=609.0, val_loss=0.0612]\n",
      "Epoch 8:   6%|▌         | 6/104 [00:01<00:22,  4.38it/s, loss=0.0946, v_num=0, reduced_train_loss=0.0156, global_step=605.0, val_loss=0.128]\n",
      "Epoch 8:   7%|▋         | 7/104 [00:01<00:22,  4.40it/s, loss=0.0987, v_num=0, reduced_train_loss=0.0847, global_step=606.0, val_loss=0.128]   \n",
      "Epoch 7:  90%|█████████ | 94/104 [00:19<00:02,  4.74it/s, loss=0.188, v_num=0, reduced_train_loss=0.148, global_step=599.0, val_loss=0.205]\n",
      "Epoch 8:   8%|▊         | 8/104 [00:01<00:21,  4.42it/s, loss=0.0949, v_num=0, reduced_train_loss=0.0156, global_step=607.0, val_loss=0.128]\n",
      "Epoch 7:  92%|█████████▏| 96/104 [00:20<00:01,  4.78it/s, loss=0.188, v_num=0, reduced_train_loss=0.148, global_step=599.0, val_loss=0.205]\n",
      "Epoch 8:   9%|▊         | 9/104 [00:02<00:21,  4.42it/s, loss=0.102, v_num=0, reduced_train_loss=0.165, global_step=608.0, val_loss=0.128]  ]\n",
      "Epoch 8:  13%|█▎        | 14/104 [00:03<00:20,  4.45it/s, loss=0.111, v_num=0, reduced_train_loss=0.0665, global_step=613.0, val_loss=0.0612]\n",
      "Epoch 8:  10%|▉         | 10/104 [00:02<00:21,  4.43it/s, loss=0.0829, v_num=0, reduced_train_loss=0.132, global_step=609.0, val_loss=0.128]\n",
      "Epoch 8:  14%|█▍        | 15/104 [00:03<00:19,  4.46it/s, loss=0.11, v_num=0, reduced_train_loss=0.0181, global_step=614.0, val_loss=0.0612] \n",
      "Epoch 8:  11%|█         | 11/104 [00:02<00:20,  4.44it/s, loss=0.0729, v_num=0, reduced_train_loss=0.0259, global_step=610.0, val_loss=0.128]\n",
      "Epoch 8:  12%|█▏        | 12/104 [00:02<00:20,  4.45it/s, loss=0.0778, v_num=0, reduced_train_loss=0.136, global_step=611.0, val_loss=0.128] \n",
      "Epoch 7:  99%|█████████▉| 103/104 [00:20<00:00,  4.92it/s, loss=0.188, v_num=0, reduced_train_loss=0.148, global_step=599.0, val_loss=0.205]\n",
      "Epoch 7: 100%|██████████| 104/104 [00:21<00:00,  4.95it/s, loss=0.188, v_num=0, reduced_train_loss=0.148, global_step=599.0, val_loss=0.205]2023-05-03 21:58:53,434 - root - INFO - val_loss: 0.13637514412403107\n",
      "Epoch 7: 100%|██████████| 104/104 [00:21<00:00,  4.95it/s, loss=0.188, v_num=0, reduced_train_loss=0.148, global_step=599.0, val_loss=0.136]\n",
      "Epoch 8:   0%|          | 0/104 [00:00<?, ?it/s, loss=0.188, v_num=0, reduced_train_loss=0.148, global_step=599.0, val_loss=0.136]          "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 21:58:53.434782 139755592410944 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.13637514412403107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8:  72%|███████▏  | 75/104 [00:16<00:06,  4.48it/s, loss=0.037, v_num=0, reduced_train_loss=0.00306, global_step=674.0, val_loss=0.0612]  \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8:  69%|██████▉   | 72/104 [00:15<00:07,  4.50it/s, loss=0.145, v_num=0, reduced_train_loss=0.547, global_step=671.0, val_loss=0.128] \n",
      "Epoch 8:  56%|█████▌    | 58/104 [00:13<00:10,  4.39it/s, loss=0.244, v_num=0, reduced_train_loss=0.433, global_step=657.0, val_loss=0.136]12]\n",
      "Epoch 8:  57%|█████▋    | 59/104 [00:13<00:10,  4.39it/s, loss=0.238, v_num=0, reduced_train_loss=0.0277, global_step=658.0, val_loss=0.136]2]\n",
      "Epoch 8:  75%|███████▌  | 78/104 [00:17<00:05,  4.54it/s, loss=0.037, v_num=0, reduced_train_loss=0.00306, global_step=674.0, val_loss=0.0612]\n",
      "Epoch 8:  58%|█████▊    | 60/104 [00:13<00:10,  4.39it/s, loss=0.262, v_num=0, reduced_train_loss=0.730, global_step=659.0, val_loss=0.136] 2]\n",
      "Epoch 8:  77%|███████▋  | 80/104 [00:17<00:05,  4.59it/s, loss=0.037, v_num=0, reduced_train_loss=0.00306, global_step=674.0, val_loss=0.0612]\n",
      "Epoch 8:  72%|███████▏  | 75/104 [00:16<00:06,  4.51it/s, loss=0.134, v_num=0, reduced_train_loss=0.0122, global_step=674.0, val_loss=0.128]2]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8:  59%|█████▊    | 61/104 [00:13<00:09,  4.40it/s, loss=0.276, v_num=0, reduced_train_loss=0.482, global_step=660.0, val_loss=0.136]\n",
      "Epoch 8:  79%|███████▉  | 82/104 [00:17<00:04,  4.64it/s, loss=0.037, v_num=0, reduced_train_loss=0.00306, global_step=674.0, val_loss=0.0612]\n",
      "Epoch 8:  73%|███████▎  | 76/104 [00:16<00:06,  4.52it/s, loss=0.134, v_num=0, reduced_train_loss=0.0122, global_step=674.0, val_loss=0.128]\n",
      "Epoch 8:  60%|█████▉    | 62/104 [00:14<00:09,  4.40it/s, loss=0.267, v_num=0, reduced_train_loss=0.189, global_step=661.0, val_loss=0.136]12]\n",
      "Epoch 8:  74%|███████▍  | 77/104 [00:16<00:05,  4.55it/s, loss=0.134, v_num=0, reduced_train_loss=0.0122, global_step=674.0, val_loss=0.128]\n",
      "Epoch 8:  81%|████████  | 84/104 [00:17<00:04,  4.69it/s, loss=0.037, v_num=0, reduced_train_loss=0.00306, global_step=674.0, val_loss=0.0612]\n",
      "Epoch 8:  75%|███████▌  | 78/104 [00:17<00:05,  4.57it/s, loss=0.134, v_num=0, reduced_train_loss=0.0122, global_step=674.0, val_loss=0.128]\n",
      "Epoch 8:  61%|██████    | 63/104 [00:14<00:09,  4.40it/s, loss=0.253, v_num=0, reduced_train_loss=0.305, global_step=662.0, val_loss=0.136]12]\n",
      "Epoch 8:  76%|███████▌  | 79/104 [00:17<00:05,  4.60it/s, loss=0.134, v_num=0, reduced_train_loss=0.0122, global_step=674.0, val_loss=0.128]\n",
      "Epoch 8:  83%|████████▎ | 86/104 [00:18<00:03,  4.74it/s, loss=0.037, v_num=0, reduced_train_loss=0.00306, global_step=674.0, val_loss=0.0612]\n",
      "Epoch 8:  62%|██████▏   | 64/104 [00:14<00:09,  4.40it/s, loss=0.253, v_num=0, reduced_train_loss=0.364, global_step=663.0, val_loss=0.136]]\n",
      "Epoch 8:  84%|████████▎ | 87/104 [00:18<00:03,  4.76it/s, loss=0.037, v_num=0, reduced_train_loss=0.00306, global_step=674.0, val_loss=0.0612]\n",
      "Epoch 8:  78%|███████▊  | 81/104 [00:17<00:04,  4.65it/s, loss=0.134, v_num=0, reduced_train_loss=0.0122, global_step=674.0, val_loss=0.128]\n",
      "Epoch 8:  85%|████████▍ | 88/104 [00:18<00:03,  4.78it/s, loss=0.037, v_num=0, reduced_train_loss=0.00306, global_step=674.0, val_loss=0.0612]\n",
      "Epoch 8:  62%|██████▎   | 65/104 [00:14<00:08,  4.41it/s, loss=0.259, v_num=0, reduced_train_loss=0.330, global_step=664.0, val_loss=0.136]]\n",
      "Epoch 8:  86%|████████▌ | 89/104 [00:18<00:03,  4.81it/s, loss=0.037, v_num=0, reduced_train_loss=0.00306, global_step=674.0, val_loss=0.0612]\n",
      "Epoch 8:  80%|███████▉  | 83/104 [00:17<00:04,  4.70it/s, loss=0.134, v_num=0, reduced_train_loss=0.0122, global_step=674.0, val_loss=0.128]\n",
      "Epoch 8:  63%|██████▎   | 66/104 [00:14<00:08,  4.41it/s, loss=0.274, v_num=0, reduced_train_loss=0.345, global_step=665.0, val_loss=0.136]12]\n",
      "Epoch 8:  81%|████████  | 84/104 [00:17<00:04,  4.72it/s, loss=0.134, v_num=0, reduced_train_loss=0.0122, global_step=674.0, val_loss=0.128]\n",
      "Epoch 8:  88%|████████▊ | 91/104 [00:18<00:02,  4.85it/s, loss=0.037, v_num=0, reduced_train_loss=0.00306, global_step=674.0, val_loss=0.0612]\n",
      "Epoch 8:  82%|████████▏ | 85/104 [00:17<00:04,  4.75it/s, loss=0.134, v_num=0, reduced_train_loss=0.0122, global_step=674.0, val_loss=0.128]\n",
      "Epoch 8:  64%|██████▍   | 67/104 [00:15<00:08,  4.41it/s, loss=0.27, v_num=0, reduced_train_loss=0.0981, global_step=666.0, val_loss=0.136]12]\n",
      "Epoch 8:  83%|████████▎ | 86/104 [00:18<00:03,  4.77it/s, loss=0.134, v_num=0, reduced_train_loss=0.0122, global_step=674.0, val_loss=0.128]\n",
      "Epoch 8:  89%|████████▉ | 93/104 [00:18<00:02,  4.90it/s, loss=0.037, v_num=0, reduced_train_loss=0.00306, global_step=674.0, val_loss=0.0612]\n",
      "Epoch 8:  84%|████████▎ | 87/104 [00:18<00:03,  4.79it/s, loss=0.134, v_num=0, reduced_train_loss=0.0122, global_step=674.0, val_loss=0.128]\n",
      "Epoch 8:  65%|██████▌   | 68/104 [00:15<00:08,  4.41it/s, loss=0.268, v_num=0, reduced_train_loss=0.0403, global_step=667.0, val_loss=0.136]2]\n",
      "Epoch 8:  85%|████████▍ | 88/104 [00:18<00:03,  4.81it/s, loss=0.134, v_num=0, reduced_train_loss=0.0122, global_step=674.0, val_loss=0.128]\n",
      "Epoch 8:  91%|█████████▏| 95/104 [00:19<00:01,  4.94it/s, loss=0.037, v_num=0, reduced_train_loss=0.00306, global_step=674.0, val_loss=0.0612]\n",
      "Epoch 8:  86%|████████▌ | 89/104 [00:18<00:03,  4.84it/s, loss=0.134, v_num=0, reduced_train_loss=0.0122, global_step=674.0, val_loss=0.128]\n",
      "Epoch 8:  66%|██████▋   | 69/104 [00:15<00:07,  4.41it/s, loss=0.276, v_num=0, reduced_train_loss=0.263, global_step=668.0, val_loss=0.136] 2]\n",
      "Epoch 8:  87%|████████▋ | 90/104 [00:18<00:02,  4.86it/s, loss=0.134, v_num=0, reduced_train_loss=0.0122, global_step=674.0, val_loss=0.128]\n",
      "Epoch 8:  93%|█████████▎| 97/104 [00:19<00:01,  4.98it/s, loss=0.037, v_num=0, reduced_train_loss=0.00306, global_step=674.0, val_loss=0.0612]\n",
      "Epoch 8:  67%|██████▋   | 70/104 [00:15<00:07,  4.41it/s, loss=0.271, v_num=0, reduced_train_loss=0.108, global_step=669.0, val_loss=0.136]]\n",
      "Epoch 8:  94%|█████████▍| 98/104 [00:19<00:01,  5.00it/s, loss=0.037, v_num=0, reduced_train_loss=0.00306, global_step=674.0, val_loss=0.0612]\n",
      "Epoch 8:  88%|████████▊ | 92/104 [00:18<00:02,  4.90it/s, loss=0.134, v_num=0, reduced_train_loss=0.0122, global_step=674.0, val_loss=0.128]\n",
      "Epoch 8:  95%|█████████▌| 99/104 [00:19<00:00,  5.02it/s, loss=0.037, v_num=0, reduced_train_loss=0.00306, global_step=674.0, val_loss=0.0612]\n",
      "Epoch 8:  68%|██████▊   | 71/104 [00:16<00:07,  4.41it/s, loss=0.287, v_num=0, reduced_train_loss=0.404, global_step=670.0, val_loss=0.136]]\n",
      "Epoch 8:  96%|█████████▌| 100/104 [00:19<00:00,  5.04it/s, loss=0.037, v_num=0, reduced_train_loss=0.00306, global_step=674.0, val_loss=0.0612]\n",
      "Epoch 8:  90%|█████████ | 94/104 [00:19<00:02,  4.95it/s, loss=0.134, v_num=0, reduced_train_loss=0.0122, global_step=674.0, val_loss=0.128]\n",
      "Epoch 8:  69%|██████▉   | 72/104 [00:16<00:07,  4.41it/s, loss=0.265, v_num=0, reduced_train_loss=0.150, global_step=671.0, val_loss=0.136]612]\n",
      "Epoch 8:  91%|█████████▏| 95/104 [00:19<00:01,  4.97it/s, loss=0.134, v_num=0, reduced_train_loss=0.0122, global_step=674.0, val_loss=0.128]\n",
      "Epoch 8:  98%|█████████▊| 102/104 [00:20<00:00,  5.08it/s, loss=0.037, v_num=0, reduced_train_loss=0.00306, global_step=674.0, val_loss=0.0612]\n",
      "Epoch 8:  92%|█████████▏| 96/104 [00:19<00:01,  4.99it/s, loss=0.134, v_num=0, reduced_train_loss=0.0122, global_step=674.0, val_loss=0.128]\n",
      "Epoch 8:  99%|█████████▉| 103/104 [00:20<00:00,  5.10it/s, loss=0.037, v_num=0, reduced_train_loss=0.00306, global_step=674.0, val_loss=0.0612]\n",
      "Epoch 8:  70%|███████   | 73/104 [00:16<00:07,  4.41it/s, loss=0.263, v_num=0, reduced_train_loss=0.115, global_step=672.0, val_loss=0.136]2023-05-03 21:59:09,981 - root - INFO - val_loss: 0.055116742849349976\n",
      "Epoch 8: 100%|██████████| 104/104 [00:20<00:00,  5.13it/s, loss=0.037, v_num=0, reduced_train_loss=0.00306, global_step=674.0, val_loss=0.0551]\n",
      "Epoch 9:   0%|          | 0/104 [00:00<?, ?it/s, loss=0.037, v_num=0, reduced_train_loss=0.00306, global_step=674.0, val_loss=0.0551]          \n",
      "Epoch 8:  93%|█████████▎| 97/104 [00:19<00:01,  5.01it/s, loss=0.134, v_num=0, reduced_train_loss=0.0122, global_step=674.0, val_loss=0.128]\n",
      "Epoch 8:  94%|█████████▍| 98/104 [00:19<00:01,  5.03it/s, loss=0.134, v_num=0, reduced_train_loss=0.0122, global_step=674.0, val_loss=0.128]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 21:59:09.981894 140649257482048 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.055116742849349976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9:   1%|          | 1/104 [00:00<00:27,  3.79it/s, loss=0.0399, v_num=0, reduced_train_loss=0.0692, global_step=675.0, val_loss=0.0551]\n",
      "Epoch 8:  95%|█████████▌| 99/104 [00:19<00:00,  5.05it/s, loss=0.134, v_num=0, reduced_train_loss=0.0122, global_step=674.0, val_loss=0.128]\n",
      "Epoch 8:  72%|███████▏  | 75/104 [00:16<00:06,  4.42it/s, loss=0.247, v_num=0, reduced_train_loss=0.0251, global_step=674.0, val_loss=0.136]]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9:   2%|▏         | 2/104 [00:00<00:25,  4.01it/s, loss=0.0349, v_num=0, reduced_train_loss=0.0259, global_step=676.0, val_loss=0.0551]\n",
      "Epoch 8:  97%|█████████▋| 101/104 [00:19<00:00,  5.09it/s, loss=0.134, v_num=0, reduced_train_loss=0.0122, global_step=674.0, val_loss=0.128]\n",
      "Epoch 8:  73%|███████▎  | 76/104 [00:17<00:06,  4.43it/s, loss=0.247, v_num=0, reduced_train_loss=0.0251, global_step=674.0, val_loss=0.136]\n",
      "Epoch 9:   3%|▎         | 3/104 [00:00<00:24,  4.15it/s, loss=0.0356, v_num=0, reduced_train_loss=0.0176, global_step=677.0, val_loss=0.0551]\n",
      "Epoch 8:  74%|███████▍  | 77/104 [00:17<00:06,  4.46it/s, loss=0.247, v_num=0, reduced_train_loss=0.0251, global_step=674.0, val_loss=0.136]\n",
      "Epoch 8:  99%|█████████▉| 103/104 [00:20<00:00,  5.13it/s, loss=0.134, v_num=0, reduced_train_loss=0.0122, global_step=674.0, val_loss=0.128]\n",
      "Epoch 8: 100%|██████████| 104/104 [00:20<00:00,  5.16it/s, loss=0.134, v_num=0, reduced_train_loss=0.0122, global_step=674.0, val_loss=0.128]2023-05-03 21:59:10,802 - root - INFO - val_loss: 0.0649709701538086\n",
      "Epoch 8: 100%|██████████| 104/104 [00:20<00:00,  5.16it/s, loss=0.134, v_num=0, reduced_train_loss=0.0122, global_step=674.0, val_loss=0.065]\n",
      "Epoch 9:   0%|          | 0/104 [00:00<?, ?it/s, loss=0.134, v_num=0, reduced_train_loss=0.0122, global_step=674.0, val_loss=0.065]          "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 21:59:10.802353 140522389899072 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.0649709701538086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9:   4%|▍         | 4/104 [00:00<00:23,  4.23it/s, loss=0.0479, v_num=0, reduced_train_loss=0.252, global_step=678.0, val_loss=0.0551] \n",
      "Epoch 9:   1%|          | 1/104 [00:00<00:27,  3.76it/s, loss=0.133, v_num=0, reduced_train_loss=0.00259, global_step=675.0, val_loss=0.065]\n",
      "Epoch 9:   5%|▍         | 5/104 [00:01<00:23,  4.27it/s, loss=0.0473, v_num=0, reduced_train_loss=0.00105, global_step=679.0, val_loss=0.0551]\n",
      "Epoch 9:   2%|▏         | 2/104 [00:00<00:24,  4.13it/s, loss=0.128, v_num=0, reduced_train_loss=0.179, global_step=676.0, val_loss=0.065]  \n",
      "Epoch 9:   6%|▌         | 6/104 [00:01<00:22,  4.30it/s, loss=0.0469, v_num=0, reduced_train_loss=0.00424, global_step=680.0, val_loss=0.0551]\n",
      "Epoch 9:   3%|▎         | 3/104 [00:00<00:23,  4.26it/s, loss=0.129, v_num=0, reduced_train_loss=0.0438, global_step=677.0, val_loss=0.065]]\n",
      "Epoch 9:   7%|▋         | 7/104 [00:01<00:22,  4.32it/s, loss=0.0419, v_num=0, reduced_train_loss=0.00393, global_step=681.0, val_loss=0.0551]\n",
      "Epoch 9:   4%|▍         | 4/104 [00:00<00:23,  4.31it/s, loss=0.126, v_num=0, reduced_train_loss=0.00446, global_step=678.0, val_loss=0.065]\n",
      "Epoch 9:   8%|▊         | 8/104 [00:01<00:22,  4.34it/s, loss=0.0409, v_num=0, reduced_train_loss=0.0122, global_step=682.0, val_loss=0.0551] \n",
      "Epoch 9:   9%|▊         | 9/104 [00:02<00:21,  4.35it/s, loss=0.04, v_num=0, reduced_train_loss=0.0294, global_step=683.0, val_loss=0.0551]  \n",
      "Epoch 9:   6%|▌         | 6/104 [00:01<00:22,  4.37it/s, loss=0.108, v_num=0, reduced_train_loss=0.0728, global_step=680.0, val_loss=0.065]]\n",
      "Epoch 9:  10%|▉         | 10/104 [00:02<00:21,  4.37it/s, loss=0.0392, v_num=0, reduced_train_loss=0.00401, global_step=684.0, val_loss=0.0551]\n",
      "Epoch 9:   7%|▋         | 7/104 [00:01<00:22,  4.40it/s, loss=0.101, v_num=0, reduced_train_loss=0.148, global_step=681.0, val_loss=0.065] ]\n",
      "Epoch 9:  11%|█         | 11/104 [00:02<00:21,  4.37it/s, loss=0.0401, v_num=0, reduced_train_loss=0.0335, global_step=685.0, val_loss=0.0551] \n",
      "Epoch 9:   8%|▊         | 8/104 [00:01<00:21,  4.40it/s, loss=0.104, v_num=0, reduced_train_loss=0.305, global_step=682.0, val_loss=0.065]6]\n",
      "Epoch 9:  12%|█▏        | 12/104 [00:02<00:21,  4.37it/s, loss=0.0657, v_num=0, reduced_train_loss=0.515, global_step=686.0, val_loss=0.0551] \n",
      "Epoch 9:   9%|▊         | 9/104 [00:02<00:21,  4.41it/s, loss=0.104, v_num=0, reduced_train_loss=0.0133, global_step=683.0, val_loss=0.065]]\n",
      "Epoch 9:  12%|█▎        | 13/104 [00:02<00:20,  4.38it/s, loss=0.0704, v_num=0, reduced_train_loss=0.0969, global_step=687.0, val_loss=0.0551]\n",
      "Epoch 9:  10%|▉         | 10/104 [00:02<00:21,  4.42it/s, loss=0.103, v_num=0, reduced_train_loss=0.014, global_step=684.0, val_loss=0.065] \n",
      "Epoch 9:  13%|█▎        | 14/104 [00:03<00:20,  4.38it/s, loss=0.0602, v_num=0, reduced_train_loss=0.00943, global_step=688.0, val_loss=0.0551]\n",
      "Epoch 9:  11%|█         | 11/104 [00:02<00:20,  4.43it/s, loss=0.114, v_num=0, reduced_train_loss=0.224, global_step=685.0, val_loss=0.065]]\n",
      "Epoch 9:  12%|█▏        | 12/104 [00:02<00:20,  4.44it/s, loss=0.117, v_num=0, reduced_train_loss=0.0828, global_step=686.0, val_loss=0.065]]  \n",
      "Epoch 9:  15%|█▌        | 16/104 [00:03<00:20,  4.39it/s, loss=0.0797, v_num=0, reduced_train_loss=0.0205, global_step=690.0, val_loss=0.0551]\n",
      "Epoch 9:  12%|█▎        | 13/104 [00:02<00:20,  4.45it/s, loss=0.117, v_num=0, reduced_train_loss=0.0139, global_step=687.0, val_loss=0.065]]\n",
      "Epoch 9:  16%|█▋        | 17/104 [00:03<00:19,  4.39it/s, loss=0.0784, v_num=0, reduced_train_loss=0.0053, global_step=691.0, val_loss=0.0551]\n",
      "Epoch 8:  99%|█████████▉| 103/104 [00:20<00:00,  5.04it/s, loss=0.247, v_num=0, reduced_train_loss=0.0251, global_step=674.0, val_loss=0.136]\n",
      "Epoch 8: 100%|██████████| 104/104 [00:20<00:00,  5.07it/s, loss=0.247, v_num=0, reduced_train_loss=0.0251, global_step=674.0, val_loss=0.136]2023-05-03 21:59:13,943 - root - INFO - val_loss: 0.14293226599693298\n",
      "Epoch 8: 100%|██████████| 104/104 [00:20<00:00,  5.07it/s, loss=0.247, v_num=0, reduced_train_loss=0.0251, global_step=674.0, val_loss=0.143]\n",
      "Epoch 9:  17%|█▋        | 18/104 [00:04<00:19,  4.40it/s, loss=0.0775, v_num=0, reduced_train_loss=0.0341, global_step=692.0, val_loss=0.0551]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 21:59:13.943663 139755592410944 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.14293226599693298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9:  72%|███████▏  | 75/104 [00:16<00:06,  4.48it/s, loss=0.0623, v_num=0, reduced_train_loss=0.0465, global_step=749.0, val_loss=0.0551] \n",
      "Epoch 9:  54%|█████▍    | 56/104 [00:12<00:10,  4.38it/s, loss=0.181, v_num=0, reduced_train_loss=0.167, global_step=730.0, val_loss=0.143]\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9:  69%|██████▉   | 72/104 [00:16<00:07,  4.49it/s, loss=0.0987, v_num=0, reduced_train_loss=0.00391, global_step=746.0, val_loss=0.065]\n",
      "Epoch 9:  55%|█████▍    | 57/104 [00:13<00:10,  4.38it/s, loss=0.182, v_num=0, reduced_train_loss=0.112, global_step=731.0, val_loss=0.143]51]\n",
      "Epoch 9:  70%|███████   | 73/104 [00:16<00:06,  4.49it/s, loss=0.0986, v_num=0, reduced_train_loss=0.0125, global_step=747.0, val_loss=0.065] \n",
      "Epoch 9:  56%|█████▌    | 58/104 [00:13<00:10,  4.38it/s, loss=0.181, v_num=0, reduced_train_loss=0.0348, global_step=732.0, val_loss=0.143]1]\n",
      "Epoch 9:  71%|███████   | 74/104 [00:16<00:06,  4.49it/s, loss=0.129, v_num=0, reduced_train_loss=0.613, global_step=748.0, val_loss=0.065]  ]\n",
      "Epoch 9:  57%|█████▋    | 59/104 [00:13<00:10,  4.38it/s, loss=0.193, v_num=0, reduced_train_loss=0.249, global_step=733.0, val_loss=0.143] 1]\n",
      "Epoch 9:  72%|███████▏  | 75/104 [00:16<00:06,  4.49it/s, loss=0.165, v_num=0, reduced_train_loss=0.781, global_step=749.0, val_loss=0.065]51]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9:  58%|█████▊    | 60/104 [00:13<00:10,  4.38it/s, loss=0.194, v_num=0, reduced_train_loss=0.285, global_step=734.0, val_loss=0.143]51]\n",
      "Epoch 9:  73%|███████▎  | 76/104 [00:16<00:06,  4.50it/s, loss=0.165, v_num=0, reduced_train_loss=0.781, global_step=749.0, val_loss=0.065]\n",
      "Epoch 9:  80%|███████▉  | 83/104 [00:17<00:04,  4.68it/s, loss=0.0623, v_num=0, reduced_train_loss=0.0465, global_step=749.0, val_loss=0.0551]\n",
      "Epoch 9:  74%|███████▍  | 77/104 [00:17<00:05,  4.53it/s, loss=0.165, v_num=0, reduced_train_loss=0.781, global_step=749.0, val_loss=0.065]\n",
      "Epoch 9:  59%|█████▊    | 61/104 [00:13<00:09,  4.38it/s, loss=0.208, v_num=0, reduced_train_loss=0.493, global_step=735.0, val_loss=0.143]51]\n",
      "Epoch 9:  75%|███████▌  | 78/104 [00:17<00:05,  4.55it/s, loss=0.165, v_num=0, reduced_train_loss=0.781, global_step=749.0, val_loss=0.065]\n",
      "Epoch 9:  82%|████████▏ | 85/104 [00:17<00:04,  4.72it/s, loss=0.0623, v_num=0, reduced_train_loss=0.0465, global_step=749.0, val_loss=0.0551]\n",
      "Epoch 9:  76%|███████▌  | 79/104 [00:17<00:05,  4.58it/s, loss=0.165, v_num=0, reduced_train_loss=0.781, global_step=749.0, val_loss=0.065]\n",
      "Epoch 9:  60%|█████▉    | 62/104 [00:14<00:09,  4.38it/s, loss=0.206, v_num=0, reduced_train_loss=0.301, global_step=736.0, val_loss=0.143]51]\n",
      "Epoch 9:  77%|███████▋  | 80/104 [00:17<00:05,  4.60it/s, loss=0.165, v_num=0, reduced_train_loss=0.781, global_step=749.0, val_loss=0.065]\n",
      "Epoch 9:  84%|████████▎ | 87/104 [00:18<00:03,  4.77it/s, loss=0.0623, v_num=0, reduced_train_loss=0.0465, global_step=749.0, val_loss=0.0551]\n",
      "Epoch 9:  61%|██████    | 63/104 [00:14<00:09,  4.38it/s, loss=0.209, v_num=0, reduced_train_loss=0.249, global_step=737.0, val_loss=0.143]\n",
      "Epoch 9:  85%|████████▍ | 88/104 [00:18<00:03,  4.80it/s, loss=0.0623, v_num=0, reduced_train_loss=0.0465, global_step=749.0, val_loss=0.0551]\n",
      "Epoch 9:  79%|███████▉  | 82/104 [00:17<00:04,  4.65it/s, loss=0.165, v_num=0, reduced_train_loss=0.781, global_step=749.0, val_loss=0.065]\n",
      "Epoch 9:  86%|████████▌ | 89/104 [00:18<00:03,  4.82it/s, loss=0.0623, v_num=0, reduced_train_loss=0.0465, global_step=749.0, val_loss=0.0551]\n",
      "Epoch 9:  62%|██████▏   | 64/104 [00:14<00:09,  4.38it/s, loss=0.226, v_num=0, reduced_train_loss=0.586, global_step=738.0, val_loss=0.143]\n",
      "Epoch 9:  87%|████████▋ | 90/104 [00:18<00:02,  4.84it/s, loss=0.0623, v_num=0, reduced_train_loss=0.0465, global_step=749.0, val_loss=0.0551]\n",
      "Epoch 9:  81%|████████  | 84/104 [00:17<00:04,  4.70it/s, loss=0.165, v_num=0, reduced_train_loss=0.781, global_step=749.0, val_loss=0.065]\n",
      "Epoch 9:  62%|██████▎   | 65/104 [00:14<00:08,  4.38it/s, loss=0.245, v_num=0, reduced_train_loss=0.471, global_step=739.0, val_loss=0.143]51]\n",
      "Epoch 9:  82%|████████▏ | 85/104 [00:17<00:04,  4.73it/s, loss=0.165, v_num=0, reduced_train_loss=0.781, global_step=749.0, val_loss=0.065]\n",
      "Epoch 9:  88%|████████▊ | 92/104 [00:18<00:02,  4.89it/s, loss=0.0623, v_num=0, reduced_train_loss=0.0465, global_step=749.0, val_loss=0.0551]\n",
      "Epoch 9:  83%|████████▎ | 86/104 [00:18<00:03,  4.75it/s, loss=0.165, v_num=0, reduced_train_loss=0.781, global_step=749.0, val_loss=0.065]\n",
      "Epoch 9:  63%|██████▎   | 66/104 [00:15<00:08,  4.38it/s, loss=0.226, v_num=0, reduced_train_loss=0.069, global_step=740.0, val_loss=0.143]51]\n",
      "Epoch 9:  84%|████████▎ | 87/104 [00:18<00:03,  4.77it/s, loss=0.165, v_num=0, reduced_train_loss=0.781, global_step=749.0, val_loss=0.065]\n",
      "Epoch 9:  90%|█████████ | 94/104 [00:19<00:02,  4.93it/s, loss=0.0623, v_num=0, reduced_train_loss=0.0465, global_step=749.0, val_loss=0.0551]\n",
      "Epoch 9:  85%|████████▍ | 88/104 [00:18<00:03,  4.79it/s, loss=0.165, v_num=0, reduced_train_loss=0.781, global_step=749.0, val_loss=0.065]\n",
      "Epoch 9:  64%|██████▍   | 67/104 [00:15<00:08,  4.38it/s, loss=0.233, v_num=0, reduced_train_loss=0.188, global_step=741.0, val_loss=0.143]51]\n",
      "Validation DataLoader 0:  72%|███████▏  | 21/29 [00:02<00:00,  8.25it/s]\u001b[A\n",
      "Epoch 9:  86%|████████▌ | 89/104 [00:18<00:03,  4.82it/s, loss=0.165, v_num=0, reduced_train_loss=0.781, global_step=749.0, val_loss=0.065]51]\n",
      "Epoch 9:  93%|█████████▎| 97/104 [00:19<00:01,  5.00it/s, loss=0.0623, v_num=0, reduced_train_loss=0.0465, global_step=749.0, val_loss=0.0551]\n",
      "Epoch 9:  65%|██████▌   | 68/104 [00:15<00:08,  4.38it/s, loss=0.247, v_num=0, reduced_train_loss=0.304, global_step=742.0, val_loss=0.143]\n",
      "Epoch 9:  94%|█████████▍| 98/104 [00:19<00:01,  5.02it/s, loss=0.0623, v_num=0, reduced_train_loss=0.0465, global_step=749.0, val_loss=0.0551]\n",
      "Epoch 9:  88%|████████▊ | 91/104 [00:18<00:02,  4.86it/s, loss=0.165, v_num=0, reduced_train_loss=0.781, global_step=749.0, val_loss=0.065]\n",
      "Epoch 9:  95%|█████████▌| 99/104 [00:19<00:00,  5.04it/s, loss=0.0623, v_num=0, reduced_train_loss=0.0465, global_step=749.0, val_loss=0.0551]\n",
      "Epoch 9:  66%|██████▋   | 69/104 [00:15<00:07,  4.38it/s, loss=0.234, v_num=0, reduced_train_loss=0.204, global_step=743.0, val_loss=0.143]\n",
      "Epoch 9:  96%|█████████▌| 100/104 [00:19<00:00,  5.06it/s, loss=0.0623, v_num=0, reduced_train_loss=0.0465, global_step=749.0, val_loss=0.0551]\n",
      "Epoch 9:  89%|████████▉ | 93/104 [00:18<00:02,  4.90it/s, loss=0.165, v_num=0, reduced_train_loss=0.781, global_step=749.0, val_loss=0.065]\n",
      "Epoch 9:  97%|█████████▋| 101/104 [00:19<00:00,  5.08it/s, loss=0.0623, v_num=0, reduced_train_loss=0.0465, global_step=749.0, val_loss=0.0551]\n",
      "Epoch 9:  67%|██████▋   | 70/104 [00:15<00:07,  4.38it/s, loss=0.234, v_num=0, reduced_train_loss=0.0683, global_step=744.0, val_loss=0.143]\n",
      "Epoch 9:  98%|█████████▊| 102/104 [00:20<00:00,  5.10it/s, loss=0.0623, v_num=0, reduced_train_loss=0.0465, global_step=749.0, val_loss=0.0551]\n",
      "Epoch 9:  91%|█████████▏| 95/104 [00:19<00:01,  4.95it/s, loss=0.165, v_num=0, reduced_train_loss=0.781, global_step=749.0, val_loss=0.065]\n",
      "Epoch 9:  99%|█████████▉| 103/104 [00:20<00:00,  5.12it/s, loss=0.0623, v_num=0, reduced_train_loss=0.0465, global_step=749.0, val_loss=0.0551]\n",
      "Epoch 9:  68%|██████▊   | 71/104 [00:16<00:07,  4.38it/s, loss=0.235, v_num=0, reduced_train_loss=0.191, global_step=745.0, val_loss=0.143] \n",
      "Epoch 9: 100%|██████████| 104/104 [00:20<00:00,  5.15it/s, loss=0.0623, v_num=0, reduced_train_loss=0.0465, global_step=749.0, val_loss=0.0551]2023-05-03 21:59:30,183 - root - INFO - val_loss: 0.05615731701254845\n",
      "Epoch 9: 100%|██████████| 104/104 [00:20<00:00,  5.15it/s, loss=0.0623, v_num=0, reduced_train_loss=0.0465, global_step=749.0, val_loss=0.0562]\n",
      "Epoch 10:   0%|          | 0/104 [00:00<?, ?it/s, loss=0.0623, v_num=0, reduced_train_loss=0.0465, global_step=749.0, val_loss=0.0562]         \n",
      "Epoch 9:  93%|█████████▎| 97/104 [00:19<00:01,  4.99it/s, loss=0.165, v_num=0, reduced_train_loss=0.781, global_step=749.0, val_loss=0.065]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 21:59:30.183434 140649257482048 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.05615731701254845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10:   1%|          | 1/104 [00:00<00:28,  3.61it/s, loss=0.062, v_num=0, reduced_train_loss=0.000707, global_step=750.0, val_loss=0.0562]\n",
      "Epoch 9:  70%|███████   | 73/104 [00:16<00:07,  4.38it/s, loss=0.228, v_num=0, reduced_train_loss=0.250, global_step=747.0, val_loss=0.143]\n",
      "Epoch 10:   2%|▏         | 2/104 [00:00<00:25,  3.96it/s, loss=0.0653, v_num=0, reduced_train_loss=0.0704, global_step=751.0, val_loss=0.0562] \n",
      "Epoch 9:  71%|███████   | 74/104 [00:16<00:06,  4.38it/s, loss=0.231, v_num=0, reduced_train_loss=0.135, global_step=748.0, val_loss=0.143]]\n",
      "Epoch 10:   3%|▎         | 3/104 [00:00<00:24,  4.11it/s, loss=0.0615, v_num=0, reduced_train_loss=0.0108, global_step=752.0, val_loss=0.0562]\n",
      "Epoch 9:  99%|█████████▉| 103/104 [00:20<00:00,  5.11it/s, loss=0.165, v_num=0, reduced_train_loss=0.781, global_step=749.0, val_loss=0.065]\n",
      "Epoch 9: 100%|██████████| 104/104 [00:20<00:00,  5.14it/s, loss=0.165, v_num=0, reduced_train_loss=0.781, global_step=749.0, val_loss=0.065]2023-05-03 21:59:31,048 - root - INFO - val_loss: 0.14145277440547943\n",
      "Epoch 9: 100%|██████████| 104/104 [00:20<00:00,  5.14it/s, loss=0.165, v_num=0, reduced_train_loss=0.781, global_step=749.0, val_loss=0.141]\n",
      "Epoch 9:  72%|███████▏  | 75/104 [00:17<00:06,  4.38it/s, loss=0.231, v_num=0, reduced_train_loss=0.0496, global_step=749.0, val_loss=0.143]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 10:   4%|▍         | 4/104 [00:00<00:23,  4.19it/s, loss=0.0592, v_num=0, reduced_train_loss=0.00232, global_step=753.0, val_loss=0.0562]\n",
      "Epoch 9:  73%|███████▎  | 76/104 [00:17<00:06,  4.39it/s, loss=0.231, v_num=0, reduced_train_loss=0.0496, global_step=749.0, val_loss=0.143]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 21:59:31.048685 140522389899072 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.14145277440547943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10:   5%|▍         | 5/104 [00:01<00:23,  4.26it/s, loss=0.0765, v_num=0, reduced_train_loss=0.412, global_step=754.0, val_loss=0.0562]  \n",
      "Epoch 10:   8%|▊         | 8/104 [00:01<00:21,  4.37it/s, loss=0.0633, v_num=0, reduced_train_loss=0.0524, global_step=757.0, val_loss=0.0562]  \n",
      "Epoch 10:   9%|▊         | 9/104 [00:02<00:21,  4.39it/s, loss=0.0588, v_num=0, reduced_train_loss=0.027, global_step=758.0, val_loss=0.0562] \n",
      "Epoch 9:  76%|███████▌  | 79/104 [00:18<00:05,  4.32it/s, loss=0.231, v_num=0, reduced_train_loss=0.0496, global_step=749.0, val_loss=0.143]\n",
      "Epoch 10:  10%|▉         | 10/104 [00:02<00:21,  4.41it/s, loss=0.0604, v_num=0, reduced_train_loss=0.0322, global_step=759.0, val_loss=0.0562]\n",
      "Epoch 9:  78%|███████▊  | 81/104 [00:18<00:05,  4.37it/s, loss=0.231, v_num=0, reduced_train_loss=0.0496, global_step=749.0, val_loss=0.143]\n",
      "Epoch 10:  11%|█         | 11/104 [00:02<00:21,  4.41it/s, loss=0.0667, v_num=0, reduced_train_loss=0.152, global_step=760.0, val_loss=0.0562] \n",
      "Epoch 10:   8%|▊         | 8/104 [00:01<00:21,  4.39it/s, loss=0.177, v_num=0, reduced_train_loss=0.0779, global_step=757.0, val_loss=0.141]\n",
      "Epoch 10:  12%|█▏        | 12/104 [00:02<00:20,  4.42it/s, loss=0.0664, v_num=0, reduced_train_loss=0.00998, global_step=761.0, val_loss=0.0562]\n",
      "Epoch 10:  12%|█▎        | 13/104 [00:02<00:20,  4.43it/s, loss=0.0691, v_num=0, reduced_train_loss=0.0576, global_step=762.0, val_loss=0.0562] \n",
      "Epoch 9:  83%|████████▎ | 86/104 [00:19<00:04,  4.48it/s, loss=0.231, v_num=0, reduced_train_loss=0.0496, global_step=749.0, val_loss=0.143]\n",
      "Epoch 10:  13%|█▎        | 14/104 [00:03<00:20,  4.44it/s, loss=0.0733, v_num=0, reduced_train_loss=0.105, global_step=763.0, val_loss=0.0562] \n",
      "Epoch 9:  85%|████████▍ | 88/104 [00:19<00:03,  4.53it/s, loss=0.231, v_num=0, reduced_train_loss=0.0496, global_step=749.0, val_loss=0.143]\n",
      "Epoch 10:  14%|█▍        | 15/104 [00:03<00:20,  4.44it/s, loss=0.0618, v_num=0, reduced_train_loss=0.0071, global_step=764.0, val_loss=0.0562]\n",
      "Epoch 9:  87%|████████▋ | 90/104 [00:19<00:03,  4.57it/s, loss=0.231, v_num=0, reduced_train_loss=0.0496, global_step=749.0, val_loss=0.143]\n",
      "Epoch 10:  15%|█▌        | 16/104 [00:03<00:19,  4.44it/s, loss=0.0688, v_num=0, reduced_train_loss=0.159, global_step=765.0, val_loss=0.0562] \n",
      "Epoch 9:  88%|████████▊ | 92/104 [00:19<00:02,  4.61it/s, loss=0.231, v_num=0, reduced_train_loss=0.0496, global_step=749.0, val_loss=0.143]\n",
      "Epoch 9:  89%|████████▉ | 93/104 [00:20<00:02,  4.63it/s, loss=0.231, v_num=0, reduced_train_loss=0.0496, global_step=749.0, val_loss=0.143]\n",
      "Epoch 10:  11%|█         | 11/104 [00:03<00:26,  3.52it/s, loss=0.172, v_num=0, reduced_train_loss=0.314, global_step=760.0, val_loss=0.141] \n",
      "Epoch 9:  91%|█████████▏| 95/104 [00:20<00:01,  4.68it/s, loss=0.231, v_num=0, reduced_train_loss=0.0496, global_step=749.0, val_loss=0.143]\n",
      "Epoch 10:  12%|█▏        | 12/104 [00:03<00:25,  3.58it/s, loss=0.153, v_num=0, reduced_train_loss=0.0149, global_step=761.0, val_loss=0.141]\n",
      "Epoch 10:  12%|█▎        | 13/104 [00:03<00:25,  3.64it/s, loss=0.163, v_num=0, reduced_train_loss=0.255, global_step=762.0, val_loss=0.141] 2]\n",
      "Epoch 9:  94%|█████████▍| 98/104 [00:20<00:01,  4.74it/s, loss=0.231, v_num=0, reduced_train_loss=0.0496, global_step=749.0, val_loss=0.143]\n",
      "Epoch 10:  13%|█▎        | 14/104 [00:03<00:24,  3.69it/s, loss=0.172, v_num=0, reduced_train_loss=0.199, global_step=763.0, val_loss=0.141]2] \n",
      "Epoch 10:  18%|█▊        | 19/104 [00:04<00:21,  3.94it/s, loss=0.0678, v_num=0, reduced_train_loss=0.0727, global_step=768.0, val_loss=0.0562]\n",
      "Epoch 10:  14%|█▍        | 15/104 [00:04<00:23,  3.74it/s, loss=0.206, v_num=0, reduced_train_loss=0.700, global_step=764.0, val_loss=0.141]]\n",
      "Epoch 10:  19%|█▉        | 20/104 [00:05<00:21,  3.97it/s, loss=0.0792, v_num=0, reduced_train_loss=0.275, global_step=769.0, val_loss=0.0562] \n",
      "Epoch 10:  15%|█▌        | 16/104 [00:04<00:23,  3.78it/s, loss=0.211, v_num=0, reduced_train_loss=0.0989, global_step=765.0, val_loss=0.141]\n",
      "Epoch 9: 100%|██████████| 104/104 [00:21<00:00,  4.86it/s, loss=0.231, v_num=0, reduced_train_loss=0.0496, global_step=749.0, val_loss=0.143]2023-05-03 21:59:35,337 - root - INFO - val_loss: 0.15198786556720734\n",
      "Epoch 9: 100%|██████████| 104/104 [00:21<00:00,  4.86it/s, loss=0.231, v_num=0, reduced_train_loss=0.0496, global_step=749.0, val_loss=0.152]\n",
      "Epoch 10:   0%|          | 0/104 [00:00<?, ?it/s, loss=0.231, v_num=0, reduced_train_loss=0.0496, global_step=749.0, val_loss=0.152]         "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 21:59:35.337958 139755592410944 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.15198786556720734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10:  72%|███████▏  | 75/104 [00:17<00:06,  4.38it/s, loss=0.0429, v_num=0, reduced_train_loss=0.0769, global_step=824.0, val_loss=0.0562]  \n",
      "Epoch 10:  68%|██████▊   | 71/104 [00:16<00:07,  4.36it/s, loss=0.0727, v_num=0, reduced_train_loss=0.0121, global_step=820.0, val_loss=0.141] \n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 10:  50%|█████     | 52/104 [00:12<00:12,  4.32it/s, loss=0.166, v_num=0, reduced_train_loss=0.363, global_step=801.0, val_loss=0.152] \n",
      "Epoch 10:  51%|█████     | 53/104 [00:12<00:11,  4.32it/s, loss=0.18, v_num=0, reduced_train_loss=0.387, global_step=802.0, val_loss=0.152] 41]\n",
      "Epoch 10:  74%|███████▍  | 77/104 [00:17<00:06,  4.42it/s, loss=0.0429, v_num=0, reduced_train_loss=0.0769, global_step=824.0, val_loss=0.0562]\n",
      "Epoch 10:  52%|█████▏    | 54/104 [00:12<00:11,  4.32it/s, loss=0.172, v_num=0, reduced_train_loss=0.026, global_step=803.0, val_loss=0.152]1] \n",
      "Epoch 10:  71%|███████   | 74/104 [00:16<00:06,  4.37it/s, loss=0.0744, v_num=0, reduced_train_loss=0.0601, global_step=823.0, val_loss=0.141]]\n",
      "Epoch 10:  53%|█████▎    | 55/104 [00:12<00:11,  4.32it/s, loss=0.167, v_num=0, reduced_train_loss=0.0303, global_step=804.0, val_loss=0.152]2]\n",
      "Epoch 10:  72%|███████▏  | 75/104 [00:17<00:06,  4.38it/s, loss=0.0753, v_num=0, reduced_train_loss=0.0335, global_step=824.0, val_loss=0.141]]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 10:  54%|█████▍    | 56/104 [00:12<00:11,  4.33it/s, loss=0.172, v_num=0, reduced_train_loss=0.160, global_step=805.0, val_loss=0.152] 2]\n",
      "Epoch 10:  73%|███████▎  | 76/104 [00:17<00:06,  4.39it/s, loss=0.0753, v_num=0, reduced_train_loss=0.0335, global_step=824.0, val_loss=0.141]\n",
      "Epoch 10:  80%|███████▉  | 83/104 [00:18<00:04,  4.57it/s, loss=0.0429, v_num=0, reduced_train_loss=0.0769, global_step=824.0, val_loss=0.0562]\n",
      "Epoch 10:  74%|███████▍  | 77/104 [00:17<00:06,  4.42it/s, loss=0.0753, v_num=0, reduced_train_loss=0.0335, global_step=824.0, val_loss=0.141]\n",
      "Epoch 10:  55%|█████▍    | 57/104 [00:13<00:10,  4.33it/s, loss=0.194, v_num=0, reduced_train_loss=0.655, global_step=806.0, val_loss=0.152]62]\n",
      "Epoch 10:  75%|███████▌  | 78/104 [00:17<00:05,  4.45it/s, loss=0.0753, v_num=0, reduced_train_loss=0.0335, global_step=824.0, val_loss=0.141]\n",
      "Epoch 10:  82%|████████▏ | 85/104 [00:18<00:04,  4.62it/s, loss=0.0429, v_num=0, reduced_train_loss=0.0769, global_step=824.0, val_loss=0.0562]\n",
      "Epoch 10:  76%|███████▌  | 79/104 [00:17<00:05,  4.47it/s, loss=0.0753, v_num=0, reduced_train_loss=0.0335, global_step=824.0, val_loss=0.141]\n",
      "Epoch 10:  56%|█████▌    | 58/104 [00:13<00:10,  4.33it/s, loss=0.16, v_num=0, reduced_train_loss=0.0449, global_step=807.0, val_loss=0.152]62]\n",
      "Epoch 10:  77%|███████▋  | 80/104 [00:17<00:05,  4.50it/s, loss=0.0753, v_num=0, reduced_train_loss=0.0335, global_step=824.0, val_loss=0.141]\n",
      "Epoch 10:  84%|████████▎ | 87/104 [00:18<00:03,  4.66it/s, loss=0.0429, v_num=0, reduced_train_loss=0.0769, global_step=824.0, val_loss=0.0562]\n",
      "Epoch 10:  57%|█████▋    | 59/104 [00:13<00:10,  4.33it/s, loss=0.143, v_num=0, reduced_train_loss=0.00908, global_step=808.0, val_loss=0.152]\n",
      "Epoch 10:  85%|████████▍ | 88/104 [00:18<00:03,  4.69it/s, loss=0.0429, v_num=0, reduced_train_loss=0.0769, global_step=824.0, val_loss=0.0562]\n",
      "Epoch 10:  79%|███████▉  | 82/104 [00:18<00:04,  4.55it/s, loss=0.0753, v_num=0, reduced_train_loss=0.0335, global_step=824.0, val_loss=0.141]\n",
      "Epoch 10:  58%|█████▊    | 60/104 [00:13<00:10,  4.34it/s, loss=0.135, v_num=0, reduced_train_loss=0.00232, global_step=809.0, val_loss=0.152]]\n",
      "Epoch 10:  80%|███████▉  | 83/104 [00:18<00:04,  4.58it/s, loss=0.0753, v_num=0, reduced_train_loss=0.0335, global_step=824.0, val_loss=0.141]\n",
      "Epoch 10:  87%|████████▋ | 90/104 [00:19<00:02,  4.73it/s, loss=0.0429, v_num=0, reduced_train_loss=0.0769, global_step=824.0, val_loss=0.0562]\n",
      "Epoch 10:  81%|████████  | 84/104 [00:18<00:04,  4.60it/s, loss=0.0753, v_num=0, reduced_train_loss=0.0335, global_step=824.0, val_loss=0.141]\n",
      "Epoch 10:  59%|█████▊    | 61/104 [00:14<00:09,  4.34it/s, loss=0.136, v_num=0, reduced_train_loss=0.107, global_step=810.0, val_loss=0.152]  ]\n",
      "Epoch 10:  82%|████████▏ | 85/104 [00:18<00:04,  4.63it/s, loss=0.0753, v_num=0, reduced_train_loss=0.0335, global_step=824.0, val_loss=0.141]\n",
      "Epoch 10:  88%|████████▊ | 92/104 [00:19<00:02,  4.78it/s, loss=0.0429, v_num=0, reduced_train_loss=0.0769, global_step=824.0, val_loss=0.0562]\n",
      "Epoch 10:  83%|████████▎ | 86/104 [00:18<00:03,  4.65it/s, loss=0.0753, v_num=0, reduced_train_loss=0.0335, global_step=824.0, val_loss=0.141]\n",
      "Epoch 10:  60%|█████▉    | 62/104 [00:14<00:09,  4.34it/s, loss=0.14, v_num=0, reduced_train_loss=0.159, global_step=811.0, val_loss=0.152] 62]\n",
      "Epoch 10:  84%|████████▎ | 87/104 [00:18<00:03,  4.67it/s, loss=0.0753, v_num=0, reduced_train_loss=0.0335, global_step=824.0, val_loss=0.141]\n",
      "Epoch 10:  90%|█████████ | 94/104 [00:19<00:02,  4.82it/s, loss=0.0429, v_num=0, reduced_train_loss=0.0769, global_step=824.0, val_loss=0.0562]\n",
      "Epoch 10:  85%|████████▍ | 88/104 [00:18<00:03,  4.70it/s, loss=0.0753, v_num=0, reduced_train_loss=0.0335, global_step=824.0, val_loss=0.141]\n",
      "Epoch 10:  61%|██████    | 63/104 [00:14<00:09,  4.34it/s, loss=0.144, v_num=0, reduced_train_loss=0.206, global_step=812.0, val_loss=0.152]62]\n",
      "Epoch 10:  86%|████████▌ | 89/104 [00:18<00:03,  4.72it/s, loss=0.0753, v_num=0, reduced_train_loss=0.0335, global_step=824.0, val_loss=0.141]\n",
      "Epoch 10:  92%|█████████▏| 96/104 [00:19<00:01,  4.86it/s, loss=0.0429, v_num=0, reduced_train_loss=0.0769, global_step=824.0, val_loss=0.0562]\n",
      "Epoch 10:  87%|████████▋ | 90/104 [00:18<00:02,  4.75it/s, loss=0.0753, v_num=0, reduced_train_loss=0.0335, global_step=824.0, val_loss=0.141]\n",
      "Epoch 10:  62%|██████▏   | 64/104 [00:14<00:09,  4.35it/s, loss=0.166, v_num=0, reduced_train_loss=0.537, global_step=813.0, val_loss=0.152]62]\n",
      "Epoch 10:  88%|████████▊ | 91/104 [00:19<00:02,  4.77it/s, loss=0.0753, v_num=0, reduced_train_loss=0.0335, global_step=824.0, val_loss=0.141]\n",
      "Epoch 10:  94%|█████████▍| 98/104 [00:19<00:01,  4.91it/s, loss=0.0429, v_num=0, reduced_train_loss=0.0769, global_step=824.0, val_loss=0.0562]\n",
      "Epoch 10:  88%|████████▊ | 92/104 [00:19<00:02,  4.79it/s, loss=0.0753, v_num=0, reduced_train_loss=0.0335, global_step=824.0, val_loss=0.141]\n",
      "Epoch 10:  62%|██████▎   | 65/104 [00:14<00:08,  4.35it/s, loss=0.16, v_num=0, reduced_train_loss=0.0247, global_step=814.0, val_loss=0.152]62]\n",
      "Epoch 10:  89%|████████▉ | 93/104 [00:19<00:02,  4.82it/s, loss=0.0753, v_num=0, reduced_train_loss=0.0335, global_step=824.0, val_loss=0.141]\n",
      "Epoch 10:  96%|█████████▌| 100/104 [00:20<00:00,  4.95it/s, loss=0.0429, v_num=0, reduced_train_loss=0.0769, global_step=824.0, val_loss=0.0562]\n",
      "Epoch 10:  63%|██████▎   | 66/104 [00:15<00:08,  4.35it/s, loss=0.151, v_num=0, reduced_train_loss=0.0251, global_step=815.0, val_loss=0.152]]\n",
      "Epoch 10:  97%|█████████▋| 101/104 [00:20<00:00,  4.97it/s, loss=0.0429, v_num=0, reduced_train_loss=0.0769, global_step=824.0, val_loss=0.0562]\n",
      "Epoch 10:  91%|█████████▏| 95/104 [00:19<00:01,  4.86it/s, loss=0.0753, v_num=0, reduced_train_loss=0.0335, global_step=824.0, val_loss=0.141]\n",
      "Epoch 10:  98%|█████████▊| 102/104 [00:20<00:00,  4.99it/s, loss=0.0429, v_num=0, reduced_train_loss=0.0769, global_step=824.0, val_loss=0.0562]\n",
      "Epoch 10:  64%|██████▍   | 67/104 [00:15<00:08,  4.35it/s, loss=0.167, v_num=0, reduced_train_loss=0.419, global_step=816.0, val_loss=0.152] ]\n",
      "Epoch 10:  99%|█████████▉| 103/104 [00:20<00:00,  5.01it/s, loss=0.0429, v_num=0, reduced_train_loss=0.0769, global_step=824.0, val_loss=0.0562]\n",
      "Epoch 10: 100%|██████████| 104/104 [00:20<00:00,  5.04it/s, loss=0.0429, v_num=0, reduced_train_loss=0.0769, global_step=824.0, val_loss=0.0562]2023-05-03 21:59:50,828 - root - INFO - val_loss: 0.06780713051557541\n",
      "Epoch 10: 100%|██████████| 104/104 [00:20<00:00,  5.04it/s, loss=0.0429, v_num=0, reduced_train_loss=0.0769, global_step=824.0, val_loss=0.0678]\n",
      "Epoch 11:   0%|          | 0/104 [00:00<?, ?it/s, loss=0.0429, v_num=0, reduced_train_loss=0.0769, global_step=824.0, val_loss=0.0678]          \n",
      "Epoch 10:  93%|█████████▎| 97/104 [00:19<00:01,  4.90it/s, loss=0.0753, v_num=0, reduced_train_loss=0.0335, global_step=824.0, val_loss=0.141]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 21:59:50.828982 140649257482048 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.06780713051557541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10:  65%|██████▌   | 68/104 [00:15<00:08,  4.35it/s, loss=0.164, v_num=0, reduced_train_loss=0.0321, global_step=817.0, val_loss=0.152]]\n",
      "Epoch 11:   1%|          | 1/104 [00:00<00:27,  3.79it/s, loss=0.0568, v_num=0, reduced_train_loss=0.383, global_step=825.0, val_loss=0.0678] \n",
      "Epoch 10:  66%|██████▋   | 69/104 [00:15<00:08,  4.35it/s, loss=0.169, v_num=0, reduced_train_loss=0.104, global_step=818.0, val_loss=0.152] 1]\n",
      "Epoch 11:   2%|▏         | 2/104 [00:00<00:24,  4.15it/s, loss=0.0592, v_num=0, reduced_train_loss=0.0563, global_step=826.0, val_loss=0.0678]]\n",
      "Epoch 10:  67%|██████▋   | 70/104 [00:16<00:07,  4.35it/s, loss=0.191, v_num=0, reduced_train_loss=0.483, global_step=819.0, val_loss=0.152]41]\n",
      "Epoch 11:   3%|▎         | 3/104 [00:00<00:23,  4.26it/s, loss=0.0591, v_num=0, reduced_train_loss=0.000503, global_step=827.0, val_loss=0.0678]\n",
      "Epoch 10: 100%|██████████| 104/104 [00:20<00:00,  5.06it/s, loss=0.0753, v_num=0, reduced_train_loss=0.0335, global_step=824.0, val_loss=0.141]2023-05-03 21:59:51,594 - root - INFO - val_loss: 0.11595574021339417\n",
      "Epoch 10: 100%|██████████| 104/104 [00:20<00:00,  5.06it/s, loss=0.0753, v_num=0, reduced_train_loss=0.0335, global_step=824.0, val_loss=0.116]\n",
      "Epoch 11:   0%|          | 0/104 [00:00<?, ?it/s, loss=0.0753, v_num=0, reduced_train_loss=0.0335, global_step=824.0, val_loss=0.116]          "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 21:59:51.594717 140522389899072 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.11595574021339417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10:  72%|███████▏  | 75/104 [00:17<00:06,  4.36it/s, loss=0.191, v_num=0, reduced_train_loss=0.196, global_step=824.0, val_loss=0.152] ]] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 11:   8%|▊         | 8/104 [00:01<00:21,  4.41it/s, loss=0.0605, v_num=0, reduced_train_loss=0.00405, global_step=832.0, val_loss=0.0678]\n",
      "Epoch 11:   5%|▍         | 5/104 [00:01<00:22,  4.42it/s, loss=0.0598, v_num=0, reduced_train_loss=0.00193, global_step=829.0, val_loss=0.116]\n",
      "Epoch 11:   6%|▌         | 6/104 [00:01<00:22,  4.44it/s, loss=0.0598, v_num=0, reduced_train_loss=0.00364, global_step=830.0, val_loss=0.116] \n",
      "Epoch 10:  75%|███████▌  | 78/104 [00:17<00:05,  4.43it/s, loss=0.191, v_num=0, reduced_train_loss=0.196, global_step=824.0, val_loss=0.152]\n",
      "Epoch 11:   7%|▋         | 7/104 [00:01<00:21,  4.47it/s, loss=0.0464, v_num=0, reduced_train_loss=0.0258, global_step=831.0, val_loss=0.116] \n",
      "Epoch 11:  11%|█         | 11/104 [00:02<00:21,  4.43it/s, loss=0.0684, v_num=0, reduced_train_loss=0.0105, global_step=835.0, val_loss=0.0678]\n",
      "Epoch 11:   8%|▊         | 8/104 [00:01<00:21,  4.49it/s, loss=0.0433, v_num=0, reduced_train_loss=0.00283, global_step=832.0, val_loss=0.116]\n",
      "Epoch 11:  12%|█▏        | 12/104 [00:02<00:20,  4.43it/s, loss=0.0674, v_num=0, reduced_train_loss=0.0245, global_step=836.0, val_loss=0.0678]\n",
      "Epoch 11:   9%|▊         | 9/104 [00:01<00:21,  4.50it/s, loss=0.0505, v_num=0, reduced_train_loss=0.155, global_step=833.0, val_loss=0.116]  \n",
      "Epoch 11:  12%|█▎        | 13/104 [00:02<00:20,  4.43it/s, loss=0.0712, v_num=0, reduced_train_loss=0.0843, global_step=837.0, val_loss=0.0678]\n",
      "Epoch 11:  10%|▉         | 10/104 [00:02<00:20,  4.50it/s, loss=0.0433, v_num=0, reduced_train_loss=0.00581, global_step=834.0, val_loss=0.116]\n",
      "Epoch 11:  11%|█         | 11/104 [00:02<00:20,  4.51it/s, loss=0.0432, v_num=0, reduced_train_loss=0.172, global_step=835.0, val_loss=0.116]  ]\n",
      "Epoch 10:  84%|████████▎ | 87/104 [00:18<00:03,  4.65it/s, loss=0.191, v_num=0, reduced_train_loss=0.196, global_step=824.0, val_loss=0.152]\n",
      "Epoch 11:  12%|█▏        | 12/104 [00:02<00:20,  4.52it/s, loss=0.0359, v_num=0, reduced_train_loss=0.000286, global_step=836.0, val_loss=0.116]\n",
      "Epoch 10:  86%|████████▌ | 89/104 [00:18<00:03,  4.69it/s, loss=0.191, v_num=0, reduced_train_loss=0.196, global_step=824.0, val_loss=0.152]\n",
      "Epoch 11:  12%|█▎        | 13/104 [00:02<00:20,  4.53it/s, loss=0.0357, v_num=0, reduced_train_loss=0.0029, global_step=837.0, val_loss=0.116]  \n",
      "Epoch 11:  16%|█▋        | 17/104 [00:03<00:19,  4.44it/s, loss=0.0793, v_num=0, reduced_train_loss=0.157, global_step=840.0, val_loss=0.0678]\n",
      "Epoch 11:  13%|█▎        | 14/104 [00:03<00:19,  4.53it/s, loss=0.0328, v_num=0, reduced_train_loss=0.0178, global_step=838.0, val_loss=0.116]]\n",
      "Epoch 11:  14%|█▍        | 15/104 [00:03<00:19,  4.54it/s, loss=0.0328, v_num=0, reduced_train_loss=0.00412, global_step=839.0, val_loss=0.116]]\n",
      "Epoch 10:  90%|█████████ | 94/104 [00:19<00:02,  4.80it/s, loss=0.191, v_num=0, reduced_train_loss=0.196, global_step=824.0, val_loss=0.152]\n",
      "Epoch 11:  15%|█▌        | 16/104 [00:03<00:19,  4.54it/s, loss=0.0336, v_num=0, reduced_train_loss=0.0271, global_step=840.0, val_loss=0.116]  \n",
      "Epoch 10:  92%|█████████▏| 96/104 [00:19<00:01,  4.85it/s, loss=0.191, v_num=0, reduced_train_loss=0.196, global_step=824.0, val_loss=0.152]\n",
      "Epoch 11:  16%|█▋        | 17/104 [00:03<00:19,  4.55it/s, loss=0.0334, v_num=0, reduced_train_loss=0.003, global_step=841.0, val_loss=0.116] 8]\n",
      "Epoch 10:  94%|█████████▍| 98/104 [00:20<00:01,  4.89it/s, loss=0.191, v_num=0, reduced_train_loss=0.196, global_step=824.0, val_loss=0.152]\n",
      "Epoch 11:  17%|█▋        | 18/104 [00:03<00:18,  4.55it/s, loss=0.0316, v_num=0, reduced_train_loss=0.00154, global_step=842.0, val_loss=0.116] \n",
      "Epoch 10:  96%|█████████▌| 100/104 [00:20<00:00,  4.93it/s, loss=0.191, v_num=0, reduced_train_loss=0.196, global_step=824.0, val_loss=0.152]\n",
      "Epoch 11:  21%|██        | 22/104 [00:04<00:18,  4.46it/s, loss=0.0532, v_num=0, reduced_train_loss=0.013, global_step=846.0, val_loss=0.0678] \n",
      "Epoch 11:  22%|██▏       | 23/104 [00:05<00:18,  4.46it/s, loss=0.0534, v_num=0, reduced_train_loss=0.00461, global_step=847.0, val_loss=0.0678]\n",
      "Epoch 10:  99%|█████████▉| 103/104 [00:20<00:00,  4.99it/s, loss=0.191, v_num=0, reduced_train_loss=0.196, global_step=824.0, val_loss=0.152]\n",
      "Epoch 10: 100%|██████████| 104/104 [00:20<00:00,  5.02it/s, loss=0.191, v_num=0, reduced_train_loss=0.196, global_step=824.0, val_loss=0.152]2023-05-03 21:59:56,059 - root - INFO - val_loss: 0.20047898590564728\n",
      "Epoch 10: 100%|██████████| 104/104 [00:20<00:00,  5.02it/s, loss=0.191, v_num=0, reduced_train_loss=0.196, global_step=824.0, val_loss=0.200]\n",
      "Epoch 11:   0%|          | 0/104 [00:00<?, ?it/s, loss=0.191, v_num=0, reduced_train_loss=0.196, global_step=824.0, val_loss=0.200]          "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 21:59:56.059919 139755592410944 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.20047898590564728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11:  69%|██████▉   | 72/104 [00:15<00:07,  4.52it/s, loss=0.0528, v_num=0, reduced_train_loss=0.0083, global_step=895.0, val_loss=0.116]   \n",
      "Epoch 11:  69%|██████▉   | 72/104 [00:15<00:07,  4.51it/s, loss=0.0725, v_num=0, reduced_train_loss=0.398, global_step=896.0, val_loss=0.116] \n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 11:  48%|████▊     | 50/104 [00:11<00:12,  4.32it/s, loss=0.205, v_num=0, reduced_train_loss=0.501, global_step=874.0, val_loss=0.200]\n",
      "Epoch 11:  70%|███████   | 73/104 [00:16<00:06,  4.52it/s, loss=0.0904, v_num=0, reduced_train_loss=0.587, global_step=897.0, val_loss=0.116]\n",
      "Epoch 11:  49%|████▉     | 51/104 [00:11<00:12,  4.32it/s, loss=0.221, v_num=0, reduced_train_loss=0.328, global_step=875.0, val_loss=0.200]]\n",
      "Epoch 11:  71%|███████   | 74/104 [00:16<00:06,  4.52it/s, loss=0.0926, v_num=0, reduced_train_loss=0.0501, global_step=898.0, val_loss=0.116]\n",
      "Epoch 11:  72%|███████▏  | 75/104 [00:16<00:06,  4.52it/s, loss=0.128, v_num=0, reduced_train_loss=0.710, global_step=899.0, val_loss=0.116]  \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11:  77%|███████▋  | 80/104 [00:17<00:05,  4.60it/s, loss=0.114, v_num=0, reduced_train_loss=0.018, global_step=899.0, val_loss=0.0678]\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 11:  51%|█████     | 53/104 [00:12<00:11,  4.33it/s, loss=0.225, v_num=0, reduced_train_loss=0.0107, global_step=877.0, val_loss=0.200]\n",
      "Epoch 11:  78%|███████▊  | 81/104 [00:17<00:04,  4.63it/s, loss=0.114, v_num=0, reduced_train_loss=0.018, global_step=899.0, val_loss=0.0678]\n",
      "Epoch 11:  73%|███████▎  | 76/104 [00:16<00:06,  4.53it/s, loss=0.128, v_num=0, reduced_train_loss=0.710, global_step=899.0, val_loss=0.116]\n",
      "Epoch 11:  79%|███████▉  | 82/104 [00:17<00:04,  4.65it/s, loss=0.114, v_num=0, reduced_train_loss=0.018, global_step=899.0, val_loss=0.0678]\n",
      "Epoch 11:  52%|█████▏    | 54/104 [00:12<00:11,  4.33it/s, loss=0.226, v_num=0, reduced_train_loss=0.121, global_step=878.0, val_loss=0.200] \n",
      "Epoch 11:  80%|███████▉  | 83/104 [00:17<00:04,  4.68it/s, loss=0.114, v_num=0, reduced_train_loss=0.018, global_step=899.0, val_loss=0.0678]\n",
      "Epoch 11:  75%|███████▌  | 78/104 [00:17<00:05,  4.58it/s, loss=0.128, v_num=0, reduced_train_loss=0.710, global_step=899.0, val_loss=0.116]\n",
      "Epoch 11:  81%|████████  | 84/104 [00:17<00:04,  4.70it/s, loss=0.114, v_num=0, reduced_train_loss=0.018, global_step=899.0, val_loss=0.0678]\n",
      "Epoch 11:  53%|█████▎    | 55/104 [00:12<00:11,  4.33it/s, loss=0.228, v_num=0, reduced_train_loss=0.051, global_step=879.0, val_loss=0.200]\n",
      "Epoch 11:  82%|████████▏ | 85/104 [00:17<00:04,  4.72it/s, loss=0.114, v_num=0, reduced_train_loss=0.018, global_step=899.0, val_loss=0.0678]\n",
      "Epoch 11:  77%|███████▋  | 80/104 [00:17<00:05,  4.64it/s, loss=0.128, v_num=0, reduced_train_loss=0.710, global_step=899.0, val_loss=0.116]\n",
      "Epoch 11:  83%|████████▎ | 86/104 [00:18<00:03,  4.75it/s, loss=0.114, v_num=0, reduced_train_loss=0.018, global_step=899.0, val_loss=0.0678]\n",
      "Epoch 11:  54%|█████▍    | 56/104 [00:12<00:11,  4.33it/s, loss=0.229, v_num=0, reduced_train_loss=0.113, global_step=880.0, val_loss=0.200]\n",
      "Epoch 11:  84%|████████▎ | 87/104 [00:18<00:03,  4.77it/s, loss=0.114, v_num=0, reduced_train_loss=0.018, global_step=899.0, val_loss=0.0678]\n",
      "Epoch 11:  79%|███████▉  | 82/104 [00:17<00:04,  4.69it/s, loss=0.128, v_num=0, reduced_train_loss=0.710, global_step=899.0, val_loss=0.116]\n",
      "Epoch 11:  85%|████████▍ | 88/104 [00:18<00:03,  4.79it/s, loss=0.114, v_num=0, reduced_train_loss=0.018, global_step=899.0, val_loss=0.0678]\n",
      "Epoch 11:  55%|█████▍    | 57/104 [00:13<00:10,  4.33it/s, loss=0.201, v_num=0, reduced_train_loss=0.165, global_step=881.0, val_loss=0.200]\n",
      "Epoch 11:  81%|████████  | 84/104 [00:17<00:04,  4.74it/s, loss=0.128, v_num=0, reduced_train_loss=0.710, global_step=899.0, val_loss=0.116]\n",
      "Epoch 11:  86%|████████▌ | 89/104 [00:18<00:03,  4.81it/s, loss=0.114, v_num=0, reduced_train_loss=0.018, global_step=899.0, val_loss=0.0678]\n",
      "Epoch 11:  82%|████████▏ | 85/104 [00:17<00:03,  4.76it/s, loss=0.128, v_num=0, reduced_train_loss=0.710, global_step=899.0, val_loss=0.116]\n",
      "Epoch 11:  56%|█████▌    | 58/104 [00:13<00:10,  4.32it/s, loss=0.203, v_num=0, reduced_train_loss=0.161, global_step=882.0, val_loss=0.200]]\n",
      "Epoch 11:  83%|████████▎ | 86/104 [00:17<00:03,  4.79it/s, loss=0.128, v_num=0, reduced_train_loss=0.710, global_step=899.0, val_loss=0.116]\n",
      "Epoch 11:  88%|████████▊ | 91/104 [00:18<00:02,  4.86it/s, loss=0.114, v_num=0, reduced_train_loss=0.018, global_step=899.0, val_loss=0.0678]\n",
      "Epoch 11:  84%|████████▎ | 87/104 [00:18<00:03,  4.81it/s, loss=0.128, v_num=0, reduced_train_loss=0.710, global_step=899.0, val_loss=0.116]\n",
      "Epoch 11:  57%|█████▋    | 59/104 [00:13<00:10,  4.32it/s, loss=0.185, v_num=0, reduced_train_loss=0.0315, global_step=883.0, val_loss=0.200]\n",
      "Epoch 11:  85%|████████▍ | 88/104 [00:18<00:03,  4.83it/s, loss=0.128, v_num=0, reduced_train_loss=0.710, global_step=899.0, val_loss=0.116]\n",
      "Epoch 11:  89%|████████▉ | 93/104 [00:18<00:02,  4.90it/s, loss=0.114, v_num=0, reduced_train_loss=0.018, global_step=899.0, val_loss=0.0678]\n",
      "Epoch 11:  58%|█████▊    | 60/104 [00:13<00:10,  4.33it/s, loss=0.205, v_num=0, reduced_train_loss=0.452, global_step=884.0, val_loss=0.200] \n",
      "Epoch 11:  90%|█████████ | 94/104 [00:19<00:02,  4.92it/s, loss=0.114, v_num=0, reduced_train_loss=0.018, global_step=899.0, val_loss=0.0678]\n",
      "Epoch 11:  87%|████████▋ | 90/104 [00:18<00:02,  4.88it/s, loss=0.128, v_num=0, reduced_train_loss=0.710, global_step=899.0, val_loss=0.116]\n",
      "Epoch 11:  91%|█████████▏| 95/104 [00:19<00:01,  4.94it/s, loss=0.114, v_num=0, reduced_train_loss=0.018, global_step=899.0, val_loss=0.0678]\n",
      "Epoch 11:  59%|█████▊    | 61/104 [00:14<00:09,  4.33it/s, loss=0.2, v_num=0, reduced_train_loss=0.0512, global_step=885.0, val_loss=0.200] \n",
      "Epoch 11:  92%|█████████▏| 96/104 [00:19<00:01,  4.96it/s, loss=0.114, v_num=0, reduced_train_loss=0.018, global_step=899.0, val_loss=0.0678]\n",
      "Epoch 11:  88%|████████▊ | 92/104 [00:18<00:02,  4.93it/s, loss=0.128, v_num=0, reduced_train_loss=0.710, global_step=899.0, val_loss=0.116]\n",
      "Epoch 11:  60%|█████▉    | 62/104 [00:14<00:09,  4.33it/s, loss=0.183, v_num=0, reduced_train_loss=0.0492, global_step=886.0, val_loss=0.200]\n",
      "Epoch 11:  89%|████████▉ | 93/104 [00:18<00:02,  4.95it/s, loss=0.128, v_num=0, reduced_train_loss=0.710, global_step=899.0, val_loss=0.116]\n",
      "Epoch 11:  94%|█████████▍| 98/104 [00:19<00:01,  5.00it/s, loss=0.114, v_num=0, reduced_train_loss=0.018, global_step=899.0, val_loss=0.0678]\n",
      "Epoch 11:  90%|█████████ | 94/104 [00:18<00:02,  4.97it/s, loss=0.128, v_num=0, reduced_train_loss=0.710, global_step=899.0, val_loss=0.116]\n",
      "Epoch 11:  61%|██████    | 63/104 [00:14<00:09,  4.33it/s, loss=0.179, v_num=0, reduced_train_loss=0.0339, global_step=887.0, val_loss=0.200]\n",
      "Epoch 11:  91%|█████████▏| 95/104 [00:19<00:01,  4.99it/s, loss=0.128, v_num=0, reduced_train_loss=0.710, global_step=899.0, val_loss=0.116]\n",
      "Epoch 11:  96%|█████████▌| 100/104 [00:19<00:00,  5.04it/s, loss=0.114, v_num=0, reduced_train_loss=0.018, global_step=899.0, val_loss=0.0678]\n",
      "Epoch 11:  92%|█████████▏| 96/104 [00:19<00:01,  5.01it/s, loss=0.128, v_num=0, reduced_train_loss=0.710, global_step=899.0, val_loss=0.116]\n",
      "Epoch 11:  62%|██████▏   | 64/104 [00:14<00:09,  4.33it/s, loss=0.19, v_num=0, reduced_train_loss=0.238, global_step=888.0, val_loss=0.200]  ]\n",
      "Epoch 11:  93%|█████████▎| 97/104 [00:19<00:01,  5.04it/s, loss=0.128, v_num=0, reduced_train_loss=0.710, global_step=899.0, val_loss=0.116]\n",
      "Epoch 11:  98%|█████████▊| 102/104 [00:20<00:00,  5.08it/s, loss=0.114, v_num=0, reduced_train_loss=0.018, global_step=899.0, val_loss=0.0678]\n",
      "Epoch 11:  94%|█████████▍| 98/104 [00:19<00:01,  5.05it/s, loss=0.128, v_num=0, reduced_train_loss=0.710, global_step=899.0, val_loss=0.116]\n",
      "Epoch 11:  62%|██████▎   | 65/104 [00:15<00:09,  4.33it/s, loss=0.173, v_num=0, reduced_train_loss=0.0436, global_step=889.0, val_loss=0.200]]\n",
      "Epoch 11: 100%|██████████| 104/104 [00:20<00:00,  5.13it/s, loss=0.114, v_num=0, reduced_train_loss=0.018, global_step=899.0, val_loss=0.0678]2023-05-03 22:00:11,100 - root - INFO - val_loss: 0.04365125298500061\n",
      "Epoch 11: 100%|██████████| 104/104 [00:20<00:00,  5.13it/s, loss=0.114, v_num=0, reduced_train_loss=0.018, global_step=899.0, val_loss=0.0437]\n",
      "Epoch 12:   0%|          | 0/104 [00:00<?, ?it/s, loss=0.114, v_num=0, reduced_train_loss=0.018, global_step=899.0, val_loss=0.0437]          \n",
      "Epoch 11:  95%|█████████▌| 99/104 [00:19<00:00,  5.07it/s, loss=0.128, v_num=0, reduced_train_loss=0.710, global_step=899.0, val_loss=0.116]\n",
      "Epoch 11:  96%|█████████▌| 100/104 [00:19<00:00,  5.09it/s, loss=0.128, v_num=0, reduced_train_loss=0.710, global_step=899.0, val_loss=0.116]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 22:00:11.100620 140649257482048 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.04365125298500061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11:  63%|██████▎   | 66/104 [00:15<00:08,  4.33it/s, loss=0.169, v_num=0, reduced_train_loss=0.0671, global_step=890.0, val_loss=0.200]\n",
      "Epoch 12:   1%|          | 1/104 [00:00<00:28,  3.63it/s, loss=0.11, v_num=0, reduced_train_loss=0.00225, global_step=900.0, val_loss=0.0437]\n",
      "Epoch 11:  64%|██████▍   | 67/104 [00:15<00:08,  4.34it/s, loss=0.191, v_num=0, reduced_train_loss=0.641, global_step=891.0, val_loss=0.200] \n",
      "Epoch 12:   2%|▏         | 2/104 [00:00<00:25,  3.97it/s, loss=0.128, v_num=0, reduced_train_loss=0.357, global_step=901.0, val_loss=0.0437] \n",
      "Epoch 11: 100%|██████████| 104/104 [00:20<00:00,  5.19it/s, loss=0.128, v_num=0, reduced_train_loss=0.710, global_step=899.0, val_loss=0.116]2023-05-03 22:00:11,646 - root - INFO - val_loss: 0.10708499699831009\n",
      "Epoch 11: 100%|██████████| 104/104 [00:20<00:00,  5.19it/s, loss=0.128, v_num=0, reduced_train_loss=0.710, global_step=899.0, val_loss=0.107]\n",
      "Epoch 12:   0%|          | 0/104 [00:00<?, ?it/s, loss=0.128, v_num=0, reduced_train_loss=0.710, global_step=899.0, val_loss=0.107]          "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 22:00:11.646350 140522389899072 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.10708499699831009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11:  72%|███████▏  | 75/104 [00:17<00:06,  4.34it/s, loss=0.185, v_num=0, reduced_train_loss=1.110, global_step=899.0, val_loss=0.200]]7]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 12:   8%|▊         | 8/104 [00:01<00:21,  4.37it/s, loss=0.157, v_num=0, reduced_train_loss=0.0548, global_step=907.0, val_loss=0.107] 37]\n",
      "Epoch 12:  11%|█         | 11/104 [00:02<00:21,  4.33it/s, loss=0.0722, v_num=0, reduced_train_loss=0.450, global_step=910.0, val_loss=0.0437]  \n",
      "Epoch 12:   9%|▊         | 9/104 [00:02<00:21,  4.39it/s, loss=0.165, v_num=0, reduced_train_loss=0.185, global_step=908.0, val_loss=0.107] \n",
      "Epoch 12:  12%|█▏        | 12/104 [00:02<00:21,  4.34it/s, loss=0.0701, v_num=0, reduced_train_loss=0.00177, global_step=911.0, val_loss=0.0437]\n",
      "Epoch 12:  10%|▉         | 10/104 [00:02<00:21,  4.40it/s, loss=0.166, v_num=0, reduced_train_loss=0.0123, global_step=909.0, val_loss=0.107]\n",
      "Epoch 12:  11%|█         | 11/104 [00:02<00:21,  4.42it/s, loss=0.166, v_num=0, reduced_train_loss=0.0189, global_step=910.0, val_loss=0.107]]  \n",
      "Epoch 11:  78%|███████▊  | 81/104 [00:18<00:05,  4.48it/s, loss=0.185, v_num=0, reduced_train_loss=1.110, global_step=899.0, val_loss=0.200]\n",
      "Epoch 12:  12%|█▏        | 12/104 [00:02<00:20,  4.43it/s, loss=0.162, v_num=0, reduced_train_loss=0.00298, global_step=911.0, val_loss=0.107]]\n",
      "Epoch 11:  80%|███████▉  | 83/104 [00:18<00:04,  4.52it/s, loss=0.185, v_num=0, reduced_train_loss=1.110, global_step=899.0, val_loss=0.200]\n",
      "Epoch 12:  12%|█▎        | 13/104 [00:02<00:20,  4.43it/s, loss=0.16, v_num=0, reduced_train_loss=0.0233, global_step=912.0, val_loss=0.107]  ]\n",
      "Epoch 12:  15%|█▌        | 16/104 [00:03<00:20,  4.37it/s, loss=0.0727, v_num=0, reduced_train_loss=0.234, global_step=915.0, val_loss=0.0437] \n",
      "Epoch 12:  13%|█▎        | 14/104 [00:03<00:20,  4.44it/s, loss=0.155, v_num=0, reduced_train_loss=0.00315, global_step=913.0, val_loss=0.107]\n",
      "Epoch 12:  14%|█▍        | 15/104 [00:03<00:20,  4.45it/s, loss=0.153, v_num=0, reduced_train_loss=0.0306, global_step=914.0, val_loss=0.107] 37]\n",
      "Epoch 11:  85%|████████▍ | 88/104 [00:18<00:03,  4.64it/s, loss=0.185, v_num=0, reduced_train_loss=1.110, global_step=899.0, val_loss=0.200]\n",
      "Epoch 12:  15%|█▌        | 16/104 [00:03<00:19,  4.45it/s, loss=0.157, v_num=0, reduced_train_loss=0.0895, global_step=915.0, val_loss=0.107]]   \n",
      "Epoch 11:  87%|████████▋ | 90/104 [00:19<00:02,  4.68it/s, loss=0.185, v_num=0, reduced_train_loss=1.110, global_step=899.0, val_loss=0.200]\n",
      "Epoch 12:  16%|█▋        | 17/104 [00:03<00:19,  4.45it/s, loss=0.153, v_num=0, reduced_train_loss=0.301, global_step=916.0, val_loss=0.107] ]\n",
      "Epoch 11:  88%|████████▊ | 92/104 [00:19<00:02,  4.73it/s, loss=0.185, v_num=0, reduced_train_loss=1.110, global_step=899.0, val_loss=0.200]\n",
      "Epoch 12:  17%|█▋        | 18/104 [00:04<00:19,  4.45it/s, loss=0.144, v_num=0, reduced_train_loss=0.410, global_step=917.0, val_loss=0.107]7]\n",
      "Epoch 12:  20%|██        | 21/104 [00:04<00:18,  4.39it/s, loss=0.0769, v_num=0, reduced_train_loss=0.00283, global_step=920.0, val_loss=0.0437]\n",
      "Epoch 12:  18%|█▊        | 19/104 [00:04<00:19,  4.45it/s, loss=0.177, v_num=0, reduced_train_loss=0.707, global_step=918.0, val_loss=0.107]\n",
      "Epoch 12:  19%|█▉        | 20/104 [00:04<00:18,  4.46it/s, loss=0.143, v_num=0, reduced_train_loss=0.0312, global_step=919.0, val_loss=0.107]   \n",
      "Epoch 11:  93%|█████████▎| 97/104 [00:20<00:01,  4.83it/s, loss=0.185, v_num=0, reduced_train_loss=1.110, global_step=899.0, val_loss=0.200]\n",
      "Epoch 12:  20%|██        | 21/104 [00:04<00:18,  4.46it/s, loss=0.132, v_num=0, reduced_train_loss=0.00212, global_step=920.0, val_loss=0.107]7]\n",
      "Epoch 11:  95%|█████████▌| 99/104 [00:20<00:01,  4.87it/s, loss=0.185, v_num=0, reduced_train_loss=1.110, global_step=899.0, val_loss=0.200]\n",
      "Epoch 12:  21%|██        | 22/104 [00:04<00:18,  4.46it/s, loss=0.131, v_num=0, reduced_train_loss=0.00471, global_step=921.0, val_loss=0.107]] \n",
      "Epoch 11:  97%|█████████▋| 101/104 [00:20<00:00,  4.91it/s, loss=0.185, v_num=0, reduced_train_loss=1.110, global_step=899.0, val_loss=0.200]\n",
      "Epoch 12:  22%|██▏       | 23/104 [00:05<00:18,  4.47it/s, loss=0.124, v_num=0, reduced_train_loss=0.00568, global_step=922.0, val_loss=0.107] \n",
      "Epoch 11:  99%|█████████▉| 103/104 [00:20<00:00,  4.94it/s, loss=0.185, v_num=0, reduced_train_loss=1.110, global_step=899.0, val_loss=0.200]\n",
      "Epoch 11: 100%|██████████| 104/104 [00:20<00:00,  4.98it/s, loss=0.185, v_num=0, reduced_train_loss=1.110, global_step=899.0, val_loss=0.200]2023-05-03 22:00:16,965 - root - INFO - val_loss: 0.12795104086399078\n",
      "Epoch 11: 100%|██████████| 104/104 [00:20<00:00,  4.98it/s, loss=0.185, v_num=0, reduced_train_loss=1.110, global_step=899.0, val_loss=0.128]\n",
      "Epoch 12:   0%|          | 0/104 [00:00<?, ?it/s, loss=0.185, v_num=0, reduced_train_loss=1.110, global_step=899.0, val_loss=0.128]          "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 22:00:16.965550 139755592410944 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.12795104086399078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12:  72%|███████▏  | 75/104 [00:16<00:06,  4.44it/s, loss=0.0344, v_num=0, reduced_train_loss=0.0191, global_step=974.0, val_loss=0.0437]  \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 12:  46%|████▌     | 48/104 [00:11<00:12,  4.33it/s, loss=0.12, v_num=0, reduced_train_loss=0.0306, global_step=947.0, val_loss=0.128]7]  \n",
      "Epoch 12:  72%|███████▏  | 75/104 [00:16<00:06,  4.51it/s, loss=0.085, v_num=0, reduced_train_loss=0.00868, global_step=974.0, val_loss=0.107]]\n",
      "Epoch 12:  74%|███████▍  | 77/104 [00:17<00:06,  4.49it/s, loss=0.0344, v_num=0, reduced_train_loss=0.0191, global_step=974.0, val_loss=0.0437]\n",
      "Epoch 12:  47%|████▋     | 49/104 [00:11<00:12,  4.33it/s, loss=0.123, v_num=0, reduced_train_loss=0.0718, global_step=948.0, val_loss=0.128]\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 12:  75%|███████▌  | 78/104 [00:17<00:05,  4.51it/s, loss=0.0344, v_num=0, reduced_train_loss=0.0191, global_step=974.0, val_loss=0.0437]\n",
      "Epoch 12:  48%|████▊     | 50/104 [00:11<00:12,  4.34it/s, loss=0.125, v_num=0, reduced_train_loss=0.048, global_step=949.0, val_loss=0.128] ]\n",
      "Epoch 12:  76%|███████▌  | 79/104 [00:17<00:05,  4.54it/s, loss=0.0344, v_num=0, reduced_train_loss=0.0191, global_step=974.0, val_loss=0.0437]\n",
      "Epoch 12:  74%|███████▍  | 77/104 [00:16<00:05,  4.55it/s, loss=0.085, v_num=0, reduced_train_loss=0.00868, global_step=974.0, val_loss=0.107]\n",
      "Epoch 12:  77%|███████▋  | 80/104 [00:17<00:05,  4.56it/s, loss=0.0344, v_num=0, reduced_train_loss=0.0191, global_step=974.0, val_loss=0.0437]\n",
      "Epoch 12:  49%|████▉     | 51/104 [00:11<00:12,  4.34it/s, loss=0.125, v_num=0, reduced_train_loss=0.00763, global_step=950.0, val_loss=0.128]\n",
      "Epoch 12:  78%|███████▊  | 81/104 [00:17<00:05,  4.59it/s, loss=0.0344, v_num=0, reduced_train_loss=0.0191, global_step=974.0, val_loss=0.0437]\n",
      "Epoch 12:  76%|███████▌  | 79/104 [00:17<00:05,  4.60it/s, loss=0.085, v_num=0, reduced_train_loss=0.00868, global_step=974.0, val_loss=0.107]\n",
      "Epoch 12:  79%|███████▉  | 82/104 [00:17<00:04,  4.61it/s, loss=0.0344, v_num=0, reduced_train_loss=0.0191, global_step=974.0, val_loss=0.0437]\n",
      "Epoch 12:  50%|█████     | 52/104 [00:11<00:11,  4.34it/s, loss=0.129, v_num=0, reduced_train_loss=0.135, global_step=951.0, val_loss=0.128]  \n",
      "Epoch 12:  80%|███████▉  | 83/104 [00:17<00:04,  4.64it/s, loss=0.0344, v_num=0, reduced_train_loss=0.0191, global_step=974.0, val_loss=0.0437]\n",
      "Epoch 12:  78%|███████▊  | 81/104 [00:17<00:04,  4.66it/s, loss=0.085, v_num=0, reduced_train_loss=0.00868, global_step=974.0, val_loss=0.107]\n",
      "Epoch 12:  81%|████████  | 84/104 [00:18<00:04,  4.66it/s, loss=0.0344, v_num=0, reduced_train_loss=0.0191, global_step=974.0, val_loss=0.0437]\n",
      "Epoch 12:  51%|█████     | 53/104 [00:12<00:11,  4.34it/s, loss=0.129, v_num=0, reduced_train_loss=0.00808, global_step=952.0, val_loss=0.128]\n",
      "Epoch 12:  82%|████████▏ | 85/104 [00:18<00:04,  4.69it/s, loss=0.0344, v_num=0, reduced_train_loss=0.0191, global_step=974.0, val_loss=0.0437]\n",
      "Epoch 12:  80%|███████▉  | 83/104 [00:17<00:04,  4.71it/s, loss=0.085, v_num=0, reduced_train_loss=0.00868, global_step=974.0, val_loss=0.107]\n",
      "Epoch 12:  83%|████████▎ | 86/104 [00:18<00:03,  4.71it/s, loss=0.0344, v_num=0, reduced_train_loss=0.0191, global_step=974.0, val_loss=0.0437]\n",
      "Epoch 12:  52%|█████▏    | 54/104 [00:12<00:11,  4.34it/s, loss=0.139, v_num=0, reduced_train_loss=0.254, global_step=953.0, val_loss=0.128]  \n",
      "Epoch 12:  84%|████████▎ | 87/104 [00:18<00:03,  4.73it/s, loss=0.0344, v_num=0, reduced_train_loss=0.0191, global_step=974.0, val_loss=0.0437]\n",
      "Epoch 12:  82%|████████▏ | 85/104 [00:17<00:03,  4.76it/s, loss=0.085, v_num=0, reduced_train_loss=0.00868, global_step=974.0, val_loss=0.107]\n",
      "Epoch 12:  53%|█████▎    | 55/104 [00:12<00:11,  4.34it/s, loss=0.128, v_num=0, reduced_train_loss=0.269, global_step=954.0, val_loss=0.128]37]\n",
      "Epoch 12:  83%|████████▎ | 86/104 [00:17<00:03,  4.78it/s, loss=0.085, v_num=0, reduced_train_loss=0.00868, global_step=974.0, val_loss=0.107]\n",
      "Epoch 12:  86%|████████▌ | 89/104 [00:18<00:03,  4.78it/s, loss=0.0344, v_num=0, reduced_train_loss=0.0191, global_step=974.0, val_loss=0.0437]\n",
      "Epoch 12:  84%|████████▎ | 87/104 [00:18<00:03,  4.80it/s, loss=0.085, v_num=0, reduced_train_loss=0.00868, global_step=974.0, val_loss=0.107]\n",
      "Epoch 12:  54%|█████▍    | 56/104 [00:12<00:11,  4.34it/s, loss=0.177, v_num=0, reduced_train_loss=0.991, global_step=955.0, val_loss=0.128]37]\n",
      "Epoch 12:  85%|████████▍ | 88/104 [00:18<00:03,  4.82it/s, loss=0.085, v_num=0, reduced_train_loss=0.00868, global_step=974.0, val_loss=0.107]\n",
      "Epoch 12:  88%|████████▊ | 91/104 [00:18<00:02,  4.82it/s, loss=0.0344, v_num=0, reduced_train_loss=0.0191, global_step=974.0, val_loss=0.0437]\n",
      "Epoch 12:  86%|████████▌ | 89/104 [00:18<00:03,  4.85it/s, loss=0.085, v_num=0, reduced_train_loss=0.00868, global_step=974.0, val_loss=0.107]\n",
      "Epoch 12:  55%|█████▍    | 57/104 [00:13<00:10,  4.34it/s, loss=0.182, v_num=0, reduced_train_loss=0.162, global_step=956.0, val_loss=0.128]37]\n",
      "Epoch 12:  87%|████████▋ | 90/104 [00:18<00:02,  4.87it/s, loss=0.085, v_num=0, reduced_train_loss=0.00868, global_step=974.0, val_loss=0.107]\n",
      "Epoch 12:  89%|████████▉ | 93/104 [00:19<00:02,  4.87it/s, loss=0.0344, v_num=0, reduced_train_loss=0.0191, global_step=974.0, val_loss=0.0437]\n",
      "Epoch 12:  56%|█████▌    | 58/104 [00:13<00:10,  4.34it/s, loss=0.229, v_num=0, reduced_train_loss=1.280, global_step=957.0, val_loss=0.128]7]\n",
      "Epoch 12:  90%|█████████ | 94/104 [00:19<00:02,  4.89it/s, loss=0.0344, v_num=0, reduced_train_loss=0.0191, global_step=974.0, val_loss=0.0437]\n",
      "Epoch 12:  88%|████████▊ | 92/104 [00:18<00:02,  4.91it/s, loss=0.085, v_num=0, reduced_train_loss=0.00868, global_step=974.0, val_loss=0.107]\n",
      "Epoch 12:  91%|█████████▏| 95/104 [00:19<00:01,  4.91it/s, loss=0.0344, v_num=0, reduced_train_loss=0.0191, global_step=974.0, val_loss=0.0437]\n",
      "Epoch 12:  57%|█████▋    | 59/104 [00:13<00:10,  4.34it/s, loss=0.222, v_num=0, reduced_train_loss=0.00456, global_step=958.0, val_loss=0.128]\n",
      "Epoch 12:  92%|█████████▏| 96/104 [00:19<00:01,  4.93it/s, loss=0.0344, v_num=0, reduced_train_loss=0.0191, global_step=974.0, val_loss=0.0437]\n",
      "Epoch 12:  90%|█████████ | 94/104 [00:18<00:02,  4.96it/s, loss=0.085, v_num=0, reduced_train_loss=0.00868, global_step=974.0, val_loss=0.107]\n",
      "Epoch 12:  93%|█████████▎| 97/104 [00:19<00:01,  4.95it/s, loss=0.0344, v_num=0, reduced_train_loss=0.0191, global_step=974.0, val_loss=0.0437]\n",
      "Epoch 12:  58%|█████▊    | 60/104 [00:13<00:10,  4.34it/s, loss=0.254, v_num=0, reduced_train_loss=0.749, global_step=959.0, val_loss=0.128]  \n",
      "Epoch 12:  94%|█████████▍| 98/104 [00:19<00:01,  4.97it/s, loss=0.0344, v_num=0, reduced_train_loss=0.0191, global_step=974.0, val_loss=0.0437]\n",
      "Epoch 12:  92%|█████████▏| 96/104 [00:19<00:01,  5.00it/s, loss=0.085, v_num=0, reduced_train_loss=0.00868, global_step=974.0, val_loss=0.107]\n",
      "Epoch 12:  95%|█████████▌| 99/104 [00:19<00:01,  4.99it/s, loss=0.0344, v_num=0, reduced_train_loss=0.0191, global_step=974.0, val_loss=0.0437]\n",
      "Epoch 12:  59%|█████▊    | 61/104 [00:14<00:09,  4.35it/s, loss=0.243, v_num=0, reduced_train_loss=0.00648, global_step=960.0, val_loss=0.128]\n",
      "Epoch 12:  96%|█████████▌| 100/104 [00:19<00:00,  5.01it/s, loss=0.0344, v_num=0, reduced_train_loss=0.0191, global_step=974.0, val_loss=0.0437]\n",
      "Epoch 12:  94%|█████████▍| 98/104 [00:19<00:01,  5.04it/s, loss=0.085, v_num=0, reduced_train_loss=0.00868, global_step=974.0, val_loss=0.107]\n",
      "Epoch 12:  97%|█████████▋| 101/104 [00:20<00:00,  5.03it/s, loss=0.0344, v_num=0, reduced_train_loss=0.0191, global_step=974.0, val_loss=0.0437]\n",
      "Epoch 12:  60%|█████▉    | 62/104 [00:14<00:09,  4.35it/s, loss=0.262, v_num=0, reduced_train_loss=0.404, global_step=961.0, val_loss=0.128]  \n",
      "Epoch 12:  98%|█████████▊| 102/104 [00:20<00:00,  5.05it/s, loss=0.0344, v_num=0, reduced_train_loss=0.0191, global_step=974.0, val_loss=0.0437]\n",
      "Epoch 12:  96%|█████████▌| 100/104 [00:19<00:00,  5.08it/s, loss=0.085, v_num=0, reduced_train_loss=0.00868, global_step=974.0, val_loss=0.107]\n",
      "Epoch 12:  61%|██████    | 63/104 [00:14<00:09,  4.35it/s, loss=0.255, v_num=0, reduced_train_loss=0.0044, global_step=962.0, val_loss=0.128]37]\n",
      "Epoch 12:  97%|█████████▋| 101/104 [00:19<00:00,  5.10it/s, loss=0.085, v_num=0, reduced_train_loss=0.00868, global_step=974.0, val_loss=0.107]\n",
      "Epoch 12: 100%|██████████| 104/104 [00:20<00:00,  5.11it/s, loss=0.0344, v_num=0, reduced_train_loss=0.0191, global_step=974.0, val_loss=0.0437]2023-05-03 22:00:31,471 - root - INFO - val_loss: 0.06056351959705353\n",
      "Epoch 12: 100%|██████████| 104/104 [00:20<00:00,  5.11it/s, loss=0.0344, v_num=0, reduced_train_loss=0.0191, global_step=974.0, val_loss=0.0606]\n",
      "Epoch 13:   0%|          | 0/104 [00:00<?, ?it/s, loss=0.0344, v_num=0, reduced_train_loss=0.0191, global_step=974.0, val_loss=0.0606]          \n",
      "Epoch 12:  98%|█████████▊| 102/104 [00:19<00:00,  5.12it/s, loss=0.085, v_num=0, reduced_train_loss=0.00868, global_step=974.0, val_loss=0.107]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 22:00:31.471611 140649257482048 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.06056351959705353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12:  62%|██████▏   | 64/104 [00:14<00:09,  4.35it/s, loss=0.275, v_num=0, reduced_train_loss=0.441, global_step=963.0, val_loss=0.128] \n",
      "Epoch 13:   1%|          | 1/104 [00:00<00:27,  3.77it/s, loss=0.0393, v_num=0, reduced_train_loss=0.101, global_step=975.0, val_loss=0.0606] ]\n",
      "Epoch 12: 100%|██████████| 104/104 [00:20<00:00,  5.17it/s, loss=0.085, v_num=0, reduced_train_loss=0.00868, global_step=974.0, val_loss=0.107]2023-05-03 22:00:31,762 - root - INFO - val_loss: 0.08079487085342407\n",
      "Epoch 12: 100%|██████████| 104/104 [00:20<00:00,  5.17it/s, loss=0.085, v_num=0, reduced_train_loss=0.00868, global_step=974.0, val_loss=0.0808]\n",
      "Epoch 13:   0%|          | 0/104 [00:00<?, ?it/s, loss=0.085, v_num=0, reduced_train_loss=0.00868, global_step=974.0, val_loss=0.0808]          "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 22:00:31.762665 140522389899072 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.08079487085342407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12:  72%|███████▏  | 75/104 [00:17<00:06,  4.37it/s, loss=0.303, v_num=0, reduced_train_loss=0.0834, global_step=974.0, val_loss=0.128]808]\n",
      "Epoch 13:  12%|█▏        | 12/104 [00:02<00:20,  4.47it/s, loss=0.0435, v_num=0, reduced_train_loss=0.0623, global_step=986.0, val_loss=0.0606] \n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 13:  11%|█         | 11/104 [00:02<00:21,  4.36it/s, loss=0.0441, v_num=0, reduced_train_loss=0.032, global_step=985.0, val_loss=0.0808]   \n",
      "Epoch 13:  12%|█▎        | 13/104 [00:02<00:20,  4.48it/s, loss=0.0436, v_num=0, reduced_train_loss=0.0024, global_step=987.0, val_loss=0.0606]\n",
      "Epoch 13:  12%|█▏        | 12/104 [00:02<00:21,  4.37it/s, loss=0.0494, v_num=0, reduced_train_loss=0.137, global_step=986.0, val_loss=0.0808]\n",
      "Epoch 13:  13%|█▎        | 14/104 [00:03<00:20,  4.48it/s, loss=0.0402, v_num=0, reduced_train_loss=0.0113, global_step=988.0, val_loss=0.0606]\n",
      "Epoch 13:  12%|█▎        | 13/104 [00:02<00:20,  4.37it/s, loss=0.0492, v_num=0, reduced_train_loss=0.00333, global_step=987.0, val_loss=0.0808]\n",
      "Epoch 13:  14%|█▍        | 15/104 [00:03<00:19,  4.48it/s, loss=0.0388, v_num=0, reduced_train_loss=0.00551, global_step=989.0, val_loss=0.0606]\n",
      "Epoch 13:  15%|█▌        | 16/104 [00:03<00:19,  4.48it/s, loss=0.0388, v_num=0, reduced_train_loss=0.00551, global_step=989.0, val_loss=0.0606]\n",
      "Epoch 12:  79%|███████▉  | 82/104 [00:18<00:04,  4.54it/s, loss=0.303, v_num=0, reduced_train_loss=0.0834, global_step=974.0, val_loss=0.128]6] \n",
      "Epoch 13:  16%|█▋        | 17/104 [00:03<00:19,  4.48it/s, loss=0.041, v_num=0, reduced_train_loss=0.00359, global_step=991.0, val_loss=0.0606]\n",
      "Epoch 13:  15%|█▌        | 16/104 [00:03<00:20,  4.39it/s, loss=0.0635, v_num=0, reduced_train_loss=0.0121, global_step=990.0, val_loss=0.0808]\n",
      "Epoch 13:  17%|█▋        | 18/104 [00:04<00:19,  4.49it/s, loss=0.0346, v_num=0, reduced_train_loss=0.0306, global_step=992.0, val_loss=0.0606]\n",
      "Epoch 13:  16%|█▋        | 17/104 [00:03<00:19,  4.39it/s, loss=0.0633, v_num=0, reduced_train_loss=0.000373, global_step=991.0, val_loss=0.0808]\n",
      "Epoch 13:  18%|█▊        | 19/104 [00:04<00:18,  4.49it/s, loss=0.0351, v_num=0, reduced_train_loss=0.012, global_step=993.0, val_loss=0.0606] \n",
      "Epoch 13:  17%|█▋        | 18/104 [00:04<00:19,  4.39it/s, loss=0.0634, v_num=0, reduced_train_loss=0.00122, global_step=992.0, val_loss=0.0808] \n",
      "Epoch 13:  19%|█▉        | 20/104 [00:04<00:18,  4.49it/s, loss=0.0428, v_num=0, reduced_train_loss=0.173, global_step=994.0, val_loss=0.0606]\n",
      "Epoch 13:  20%|██        | 21/104 [00:04<00:18,  4.50it/s, loss=0.0379, v_num=0, reduced_train_loss=0.00279, global_step=995.0, val_loss=0.0606]\n",
      "Epoch 12:  88%|████████▊ | 91/104 [00:19<00:02,  4.75it/s, loss=0.303, v_num=0, reduced_train_loss=0.0834, global_step=974.0, val_loss=0.128]\n",
      "Epoch 13:  21%|██        | 22/104 [00:04<00:18,  4.50it/s, loss=0.0346, v_num=0, reduced_train_loss=0.00725, global_step=996.0, val_loss=0.0606]\n",
      "Epoch 12:  89%|████████▉ | 93/104 [00:19<00:02,  4.79it/s, loss=0.303, v_num=0, reduced_train_loss=0.0834, global_step=974.0, val_loss=0.128]\n",
      "Epoch 13:  22%|██▏       | 23/104 [00:05<00:17,  4.50it/s, loss=0.0328, v_num=0, reduced_train_loss=0.00535, global_step=997.0, val_loss=0.0606]\n",
      "Epoch 12:  91%|█████████▏| 95/104 [00:19<00:01,  4.83it/s, loss=0.303, v_num=0, reduced_train_loss=0.0834, global_step=974.0, val_loss=0.128]\n",
      "Epoch 13:  23%|██▎       | 24/104 [00:05<00:17,  4.51it/s, loss=0.0274, v_num=0, reduced_train_loss=0.00138, global_step=998.0, val_loss=0.0606]\n",
      "Epoch 12:  93%|█████████▎| 97/104 [00:19<00:01,  4.87it/s, loss=0.303, v_num=0, reduced_train_loss=0.0834, global_step=974.0, val_loss=0.128]\n",
      "Epoch 13:  24%|██▍       | 25/104 [00:05<00:17,  4.51it/s, loss=0.0264, v_num=0, reduced_train_loss=0.00373, global_step=999.0, val_loss=0.0606]\n",
      "Epoch 13:  23%|██▎       | 24/104 [00:05<00:18,  4.41it/s, loss=0.0509, v_num=0, reduced_train_loss=0.115, global_step=998.0, val_loss=0.0808]  \n",
      "Epoch 13:  25%|██▌       | 26/104 [00:05<00:17,  4.51it/s, loss=0.0391, v_num=0, reduced_train_loss=0.263, global_step=1e+3, val_loss=0.0606]   \n",
      "Epoch 13:  26%|██▌       | 27/104 [00:05<00:17,  4.52it/s, loss=0.0394, v_num=0, reduced_train_loss=0.0148, global_step=1e+3, val_loss=0.0606]\n",
      "Epoch 12:  98%|█████████▊| 102/104 [00:20<00:00,  4.98it/s, loss=0.303, v_num=0, reduced_train_loss=0.0834, global_step=974.0, val_loss=0.128]\n",
      "Epoch 12:  99%|█████████▉| 103/104 [00:20<00:00,  4.99it/s, loss=0.303, v_num=0, reduced_train_loss=0.0834, global_step=974.0, val_loss=0.128]\n",
      "Epoch 13:  25%|██▌       | 26/104 [00:05<00:17,  4.41it/s, loss=0.0522, v_num=0, reduced_train_loss=0.0012, global_step=1e+3, val_loss=0.0808]2023-05-03 22:00:37,661 - root - INFO - val_loss: 0.15789301693439484\n",
      "Epoch 12: 100%|██████████| 104/104 [00:20<00:00,  5.03it/s, loss=0.303, v_num=0, reduced_train_loss=0.0834, global_step=974.0, val_loss=0.158]\n",
      "Epoch 13:  27%|██▋       | 28/104 [00:06<00:16,  4.52it/s, loss=0.044, v_num=0, reduced_train_loss=0.092, global_step=1e+3, val_loss=0.0606]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 22:00:37.661150 139755592410944 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.15789301693439484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13:  72%|███████▏  | 75/104 [00:17<00:06,  4.22it/s, loss=0.108, v_num=0, reduced_train_loss=0.00187, global_step=1049.0, val_loss=0.0606]] \n",
      "Epoch 13:  72%|███████▏  | 75/104 [00:17<00:06,  4.28it/s, loss=0.0831, v_num=0, reduced_train_loss=0.0122, global_step=1049.0, val_loss=0.0808] \n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 13:  73%|███████▎  | 76/104 [00:17<00:06,  4.23it/s, loss=0.108, v_num=0, reduced_train_loss=0.00187, global_step=1049.0, val_loss=0.0606]\n",
      "Epoch 13:  49%|████▉     | 51/104 [00:11<00:12,  4.32it/s, loss=0.164, v_num=0, reduced_train_loss=0.0667, global_step=1025.0, val_loss=0.158]8]\n",
      "Epoch 13:  74%|███████▍  | 77/104 [00:18<00:06,  4.25it/s, loss=0.108, v_num=0, reduced_train_loss=0.00187, global_step=1049.0, val_loss=0.0606]\n",
      "Epoch 13:  50%|█████     | 52/104 [00:12<00:12,  4.32it/s, loss=0.157, v_num=0, reduced_train_loss=0.209, global_step=1026.0, val_loss=0.158] 8]\n",
      "Epoch 13:  75%|███████▌  | 78/104 [00:18<00:06,  4.27it/s, loss=0.108, v_num=0, reduced_train_loss=0.00187, global_step=1049.0, val_loss=0.0606]\n",
      "Epoch 13:  75%|███████▌  | 78/104 [00:17<00:05,  4.34it/s, loss=0.0831, v_num=0, reduced_train_loss=0.0122, global_step=1049.0, val_loss=0.0808]\n",
      "Epoch 13:  76%|███████▌  | 79/104 [00:18<00:05,  4.37it/s, loss=0.0831, v_num=0, reduced_train_loss=0.0122, global_step=1049.0, val_loss=0.0808]\n",
      "Epoch 13:  51%|█████     | 53/104 [00:12<00:11,  4.32it/s, loss=0.151, v_num=0, reduced_train_loss=0.0249, global_step=1027.0, val_loss=0.158]6]\n",
      "Epoch 13:  77%|███████▋  | 80/104 [00:18<00:05,  4.39it/s, loss=0.0831, v_num=0, reduced_train_loss=0.0122, global_step=1049.0, val_loss=0.0808]\n",
      "Epoch 13:  77%|███████▋  | 80/104 [00:18<00:05,  4.31it/s, loss=0.108, v_num=0, reduced_train_loss=0.00187, global_step=1049.0, val_loss=0.0606]\n",
      "Epoch 13:  52%|█████▏    | 54/104 [00:12<00:11,  4.32it/s, loss=0.145, v_num=0, reduced_train_loss=0.177, global_step=1028.0, val_loss=0.158] 8]\n",
      "Epoch 13:  78%|███████▊  | 81/104 [00:18<00:05,  4.33it/s, loss=0.108, v_num=0, reduced_train_loss=0.00187, global_step=1049.0, val_loss=0.0606]\n",
      "Epoch 13:  79%|███████▉  | 82/104 [00:18<00:04,  4.44it/s, loss=0.0831, v_num=0, reduced_train_loss=0.0122, global_step=1049.0, val_loss=0.0808]\n",
      "Epoch 13:  79%|███████▉  | 82/104 [00:18<00:05,  4.35it/s, loss=0.108, v_num=0, reduced_train_loss=0.00187, global_step=1049.0, val_loss=0.0606]\n",
      "Epoch 13:  53%|█████▎    | 55/104 [00:12<00:11,  4.33it/s, loss=0.151, v_num=0, reduced_train_loss=0.134, global_step=1029.0, val_loss=0.158]08]\n",
      "Epoch 13:  80%|███████▉  | 83/104 [00:18<00:04,  4.37it/s, loss=0.108, v_num=0, reduced_train_loss=0.00187, global_step=1049.0, val_loss=0.0606]\n",
      "Epoch 13:  81%|████████  | 84/104 [00:18<00:04,  4.49it/s, loss=0.0831, v_num=0, reduced_train_loss=0.0122, global_step=1049.0, val_loss=0.0808]\n",
      "Epoch 13:  54%|█████▍    | 56/104 [00:12<00:11,  4.32it/s, loss=0.149, v_num=0, reduced_train_loss=0.0267, global_step=1030.0, val_loss=0.158]6]\n",
      "Epoch 13:  82%|████████▏ | 85/104 [00:18<00:04,  4.51it/s, loss=0.0831, v_num=0, reduced_train_loss=0.0122, global_step=1049.0, val_loss=0.0808]\n",
      "Epoch 13:  83%|████████▎ | 86/104 [00:18<00:03,  4.53it/s, loss=0.0831, v_num=0, reduced_train_loss=0.0122, global_step=1049.0, val_loss=0.0808]\n",
      "Epoch 13:  55%|█████▍    | 57/104 [00:13<00:10,  4.33it/s, loss=0.159, v_num=0, reduced_train_loss=0.238, global_step=1031.0, val_loss=0.158] 6]\n",
      "Epoch 13:  84%|████████▎ | 87/104 [00:19<00:03,  4.56it/s, loss=0.0831, v_num=0, reduced_train_loss=0.0122, global_step=1049.0, val_loss=0.0808]\n",
      "Epoch 13:  83%|████████▎ | 86/104 [00:19<00:04,  4.43it/s, loss=0.108, v_num=0, reduced_train_loss=0.00187, global_step=1049.0, val_loss=0.0606]\n",
      "Epoch 13:  85%|████████▍ | 88/104 [00:19<00:03,  4.58it/s, loss=0.0831, v_num=0, reduced_train_loss=0.0122, global_step=1049.0, val_loss=0.0808]\n",
      "Epoch 13:  56%|█████▌    | 58/104 [00:13<00:10,  4.33it/s, loss=0.164, v_num=0, reduced_train_loss=0.157, global_step=1032.0, val_loss=0.158]06]\n",
      "Epoch 13:  86%|████████▌ | 89/104 [00:19<00:03,  4.60it/s, loss=0.0831, v_num=0, reduced_train_loss=0.0122, global_step=1049.0, val_loss=0.0808]\n",
      "Epoch 13:  85%|████████▍ | 88/104 [00:19<00:03,  4.46it/s, loss=0.108, v_num=0, reduced_train_loss=0.00187, global_step=1049.0, val_loss=0.0606]\n",
      "Epoch 13:  57%|█████▋    | 59/104 [00:13<00:10,  4.33it/s, loss=0.178, v_num=0, reduced_train_loss=0.283, global_step=1033.0, val_loss=0.158]08]\n",
      "Epoch 13:  86%|████████▌ | 89/104 [00:19<00:03,  4.48it/s, loss=0.108, v_num=0, reduced_train_loss=0.00187, global_step=1049.0, val_loss=0.0606]\n",
      "Epoch 13:  88%|████████▊ | 91/104 [00:19<00:02,  4.65it/s, loss=0.0831, v_num=0, reduced_train_loss=0.0122, global_step=1049.0, val_loss=0.0808]\n",
      "Epoch 13:  88%|████████▊ | 92/104 [00:19<00:02,  4.67it/s, loss=0.0831, v_num=0, reduced_train_loss=0.0122, global_step=1049.0, val_loss=0.0808]\n",
      "Epoch 13:  58%|█████▊    | 60/104 [00:13<00:10,  4.33it/s, loss=0.182, v_num=0, reduced_train_loss=0.0817, global_step=1034.0, val_loss=0.158]6]\n",
      "Epoch 13:  89%|████████▉ | 93/104 [00:19<00:02,  4.69it/s, loss=0.0831, v_num=0, reduced_train_loss=0.0122, global_step=1049.0, val_loss=0.0808]\n",
      "Epoch 13:  88%|████████▊ | 91/104 [00:20<00:02,  4.52it/s, loss=0.108, v_num=0, reduced_train_loss=0.00187, global_step=1049.0, val_loss=0.0606]\n",
      "Epoch 13:  59%|█████▊    | 61/104 [00:14<00:09,  4.33it/s, loss=0.163, v_num=0, reduced_train_loss=0.0517, global_step=1035.0, val_loss=0.158]8]\n",
      "Epoch 13:  88%|████████▊ | 92/104 [00:20<00:02,  4.53it/s, loss=0.108, v_num=0, reduced_train_loss=0.00187, global_step=1049.0, val_loss=0.0606]\n",
      "Epoch 13:  91%|█████████▏| 95/104 [00:20<00:01,  4.74it/s, loss=0.0831, v_num=0, reduced_train_loss=0.0122, global_step=1049.0, val_loss=0.0808]\n",
      "Epoch 13:  89%|████████▉ | 93/104 [00:20<00:02,  4.55it/s, loss=0.108, v_num=0, reduced_train_loss=0.00187, global_step=1049.0, val_loss=0.0606]\n",
      "Epoch 13:  60%|█████▉    | 62/104 [00:14<00:09,  4.34it/s, loss=0.17, v_num=0, reduced_train_loss=0.133, global_step=1036.0, val_loss=0.158]  8]\n",
      "Epoch 13:  90%|█████████ | 94/104 [00:20<00:02,  4.57it/s, loss=0.108, v_num=0, reduced_train_loss=0.00187, global_step=1049.0, val_loss=0.0606]\n",
      "Epoch 13:  61%|██████    | 63/104 [00:14<00:09,  4.34it/s, loss=0.173, v_num=0, reduced_train_loss=0.404, global_step=1037.0, val_loss=0.158]08]\n",
      "Epoch 13:  91%|█████████▏| 95/104 [00:20<00:01,  4.59it/s, loss=0.108, v_num=0, reduced_train_loss=0.00187, global_step=1049.0, val_loss=0.0606]\n",
      "Epoch 13:  94%|█████████▍| 98/104 [00:20<00:01,  4.80it/s, loss=0.0831, v_num=0, reduced_train_loss=0.0122, global_step=1049.0, val_loss=0.0808]\n",
      "Epoch 13:  95%|█████████▌| 99/104 [00:20<00:01,  4.82it/s, loss=0.0831, v_num=0, reduced_train_loss=0.0122, global_step=1049.0, val_loss=0.0808]\n",
      "Epoch 13:  62%|██████▏   | 64/104 [00:14<00:09,  4.34it/s, loss=0.157, v_num=0, reduced_train_loss=0.0299, global_step=1038.0, val_loss=0.158]6]\n",
      "Epoch 13:  96%|█████████▌| 100/104 [00:20<00:00,  4.84it/s, loss=0.0831, v_num=0, reduced_train_loss=0.0122, global_step=1049.0, val_loss=0.0808]\n",
      "Epoch 13:  93%|█████████▎| 97/104 [00:21<00:01,  4.62it/s, loss=0.108, v_num=0, reduced_train_loss=0.00187, global_step=1049.0, val_loss=0.0606]\n",
      "Epoch 13:  97%|█████████▋| 101/104 [00:20<00:00,  4.86it/s, loss=0.0831, v_num=0, reduced_train_loss=0.0122, global_step=1049.0, val_loss=0.0808]\n",
      "Epoch 13:  62%|██████▎   | 65/104 [00:14<00:08,  4.34it/s, loss=0.154, v_num=0, reduced_train_loss=0.050, global_step=1039.0, val_loss=0.158] 6]\n",
      "Epoch 13:  98%|█████████▊| 102/104 [00:20<00:00,  4.88it/s, loss=0.0831, v_num=0, reduced_train_loss=0.0122, global_step=1049.0, val_loss=0.0808]\n",
      "Epoch 13:  95%|█████████▌| 99/104 [00:21<00:01,  4.65it/s, loss=0.108, v_num=0, reduced_train_loss=0.00187, global_step=1049.0, val_loss=0.0606]\n",
      "Epoch 13:  99%|█████████▉| 103/104 [00:21<00:00,  4.90it/s, loss=0.0831, v_num=0, reduced_train_loss=0.0122, global_step=1049.0, val_loss=0.0808]\n",
      "Epoch 13:  63%|██████▎   | 66/104 [00:15<00:08,  4.34it/s, loss=0.138, v_num=0, reduced_train_loss=0.109, global_step=1040.0, val_loss=0.158]2023-05-03 22:00:52,860 - root - INFO - val_loss: 0.08299127221107483\n",
      "Epoch 13: 100%|██████████| 104/104 [00:21<00:00,  4.93it/s, loss=0.0831, v_num=0, reduced_train_loss=0.0122, global_step=1049.0, val_loss=0.083] \n",
      "Epoch 14:   0%|          | 0/104 [00:00<?, ?it/s, loss=0.0831, v_num=0, reduced_train_loss=0.0122, global_step=1049.0, val_loss=0.083]          \n",
      "Epoch 13:  96%|█████████▌| 100/104 [00:21<00:00,  4.67it/s, loss=0.108, v_num=0, reduced_train_loss=0.00187, global_step=1049.0, val_loss=0.0606]\n",
      "Epoch 13:  97%|█████████▋| 101/104 [00:21<00:00,  4.68it/s, loss=0.108, v_num=0, reduced_train_loss=0.00187, global_step=1049.0, val_loss=0.0606]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 22:00:52.860242 140522389899072 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.08299127221107483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14:   1%|          | 1/104 [00:00<00:28,  3.57it/s, loss=0.083, v_num=0, reduced_train_loss=0.00112, global_step=1050.0, val_loss=0.083]\n",
      "Epoch 13:  65%|██████▌   | 68/104 [00:15<00:08,  4.35it/s, loss=0.141, v_num=0, reduced_train_loss=0.196, global_step=1042.0, val_loss=0.158]606]\n",
      "Epoch 14:   2%|▏         | 2/104 [00:00<00:25,  3.95it/s, loss=0.0869, v_num=0, reduced_train_loss=0.0929, global_step=1051.0, val_loss=0.083]06]\n",
      "Epoch 13: 100%|██████████| 104/104 [00:21<00:00,  4.74it/s, loss=0.108, v_num=0, reduced_train_loss=0.00187, global_step=1049.0, val_loss=0.0606]2023-05-03 22:00:53,418 - root - INFO - val_loss: 0.0606265589594841\n",
      "Epoch 13: 100%|██████████| 104/104 [00:21<00:00,  4.74it/s, loss=0.108, v_num=0, reduced_train_loss=0.00187, global_step=1049.0, val_loss=0.0606]\n",
      "Epoch 14:   0%|          | 0/104 [00:00<?, ?it/s, loss=0.108, v_num=0, reduced_train_loss=0.00187, global_step=1049.0, val_loss=0.0606]          "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 22:00:53.418156 140649257482048 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.0606265589594841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13:  72%|███████▏  | 75/104 [00:17<00:06,  4.36it/s, loss=0.15, v_num=0, reduced_train_loss=0.461, global_step=1049.0, val_loss=0.158]   6]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 14:   9%|▊         | 9/104 [00:02<00:22,  4.27it/s, loss=0.107, v_num=0, reduced_train_loss=0.0125, global_step=1058.0, val_loss=0.083]]   \n",
      "Epoch 13:  73%|███████▎  | 76/104 [00:17<00:06,  4.37it/s, loss=0.15, v_num=0, reduced_train_loss=0.461, global_step=1049.0, val_loss=0.158]\n",
      "Epoch 14:  10%|▉         | 10/104 [00:02<00:21,  4.29it/s, loss=0.105, v_num=0, reduced_train_loss=0.00193, global_step=1059.0, val_loss=0.083]\n",
      "Epoch 13:  75%|███████▌  | 78/104 [00:17<00:05,  4.42it/s, loss=0.15, v_num=0, reduced_train_loss=0.461, global_step=1049.0, val_loss=0.158]\n",
      "Epoch 14:   8%|▊         | 8/104 [00:02<00:24,  3.96it/s, loss=0.0601, v_num=0, reduced_train_loss=0.00483, global_step=1057.0, val_loss=0.0606]\n",
      "Epoch 14:  12%|█▏        | 12/104 [00:02<00:21,  4.31it/s, loss=0.115, v_num=0, reduced_train_loss=0.0302, global_step=1061.0, val_loss=0.083]\n",
      "Epoch 14:   9%|▊         | 9/104 [00:02<00:23,  3.97it/s, loss=0.0604, v_num=0, reduced_train_loss=0.00684, global_step=1058.0, val_loss=0.0606]\n",
      "Epoch 14:  12%|█▎        | 13/104 [00:03<00:21,  4.32it/s, loss=0.0956, v_num=0, reduced_train_loss=0.0125, global_step=1062.0, val_loss=0.083]\n",
      "Epoch 14:  10%|▉         | 10/104 [00:02<00:23,  3.97it/s, loss=0.0587, v_num=0, reduced_train_loss=0.0109, global_step=1059.0, val_loss=0.0606] \n",
      "Epoch 14:  13%|█▎        | 14/104 [00:03<00:20,  4.33it/s, loss=0.105, v_num=0, reduced_train_loss=0.212, global_step=1063.0, val_loss=0.083]  \n",
      "Epoch 14:  11%|█         | 11/104 [00:02<00:23,  3.97it/s, loss=0.0523, v_num=0, reduced_train_loss=0.0018, global_step=1060.0, val_loss=0.0606]\n",
      "Epoch 14:  14%|█▍        | 15/104 [00:03<00:20,  4.30it/s, loss=0.0934, v_num=0, reduced_train_loss=0.00458, global_step=1064.0, val_loss=0.083]\n",
      "Epoch 14:  12%|█▏        | 12/104 [00:03<00:23,  3.99it/s, loss=0.0527, v_num=0, reduced_train_loss=0.0665, global_step=1061.0, val_loss=0.0606]\n",
      "Epoch 14:  15%|█▌        | 16/104 [00:03<00:20,  4.31it/s, loss=0.0949, v_num=0, reduced_train_loss=0.0597, global_step=1065.0, val_loss=0.083] \n",
      "Epoch 14:  12%|█▎        | 13/104 [00:03<00:22,  4.02it/s, loss=0.053, v_num=0, reduced_train_loss=0.0506, global_step=1062.0, val_loss=0.0606] \n",
      "Epoch 14:  13%|█▎        | 14/104 [00:03<00:22,  4.05it/s, loss=0.053, v_num=0, reduced_train_loss=0.00157, global_step=1063.0, val_loss=0.0606]\n",
      "Epoch 13:  88%|████████▊ | 91/104 [00:19<00:02,  4.74it/s, loss=0.15, v_num=0, reduced_train_loss=0.461, global_step=1049.0, val_loss=0.158]\n",
      "Epoch 14:  14%|█▍        | 15/104 [00:03<00:21,  4.08it/s, loss=0.0423, v_num=0, reduced_train_loss=0.00554, global_step=1064.0, val_loss=0.0606]\n",
      "Epoch 13:  89%|████████▉ | 93/104 [00:19<00:02,  4.78it/s, loss=0.15, v_num=0, reduced_train_loss=0.461, global_step=1049.0, val_loss=0.158]\n",
      "Epoch 14:  15%|█▌        | 16/104 [00:03<00:21,  4.10it/s, loss=0.0233, v_num=0, reduced_train_loss=0.0101, global_step=1065.0, val_loss=0.0606] \n",
      "Epoch 14:  19%|█▉        | 20/104 [00:04<00:19,  4.34it/s, loss=0.0789, v_num=0, reduced_train_loss=0.00316, global_step=1069.0, val_loss=0.083]\n",
      "Epoch 14:  16%|█▋        | 17/104 [00:04<00:21,  4.12it/s, loss=0.0234, v_num=0, reduced_train_loss=0.00222, global_step=1066.0, val_loss=0.0606]\n",
      "Epoch 14:  20%|██        | 21/104 [00:04<00:19,  4.34it/s, loss=0.0911, v_num=0, reduced_train_loss=0.246, global_step=1070.0, val_loss=0.083]  \n",
      "Epoch 14:  17%|█▋        | 18/104 [00:04<00:20,  4.13it/s, loss=0.0315, v_num=0, reduced_train_loss=0.170, global_step=1067.0, val_loss=0.0606]  \n",
      "Epoch 14:  21%|██        | 22/104 [00:05<00:18,  4.35it/s, loss=0.0875, v_num=0, reduced_train_loss=0.0203, global_step=1071.0, val_loss=0.083]\n",
      "Epoch 14:  18%|█▊        | 19/104 [00:04<00:20,  4.15it/s, loss=0.0418, v_num=0, reduced_train_loss=0.209, global_step=1068.0, val_loss=0.0606]\n",
      "Epoch 14:  22%|██▏       | 23/104 [00:05<00:18,  4.35it/s, loss=0.0869, v_num=0, reduced_train_loss=0.00892, global_step=1072.0, val_loss=0.083]\n",
      "Epoch 14:  19%|█▉        | 20/104 [00:04<00:20,  4.16it/s, loss=0.0437, v_num=0, reduced_train_loss=0.0414, global_step=1069.0, val_loss=0.0606]\n",
      "Epoch 14:  23%|██▎       | 24/104 [00:05<00:18,  4.34it/s, loss=0.0867, v_num=0, reduced_train_loss=0.0025, global_step=1073.0, val_loss=0.083] \n",
      "Epoch 13: 100%|██████████| 104/104 [00:20<00:00,  5.02it/s, loss=0.15, v_num=0, reduced_train_loss=0.461, global_step=1049.0, val_loss=0.158]2023-05-03 22:00:58,406 - root - INFO - val_loss: 0.1434473693370819\n",
      "Epoch 13: 100%|██████████| 104/104 [00:20<00:00,  5.01it/s, loss=0.15, v_num=0, reduced_train_loss=0.461, global_step=1049.0, val_loss=0.143]\n",
      "Epoch 14:  20%|██        | 21/104 [00:05<00:19,  4.17it/s, loss=0.0473, v_num=0, reduced_train_loss=0.0849, global_step=1070.0, val_loss=0.0606]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 22:00:58.406319 139755592410944 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.1434473693370819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14:  72%|███████▏  | 75/104 [00:17<00:06,  4.40it/s, loss=0.0653, v_num=0, reduced_train_loss=0.0919, global_step=1124.0, val_loss=0.083]6] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 14:  49%|████▉     | 51/104 [00:11<00:12,  4.36it/s, loss=0.126, v_num=0, reduced_train_loss=0.160, global_step=1100.0, val_loss=0.143]0606]\n",
      "Epoch 14:  50%|█████     | 52/104 [00:11<00:11,  4.36it/s, loss=0.126, v_num=0, reduced_train_loss=0.160, global_step=1100.0, val_loss=0.143]6]   \n",
      "Epoch 14:  75%|███████▌  | 78/104 [00:17<00:05,  4.47it/s, loss=0.0653, v_num=0, reduced_train_loss=0.0919, global_step=1124.0, val_loss=0.083]\n",
      "Epoch 14:  72%|███████▏  | 75/104 [00:17<00:06,  4.38it/s, loss=0.0134, v_num=0, reduced_train_loss=0.0018, global_step=1124.0, val_loss=0.0606]\n",
      "Epoch 14:  51%|█████     | 53/104 [00:12<00:11,  4.36it/s, loss=0.117, v_num=0, reduced_train_loss=0.00431, global_step=1102.0, val_loss=0.143]\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 14:  77%|███████▋  | 80/104 [00:17<00:05,  4.52it/s, loss=0.0653, v_num=0, reduced_train_loss=0.0919, global_step=1124.0, val_loss=0.083]\n",
      "Epoch 14:  78%|███████▊  | 81/104 [00:17<00:05,  4.54it/s, loss=0.0653, v_num=0, reduced_train_loss=0.0919, global_step=1124.0, val_loss=0.083]\n",
      "Epoch 14:  52%|█████▏    | 54/104 [00:12<00:11,  4.37it/s, loss=0.136, v_num=0, reduced_train_loss=0.389, global_step=1103.0, val_loss=0.143]  ]\n",
      "Epoch 14:  79%|███████▉  | 82/104 [00:17<00:04,  4.57it/s, loss=0.0653, v_num=0, reduced_train_loss=0.0919, global_step=1124.0, val_loss=0.083]\n",
      "Epoch 14:  74%|███████▍  | 77/104 [00:17<00:06,  4.42it/s, loss=0.0134, v_num=0, reduced_train_loss=0.0018, global_step=1124.0, val_loss=0.0606]\n",
      "Epoch 14:  80%|███████▉  | 83/104 [00:18<00:04,  4.59it/s, loss=0.0653, v_num=0, reduced_train_loss=0.0919, global_step=1124.0, val_loss=0.083]\n",
      "Epoch 14:  53%|█████▎    | 55/104 [00:12<00:11,  4.37it/s, loss=0.153, v_num=0, reduced_train_loss=0.437, global_step=1104.0, val_loss=0.143]06]\n",
      "Epoch 14:  81%|████████  | 84/104 [00:18<00:04,  4.62it/s, loss=0.0653, v_num=0, reduced_train_loss=0.0919, global_step=1124.0, val_loss=0.083]\n",
      "Epoch 14:  76%|███████▌  | 79/104 [00:17<00:05,  4.47it/s, loss=0.0134, v_num=0, reduced_train_loss=0.0018, global_step=1124.0, val_loss=0.0606]\n",
      "Epoch 14:  82%|████████▏ | 85/104 [00:18<00:04,  4.64it/s, loss=0.0653, v_num=0, reduced_train_loss=0.0919, global_step=1124.0, val_loss=0.083]\n",
      "Epoch 14:  54%|█████▍    | 56/104 [00:12<00:10,  4.37it/s, loss=0.155, v_num=0, reduced_train_loss=0.249, global_step=1105.0, val_loss=0.143]06]\n",
      "Epoch 14:  83%|████████▎ | 86/104 [00:18<00:03,  4.67it/s, loss=0.0653, v_num=0, reduced_train_loss=0.0919, global_step=1124.0, val_loss=0.083]\n",
      "Epoch 14:  78%|███████▊  | 81/104 [00:17<00:05,  4.52it/s, loss=0.0134, v_num=0, reduced_train_loss=0.0018, global_step=1124.0, val_loss=0.0606]\n",
      "Epoch 14:  55%|█████▍    | 57/104 [00:13<00:10,  4.37it/s, loss=0.153, v_num=0, reduced_train_loss=0.00402, global_step=1106.0, val_loss=0.143]\n",
      "Epoch 14:  79%|███████▉  | 82/104 [00:18<00:04,  4.54it/s, loss=0.0134, v_num=0, reduced_train_loss=0.0018, global_step=1124.0, val_loss=0.0606]\n",
      "Epoch 14:  85%|████████▍ | 88/104 [00:18<00:03,  4.71it/s, loss=0.0653, v_num=0, reduced_train_loss=0.0919, global_step=1124.0, val_loss=0.083]\n",
      "Epoch 14:  80%|███████▉  | 83/104 [00:18<00:04,  4.57it/s, loss=0.0134, v_num=0, reduced_train_loss=0.0018, global_step=1124.0, val_loss=0.0606]\n",
      "Epoch 14:  56%|█████▌    | 58/104 [00:13<00:10,  4.37it/s, loss=0.165, v_num=0, reduced_train_loss=0.350, global_step=1107.0, val_loss=0.143]  \n",
      "Epoch 14:  81%|████████  | 84/104 [00:18<00:04,  4.59it/s, loss=0.0134, v_num=0, reduced_train_loss=0.0018, global_step=1124.0, val_loss=0.0606]\n",
      "Epoch 14:  87%|████████▋ | 90/104 [00:18<00:02,  4.76it/s, loss=0.0653, v_num=0, reduced_train_loss=0.0919, global_step=1124.0, val_loss=0.083]\n",
      "Epoch 14:  82%|████████▏ | 85/104 [00:18<00:04,  4.61it/s, loss=0.0134, v_num=0, reduced_train_loss=0.0018, global_step=1124.0, val_loss=0.0606]\n",
      "Epoch 14:  57%|█████▋    | 59/104 [00:13<00:10,  4.37it/s, loss=0.158, v_num=0, reduced_train_loss=0.0136, global_step=1108.0, val_loss=0.143]]\n",
      "Epoch 14:  83%|████████▎ | 86/104 [00:18<00:03,  4.64it/s, loss=0.0134, v_num=0, reduced_train_loss=0.0018, global_step=1124.0, val_loss=0.0606]\n",
      "Epoch 14:  88%|████████▊ | 92/104 [00:19<00:02,  4.80it/s, loss=0.0653, v_num=0, reduced_train_loss=0.0919, global_step=1124.0, val_loss=0.083]\n",
      "Epoch 14:  84%|████████▎ | 87/104 [00:18<00:03,  4.66it/s, loss=0.0134, v_num=0, reduced_train_loss=0.0018, global_step=1124.0, val_loss=0.0606]\n",
      "Epoch 14:  58%|█████▊    | 60/104 [00:13<00:10,  4.37it/s, loss=0.163, v_num=0, reduced_train_loss=0.195, global_step=1109.0, val_loss=0.143] ]\n",
      "Epoch 14:  85%|████████▍ | 88/104 [00:18<00:03,  4.68it/s, loss=0.0134, v_num=0, reduced_train_loss=0.0018, global_step=1124.0, val_loss=0.0606]\n",
      "Epoch 14:  90%|█████████ | 94/104 [00:19<00:02,  4.85it/s, loss=0.0653, v_num=0, reduced_train_loss=0.0919, global_step=1124.0, val_loss=0.083]\n",
      "Epoch 14:  59%|█████▊    | 61/104 [00:13<00:09,  4.37it/s, loss=0.163, v_num=0, reduced_train_loss=0.195, global_step=1109.0, val_loss=0.143]06]\n",
      "Epoch 14:  91%|█████████▏| 95/104 [00:19<00:01,  4.87it/s, loss=0.0653, v_num=0, reduced_train_loss=0.0919, global_step=1124.0, val_loss=0.083]\n",
      "Epoch 14:  87%|████████▋ | 90/104 [00:19<00:02,  4.72it/s, loss=0.0134, v_num=0, reduced_train_loss=0.0018, global_step=1124.0, val_loss=0.0606]\n",
      "Epoch 14:  92%|█████████▏| 96/104 [00:19<00:01,  4.89it/s, loss=0.0653, v_num=0, reduced_train_loss=0.0919, global_step=1124.0, val_loss=0.083]\n",
      "Epoch 14:  60%|█████▉    | 62/104 [00:14<00:09,  4.37it/s, loss=0.158, v_num=0, reduced_train_loss=0.204, global_step=1111.0, val_loss=0.143] 6]\n",
      "Epoch 14:  93%|█████████▎| 97/104 [00:19<00:01,  4.91it/s, loss=0.0653, v_num=0, reduced_train_loss=0.0919, global_step=1124.0, val_loss=0.083]\n",
      "Epoch 14:  88%|████████▊ | 92/104 [00:19<00:02,  4.77it/s, loss=0.0134, v_num=0, reduced_train_loss=0.0018, global_step=1124.0, val_loss=0.0606]\n",
      "Epoch 14:  61%|██████    | 63/104 [00:14<00:09,  4.37it/s, loss=0.152, v_num=0, reduced_train_loss=0.0514, global_step=1112.0, val_loss=0.143]]\n",
      "Epoch 14:  89%|████████▉ | 93/104 [00:19<00:02,  4.79it/s, loss=0.0134, v_num=0, reduced_train_loss=0.0018, global_step=1124.0, val_loss=0.0606]\n",
      "Epoch 14:  95%|█████████▌| 99/104 [00:19<00:01,  4.95it/s, loss=0.0653, v_num=0, reduced_train_loss=0.0919, global_step=1124.0, val_loss=0.083]\n",
      "Epoch 14:  96%|█████████▌| 100/104 [00:20<00:00,  4.98it/s, loss=0.0653, v_num=0, reduced_train_loss=0.0919, global_step=1124.0, val_loss=0.083]\n",
      "Epoch 14:  62%|██████▏   | 64/104 [00:14<00:09,  4.37it/s, loss=0.144, v_num=0, reduced_train_loss=0.137, global_step=1113.0, val_loss=0.143] 6]\n",
      "Epoch 14:  97%|█████████▋| 101/104 [00:20<00:00,  5.00it/s, loss=0.0653, v_num=0, reduced_train_loss=0.0919, global_step=1124.0, val_loss=0.083]\n",
      "Epoch 14:  91%|█████████▏| 95/104 [00:19<00:01,  4.83it/s, loss=0.0134, v_num=0, reduced_train_loss=0.0018, global_step=1124.0, val_loss=0.0606]\n",
      "Epoch 14:  98%|█████████▊| 102/104 [00:20<00:00,  5.02it/s, loss=0.0653, v_num=0, reduced_train_loss=0.0919, global_step=1124.0, val_loss=0.083]\n",
      "Epoch 14:  62%|██████▎   | 65/104 [00:14<00:08,  4.38it/s, loss=0.142, v_num=0, reduced_train_loss=0.0872, global_step=1114.0, val_loss=0.143]6]\n",
      "Epoch 14:  99%|█████████▉| 103/104 [00:20<00:00,  5.04it/s, loss=0.0653, v_num=0, reduced_train_loss=0.0919, global_step=1124.0, val_loss=0.083]\n",
      "Epoch 14:  93%|█████████▎| 97/104 [00:19<00:01,  4.87it/s, loss=0.0134, v_num=0, reduced_train_loss=0.0018, global_step=1124.0, val_loss=0.0606]\n",
      "Epoch 14: 100%|██████████| 104/104 [00:20<00:00,  5.07it/s, loss=0.0653, v_num=0, reduced_train_loss=0.0919, global_step=1124.0, val_loss=0.083]2023-05-03 22:01:13,383 - root - INFO - val_loss: 0.05828892067074776\n",
      "Epoch 14: 100%|██████████| 104/104 [00:20<00:00,  5.07it/s, loss=0.0653, v_num=0, reduced_train_loss=0.0919, global_step=1124.0, val_loss=0.0583]\n",
      "Epoch 15:   0%|          | 0/104 [00:00<?, ?it/s, loss=0.0653, v_num=0, reduced_train_loss=0.0919, global_step=1124.0, val_loss=0.0583]          \n",
      "Epoch 14:  94%|█████████▍| 98/104 [00:20<00:01,  4.89it/s, loss=0.0134, v_num=0, reduced_train_loss=0.0018, global_step=1124.0, val_loss=0.0606]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 22:01:13.383130 140522389899072 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.05828892067074776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14:  63%|██████▎   | 66/104 [00:15<00:08,  4.38it/s, loss=0.146, v_num=0, reduced_train_loss=0.160, global_step=1115.0, val_loss=0.143] \n",
      "Epoch 15:   1%|          | 1/104 [00:00<00:28,  3.64it/s, loss=0.0395, v_num=0, reduced_train_loss=0.0011, global_step=1125.0, val_loss=0.0583]]\n",
      "Epoch 14:  64%|██████▍   | 67/104 [00:15<00:08,  4.38it/s, loss=0.137, v_num=0, reduced_train_loss=0.0249, global_step=1116.0, val_loss=0.143]06]\n",
      "Epoch 14:  65%|██████▌   | 68/104 [00:15<00:08,  4.38it/s, loss=0.141, v_num=0, reduced_train_loss=0.0873, global_step=1117.0, val_loss=0.143]]6]\n",
      "Epoch 14:  98%|█████████▊| 102/104 [00:20<00:00,  4.97it/s, loss=0.0134, v_num=0, reduced_train_loss=0.0018, global_step=1124.0, val_loss=0.0606]\n",
      "Epoch 15:   3%|▎         | 3/104 [00:00<00:24,  4.14it/s, loss=0.0422, v_num=0, reduced_train_loss=0.00371, global_step=1127.0, val_loss=0.0583]]\n",
      "Epoch 14: 100%|██████████| 104/104 [00:20<00:00,  5.02it/s, loss=0.0134, v_num=0, reduced_train_loss=0.0018, global_step=1124.0, val_loss=0.0606]2023-05-03 22:01:14,142 - root - INFO - val_loss: 0.042552873492240906\n",
      "Epoch 14: 100%|██████████| 104/104 [00:20<00:00,  5.02it/s, loss=0.0134, v_num=0, reduced_train_loss=0.0018, global_step=1124.0, val_loss=0.0426]\n",
      "Epoch 15:   4%|▍         | 4/104 [00:00<00:23,  4.23it/s, loss=0.0424, v_num=0, reduced_train_loss=0.00823, global_step=1128.0, val_loss=0.0583] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 22:01:14.142695 140649257482048 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.042552873492240906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14:  72%|███████▏  | 75/104 [00:17<00:06,  4.39it/s, loss=0.121, v_num=0, reduced_train_loss=0.300, global_step=1124.0, val_loss=0.143]    \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 15:   6%|▌         | 6/104 [00:01<00:22,  4.32it/s, loss=0.0208, v_num=0, reduced_train_loss=0.00308, global_step=1130.0, val_loss=0.0426]\n",
      "Epoch 15:   7%|▋         | 7/104 [00:01<00:22,  4.33it/s, loss=0.0205, v_num=0, reduced_train_loss=0.000535, global_step=1131.0, val_loss=0.0426]\n",
      "Epoch 14:  74%|███████▍  | 77/104 [00:17<00:06,  4.43it/s, loss=0.121, v_num=0, reduced_train_loss=0.300, global_step=1124.0, val_loss=0.143]\n",
      "Epoch 15:   8%|▊         | 8/104 [00:01<00:22,  4.35it/s, loss=0.0178, v_num=0, reduced_train_loss=0.000187, global_step=1132.0, val_loss=0.0426]\n",
      "Epoch 14:  76%|███████▌  | 79/104 [00:17<00:05,  4.48it/s, loss=0.121, v_num=0, reduced_train_loss=0.300, global_step=1124.0, val_loss=0.143]\n",
      "Epoch 15:   9%|▊         | 9/104 [00:02<00:21,  4.36it/s, loss=0.0171, v_num=0, reduced_train_loss=0.000528, global_step=1133.0, val_loss=0.0426]\n",
      "Epoch 15:  12%|█▎        | 13/104 [00:03<00:21,  4.30it/s, loss=0.0309, v_num=0, reduced_train_loss=0.0702, global_step=1137.0, val_loss=0.0583] \n",
      "Epoch 15:  10%|▉         | 10/104 [00:02<00:21,  4.38it/s, loss=0.0171, v_num=0, reduced_train_loss=0.00285, global_step=1134.0, val_loss=0.0426] \n",
      "Epoch 15:  11%|█         | 11/104 [00:02<00:21,  4.39it/s, loss=0.0299, v_num=0, reduced_train_loss=0.256, global_step=1135.0, val_loss=0.0426]  \n",
      "Epoch 14:  81%|████████  | 84/104 [00:18<00:04,  4.60it/s, loss=0.121, v_num=0, reduced_train_loss=0.300, global_step=1124.0, val_loss=0.143]\n",
      "Epoch 15:  12%|█▏        | 12/104 [00:02<00:20,  4.40it/s, loss=0.0295, v_num=0, reduced_train_loss=0.00244, global_step=1136.0, val_loss=0.0426]\n",
      "Epoch 14:  83%|████████▎ | 86/104 [00:18<00:03,  4.64it/s, loss=0.121, v_num=0, reduced_train_loss=0.300, global_step=1124.0, val_loss=0.143]\n",
      "Epoch 15:  15%|█▌        | 16/104 [00:03<00:20,  4.31it/s, loss=0.0363, v_num=0, reduced_train_loss=0.0715, global_step=1140.0, val_loss=0.0583] \n",
      "Epoch 14:  85%|████████▍ | 88/104 [00:18<00:03,  4.69it/s, loss=0.121, v_num=0, reduced_train_loss=0.300, global_step=1124.0, val_loss=0.143]\n",
      "Epoch 15:  16%|█▋        | 17/104 [00:03<00:20,  4.31it/s, loss=0.0436, v_num=0, reduced_train_loss=0.153, global_step=1141.0, val_loss=0.0583] ]\n",
      "Epoch 14:  87%|████████▋ | 90/104 [00:19<00:02,  4.73it/s, loss=0.121, v_num=0, reduced_train_loss=0.300, global_step=1124.0, val_loss=0.143]\n",
      "Epoch 15:  17%|█▋        | 18/104 [00:04<00:19,  4.31it/s, loss=0.0431, v_num=0, reduced_train_loss=0.000421, global_step=1142.0, val_loss=0.0583]\n",
      "Epoch 15:  15%|█▌        | 16/104 [00:03<00:19,  4.41it/s, loss=0.0298, v_num=0, reduced_train_loss=0.00284, global_step=1140.0, val_loss=0.0426]\n",
      "Epoch 15:  18%|█▊        | 19/104 [00:04<00:19,  4.32it/s, loss=0.0354, v_num=0, reduced_train_loss=0.00437, global_step=1143.0, val_loss=0.0583] \n",
      "Epoch 15:  19%|█▉        | 20/104 [00:04<00:19,  4.32it/s, loss=0.0309, v_num=0, reduced_train_loss=0.00263, global_step=1144.0, val_loss=0.0583]\n",
      "Epoch 14:  91%|█████████▏| 95/104 [00:19<00:01,  4.84it/s, loss=0.121, v_num=0, reduced_train_loss=0.300, global_step=1124.0, val_loss=0.143]\n",
      "Epoch 15:  20%|██        | 21/104 [00:04<00:19,  4.32it/s, loss=0.031, v_num=0, reduced_train_loss=0.00286, global_step=1145.0, val_loss=0.0583] ]\n",
      "Epoch 14:  93%|█████████▎| 97/104 [00:19<00:01,  4.88it/s, loss=0.121, v_num=0, reduced_train_loss=0.300, global_step=1124.0, val_loss=0.143]\n",
      "Epoch 15:  21%|██        | 22/104 [00:05<00:18,  4.33it/s, loss=0.0287, v_num=0, reduced_train_loss=0.00945, global_step=1146.0, val_loss=0.0583] \n",
      "Epoch 14:  95%|█████████▌| 99/104 [00:20<00:01,  4.92it/s, loss=0.121, v_num=0, reduced_train_loss=0.300, global_step=1124.0, val_loss=0.143]\n",
      "Epoch 15:  22%|██▏       | 23/104 [00:05<00:18,  4.33it/s, loss=0.0286, v_num=0, reduced_train_loss=0.00152, global_step=1147.0, val_loss=0.0583]\n",
      "Epoch 15:  20%|██        | 21/104 [00:04<00:18,  4.42it/s, loss=0.0273, v_num=0, reduced_train_loss=0.014, global_step=1145.0, val_loss=0.0426]  \n",
      "Epoch 15:  23%|██▎       | 24/104 [00:05<00:18,  4.33it/s, loss=0.0283, v_num=0, reduced_train_loss=0.0021, global_step=1148.0, val_loss=0.0583] \n",
      "Epoch 14:  99%|█████████▉| 103/104 [00:20<00:00,  5.00it/s, loss=0.121, v_num=0, reduced_train_loss=0.300, global_step=1124.0, val_loss=0.143]\n",
      "Epoch 14: 100%|██████████| 104/104 [00:20<00:00,  5.03it/s, loss=0.121, v_num=0, reduced_train_loss=0.300, global_step=1124.0, val_loss=0.143]2023-05-03 22:01:19,088 - root - INFO - val_loss: 0.14755719900131226\n",
      "Epoch 14: 100%|██████████| 104/104 [00:20<00:00,  5.03it/s, loss=0.121, v_num=0, reduced_train_loss=0.300, global_step=1124.0, val_loss=0.148]\n",
      "Epoch 15:   0%|          | 0/104 [00:00<?, ?it/s, loss=0.121, v_num=0, reduced_train_loss=0.300, global_step=1124.0, val_loss=0.148]          "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 22:01:19.088216 139755592410944 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.14755719900131226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15:  72%|███████▏  | 75/104 [00:17<00:06,  4.41it/s, loss=0.055, v_num=0, reduced_train_loss=0.00802, global_step=1199.0, val_loss=0.0583]] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 15:  71%|███████   | 74/104 [00:16<00:06,  4.51it/s, loss=0.0186, v_num=0, reduced_train_loss=0.045, global_step=1198.0, val_loss=0.0426]  \n",
      "Epoch 15:  49%|████▉     | 51/104 [00:11<00:12,  4.39it/s, loss=0.133, v_num=0, reduced_train_loss=0.111, global_step=1175.0, val_loss=0.148]83]\n",
      "Epoch 15:  72%|███████▏  | 75/104 [00:16<00:06,  4.51it/s, loss=0.0185, v_num=0, reduced_train_loss=0.0015, global_step=1199.0, val_loss=0.0426]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 15:  50%|█████     | 52/104 [00:11<00:11,  4.39it/s, loss=0.129, v_num=0, reduced_train_loss=0.185, global_step=1176.0, val_loss=0.148]83]\n",
      "Epoch 15:  73%|███████▎  | 76/104 [00:16<00:06,  4.52it/s, loss=0.0185, v_num=0, reduced_train_loss=0.0015, global_step=1199.0, val_loss=0.0426]\n",
      "Epoch 15:  76%|███████▌  | 79/104 [00:17<00:05,  4.49it/s, loss=0.055, v_num=0, reduced_train_loss=0.00802, global_step=1199.0, val_loss=0.0583]\n",
      "Epoch 15:  74%|███████▍  | 77/104 [00:16<00:05,  4.55it/s, loss=0.0185, v_num=0, reduced_train_loss=0.0015, global_step=1199.0, val_loss=0.0426]\n",
      "Epoch 15:  51%|█████     | 53/104 [00:12<00:11,  4.39it/s, loss=0.123, v_num=0, reduced_train_loss=0.230, global_step=1177.0, val_loss=0.148]83]\n",
      "Epoch 15:  75%|███████▌  | 78/104 [00:17<00:05,  4.57it/s, loss=0.0185, v_num=0, reduced_train_loss=0.0015, global_step=1199.0, val_loss=0.0426]\n",
      "Epoch 15:  78%|███████▊  | 81/104 [00:17<00:05,  4.54it/s, loss=0.055, v_num=0, reduced_train_loss=0.00802, global_step=1199.0, val_loss=0.0583]\n",
      "Epoch 15:  76%|███████▌  | 79/104 [00:17<00:05,  4.60it/s, loss=0.0185, v_num=0, reduced_train_loss=0.0015, global_step=1199.0, val_loss=0.0426]\n",
      "Epoch 15:  52%|█████▏    | 54/104 [00:12<00:11,  4.39it/s, loss=0.127, v_num=0, reduced_train_loss=0.120, global_step=1178.0, val_loss=0.148]83]\n",
      "Epoch 15:  77%|███████▋  | 80/104 [00:17<00:05,  4.62it/s, loss=0.0185, v_num=0, reduced_train_loss=0.0015, global_step=1199.0, val_loss=0.0426]\n",
      "Epoch 15:  80%|███████▉  | 83/104 [00:18<00:04,  4.59it/s, loss=0.055, v_num=0, reduced_train_loss=0.00802, global_step=1199.0, val_loss=0.0583]\n",
      "Epoch 15:  78%|███████▊  | 81/104 [00:17<00:04,  4.65it/s, loss=0.0185, v_num=0, reduced_train_loss=0.0015, global_step=1199.0, val_loss=0.0426]\n",
      "Epoch 15:  53%|█████▎    | 55/104 [00:12<00:11,  4.39it/s, loss=0.125, v_num=0, reduced_train_loss=0.0156, global_step=1179.0, val_loss=0.148]3]\n",
      "Epoch 15:  79%|███████▉  | 82/104 [00:17<00:04,  4.67it/s, loss=0.0185, v_num=0, reduced_train_loss=0.0015, global_step=1199.0, val_loss=0.0426]\n",
      "Epoch 15:  82%|████████▏ | 85/104 [00:18<00:04,  4.64it/s, loss=0.055, v_num=0, reduced_train_loss=0.00802, global_step=1199.0, val_loss=0.0583]\n",
      "Epoch 15:  80%|███████▉  | 83/104 [00:17<00:04,  4.70it/s, loss=0.0185, v_num=0, reduced_train_loss=0.0015, global_step=1199.0, val_loss=0.0426]\n",
      "Epoch 15:  54%|█████▍    | 56/104 [00:12<00:10,  4.39it/s, loss=0.125, v_num=0, reduced_train_loss=0.00728, global_step=1180.0, val_loss=0.148]]\n",
      "Epoch 15:  81%|████████  | 84/104 [00:17<00:04,  4.72it/s, loss=0.0185, v_num=0, reduced_train_loss=0.0015, global_step=1199.0, val_loss=0.0426]\n",
      "Epoch 15:  84%|████████▎ | 87/104 [00:18<00:03,  4.68it/s, loss=0.055, v_num=0, reduced_train_loss=0.00802, global_step=1199.0, val_loss=0.0583]\n",
      "Epoch 15:  55%|█████▍    | 57/104 [00:12<00:10,  4.39it/s, loss=0.125, v_num=0, reduced_train_loss=0.00728, global_step=1180.0, val_loss=0.148]]\n",
      "Epoch 15:  85%|████████▍ | 88/104 [00:18<00:03,  4.71it/s, loss=0.055, v_num=0, reduced_train_loss=0.00802, global_step=1199.0, val_loss=0.0583]\n",
      "Epoch 15:  83%|████████▎ | 86/104 [00:18<00:03,  4.77it/s, loss=0.0185, v_num=0, reduced_train_loss=0.0015, global_step=1199.0, val_loss=0.0426]\n",
      "Epoch 15:  86%|████████▌ | 89/104 [00:18<00:03,  4.73it/s, loss=0.055, v_num=0, reduced_train_loss=0.00802, global_step=1199.0, val_loss=0.0583]\n",
      "Epoch 15:  56%|█████▌    | 58/104 [00:13<00:10,  4.39it/s, loss=0.126, v_num=0, reduced_train_loss=0.136, global_step=1182.0, val_loss=0.148]26]\n",
      "Epoch 15:  87%|████████▋ | 90/104 [00:18<00:02,  4.75it/s, loss=0.055, v_num=0, reduced_train_loss=0.00802, global_step=1199.0, val_loss=0.0583]\n",
      "Epoch 15:  85%|████████▍ | 88/104 [00:18<00:03,  4.82it/s, loss=0.0185, v_num=0, reduced_train_loss=0.0015, global_step=1199.0, val_loss=0.0426]\n",
      "Epoch 15:  88%|████████▊ | 91/104 [00:19<00:02,  4.77it/s, loss=0.055, v_num=0, reduced_train_loss=0.00802, global_step=1199.0, val_loss=0.0583]\n",
      "Epoch 15:  57%|█████▋    | 59/104 [00:13<00:10,  4.39it/s, loss=0.119, v_num=0, reduced_train_loss=0.029, global_step=1183.0, val_loss=0.148]26]\n",
      "Epoch 15:  88%|████████▊ | 92/104 [00:19<00:02,  4.79it/s, loss=0.055, v_num=0, reduced_train_loss=0.00802, global_step=1199.0, val_loss=0.0583]\n",
      "Epoch 15:  87%|████████▋ | 90/104 [00:18<00:02,  4.86it/s, loss=0.0185, v_num=0, reduced_train_loss=0.0015, global_step=1199.0, val_loss=0.0426]\n",
      "Epoch 15:  58%|█████▊    | 60/104 [00:13<00:10,  4.39it/s, loss=0.113, v_num=0, reduced_train_loss=0.108, global_step=1184.0, val_loss=0.148]83]\n",
      "Epoch 15:  88%|████████▊ | 91/104 [00:18<00:02,  4.88it/s, loss=0.0185, v_num=0, reduced_train_loss=0.0015, global_step=1199.0, val_loss=0.0426]\n",
      "Epoch 15:  90%|█████████ | 94/104 [00:19<00:02,  4.84it/s, loss=0.055, v_num=0, reduced_train_loss=0.00802, global_step=1199.0, val_loss=0.0583]\n",
      "Epoch 15:  88%|████████▊ | 92/104 [00:18<00:02,  4.91it/s, loss=0.0185, v_num=0, reduced_train_loss=0.0015, global_step=1199.0, val_loss=0.0426]\n",
      "Epoch 15:  59%|█████▊    | 61/104 [00:13<00:09,  4.39it/s, loss=0.108, v_num=0, reduced_train_loss=0.0448, global_step=1185.0, val_loss=0.148]3]\n",
      "Epoch 15:  89%|████████▉ | 93/104 [00:18<00:02,  4.93it/s, loss=0.0185, v_num=0, reduced_train_loss=0.0015, global_step=1199.0, val_loss=0.0426]\n",
      "Epoch 15:  92%|█████████▏| 96/104 [00:19<00:01,  4.88it/s, loss=0.055, v_num=0, reduced_train_loss=0.00802, global_step=1199.0, val_loss=0.0583]\n",
      "Epoch 15:  90%|█████████ | 94/104 [00:18<00:02,  4.95it/s, loss=0.0185, v_num=0, reduced_train_loss=0.0015, global_step=1199.0, val_loss=0.0426]\n",
      "Epoch 15:  60%|█████▉    | 62/104 [00:14<00:09,  4.39it/s, loss=0.0898, v_num=0, reduced_train_loss=0.00258, global_step=1186.0, val_loss=0.148]\n",
      "Epoch 15:  91%|█████████▏| 95/104 [00:19<00:01,  4.97it/s, loss=0.0185, v_num=0, reduced_train_loss=0.0015, global_step=1199.0, val_loss=0.0426]\n",
      "Epoch 15:  94%|█████████▍| 98/104 [00:19<00:01,  4.92it/s, loss=0.055, v_num=0, reduced_train_loss=0.00802, global_step=1199.0, val_loss=0.0583]\n",
      "Epoch 15:  92%|█████████▏| 96/104 [00:19<00:01,  4.99it/s, loss=0.0185, v_num=0, reduced_train_loss=0.0015, global_step=1199.0, val_loss=0.0426]\n",
      "Epoch 15:  61%|██████    | 63/104 [00:14<00:09,  4.39it/s, loss=0.0915, v_num=0, reduced_train_loss=0.0539, global_step=1187.0, val_loss=0.148] \n",
      "Epoch 15:  93%|█████████▎| 97/104 [00:19<00:01,  5.01it/s, loss=0.0185, v_num=0, reduced_train_loss=0.0015, global_step=1199.0, val_loss=0.0426]\n",
      "Epoch 15:  96%|█████████▌| 100/104 [00:20<00:00,  4.96it/s, loss=0.055, v_num=0, reduced_train_loss=0.00802, global_step=1199.0, val_loss=0.0583]\n",
      "Epoch 15:  62%|██████▏   | 64/104 [00:14<00:09,  4.39it/s, loss=0.0988, v_num=0, reduced_train_loss=0.188, global_step=1188.0, val_loss=0.148] ]\n",
      "Epoch 15:  97%|█████████▋| 101/104 [00:20<00:00,  4.98it/s, loss=0.055, v_num=0, reduced_train_loss=0.00802, global_step=1199.0, val_loss=0.0583]\n",
      "Epoch 15:  95%|█████████▌| 99/104 [00:19<00:00,  5.05it/s, loss=0.0185, v_num=0, reduced_train_loss=0.0015, global_step=1199.0, val_loss=0.0426]\n",
      "Epoch 15:  98%|█████████▊| 102/104 [00:20<00:00,  5.00it/s, loss=0.055, v_num=0, reduced_train_loss=0.00802, global_step=1199.0, val_loss=0.0583]\n",
      "Epoch 15:  62%|██████▎   | 65/104 [00:14<00:08,  4.39it/s, loss=0.104, v_num=0, reduced_train_loss=0.230, global_step=1189.0, val_loss=0.148] 26]\n",
      "Epoch 15:  99%|█████████▉| 103/104 [00:20<00:00,  5.02it/s, loss=0.055, v_num=0, reduced_train_loss=0.00802, global_step=1199.0, val_loss=0.0583]\n",
      "Epoch 15: 100%|██████████| 104/104 [00:20<00:00,  5.05it/s, loss=0.055, v_num=0, reduced_train_loss=0.00802, global_step=1199.0, val_loss=0.0583]2023-05-03 22:01:33,977 - root - INFO - val_loss: 0.07636048644781113\n",
      "Epoch 15: 100%|██████████| 104/104 [00:20<00:00,  5.05it/s, loss=0.055, v_num=0, reduced_train_loss=0.00802, global_step=1199.0, val_loss=0.0764]\n",
      "Epoch 16:   0%|          | 0/104 [00:00<?, ?it/s, loss=0.055, v_num=0, reduced_train_loss=0.00802, global_step=1199.0, val_loss=0.0764]          \n",
      "Epoch 15:  97%|█████████▋| 101/104 [00:19<00:00,  5.09it/s, loss=0.0185, v_num=0, reduced_train_loss=0.0015, global_step=1199.0, val_loss=0.0426]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 22:01:33.977322 140522389899072 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.07636048644781113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15:  63%|██████▎   | 66/104 [00:15<00:08,  4.39it/s, loss=0.132, v_num=0, reduced_train_loss=0.596, global_step=1190.0, val_loss=0.148]426]\n",
      "Epoch 16:   1%|          | 1/104 [00:00<00:28,  3.66it/s, loss=0.0578, v_num=0, reduced_train_loss=0.0603, global_step=1200.0, val_loss=0.0764]6]\n",
      "Epoch 15: 100%|██████████| 104/104 [00:20<00:00,  5.16it/s, loss=0.0185, v_num=0, reduced_train_loss=0.0015, global_step=1199.0, val_loss=0.0426]2023-05-03 22:01:34,289 - root - INFO - val_loss: 0.05835936963558197\n",
      "Epoch 15: 100%|██████████| 104/104 [00:20<00:00,  5.16it/s, loss=0.0185, v_num=0, reduced_train_loss=0.0015, global_step=1199.0, val_loss=0.0584]\n",
      "Epoch 16:   0%|          | 0/104 [00:00<?, ?it/s, loss=0.0185, v_num=0, reduced_train_loss=0.0015, global_step=1199.0, val_loss=0.0584]          "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 22:01:34.289492 140649257482048 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.05835936963558197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15:  72%|███████▏  | 75/104 [00:17<00:06,  4.40it/s, loss=0.118, v_num=0, reduced_train_loss=0.0754, global_step=1199.0, val_loss=0.148] 4]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 16:  10%|▉         | 10/104 [00:02<00:21,  4.42it/s, loss=0.0509, v_num=0, reduced_train_loss=0.0263, global_step=1209.0, val_loss=0.0764]  \n",
      "Epoch 16:   9%|▊         | 9/104 [00:02<00:21,  4.42it/s, loss=0.0396, v_num=0, reduced_train_loss=0.00135, global_step=1208.0, val_loss=0.0584]\n",
      "Epoch 16:  10%|▉         | 10/104 [00:02<00:21,  4.43it/s, loss=0.0396, v_num=0, reduced_train_loss=0.000368, global_step=1209.0, val_loss=0.0584]\n",
      "Epoch 15:  75%|███████▌  | 78/104 [00:17<00:05,  4.46it/s, loss=0.118, v_num=0, reduced_train_loss=0.0754, global_step=1199.0, val_loss=0.148]\n",
      "Epoch 16:  11%|█         | 11/104 [00:02<00:21,  4.43it/s, loss=0.039, v_num=0, reduced_train_loss=0.000559, global_step=1210.0, val_loss=0.0584] \n",
      "Epoch 16:  12%|█▎        | 13/104 [00:02<00:20,  4.44it/s, loss=0.0509, v_num=0, reduced_train_loss=0.00161, global_step=1212.0, val_loss=0.0764]\n",
      "Epoch 16:  12%|█▏        | 12/104 [00:02<00:20,  4.44it/s, loss=0.0455, v_num=0, reduced_train_loss=0.130, global_step=1211.0, val_loss=0.0584]  \n",
      "Epoch 16:  13%|█▎        | 14/104 [00:03<00:20,  4.45it/s, loss=0.0483, v_num=0, reduced_train_loss=0.000966, global_step=1213.0, val_loss=0.0764]\n",
      "Epoch 16:  12%|█▎        | 13/104 [00:02<00:20,  4.45it/s, loss=0.0376, v_num=0, reduced_train_loss=0.00173, global_step=1212.0, val_loss=0.0584]\n",
      "Epoch 16:  13%|█▎        | 14/104 [00:03<00:20,  4.46it/s, loss=0.0379, v_num=0, reduced_train_loss=0.00547, global_step=1213.0, val_loss=0.0584] \n",
      "Epoch 16:  15%|█▌        | 16/104 [00:03<00:19,  4.46it/s, loss=0.025, v_num=0, reduced_train_loss=0.00231, global_step=1215.0, val_loss=0.0764]\n",
      "Epoch 16:  14%|█▍        | 15/104 [00:03<00:19,  4.46it/s, loss=0.0456, v_num=0, reduced_train_loss=0.159, global_step=1214.0, val_loss=0.0584]  \n",
      "Epoch 16:  16%|█▋        | 17/104 [00:03<00:19,  4.46it/s, loss=0.0345, v_num=0, reduced_train_loss=0.212, global_step=1216.0, val_loss=0.0764] \n",
      "Epoch 16:  15%|█▌        | 16/104 [00:03<00:19,  4.47it/s, loss=0.0454, v_num=0, reduced_train_loss=0.00863, global_step=1215.0, val_loss=0.0584]\n",
      "Epoch 16:  17%|█▋        | 18/104 [00:04<00:19,  4.47it/s, loss=0.0344, v_num=0, reduced_train_loss=0.000443, global_step=1217.0, val_loss=0.0764]\n",
      "Epoch 16:  16%|█▋        | 17/104 [00:03<00:19,  4.48it/s, loss=0.0458, v_num=0, reduced_train_loss=0.00885, global_step=1216.0, val_loss=0.0584]\n",
      "Epoch 16:  17%|█▋        | 18/104 [00:04<00:19,  4.48it/s, loss=0.0562, v_num=0, reduced_train_loss=0.213, global_step=1217.0, val_loss=0.0584]   \n",
      "Epoch 15:  88%|████████▊ | 92/104 [00:19<00:02,  4.79it/s, loss=0.118, v_num=0, reduced_train_loss=0.0754, global_step=1199.0, val_loss=0.148]\n",
      "Epoch 16:  18%|█▊        | 19/104 [00:04<00:18,  4.49it/s, loss=0.07, v_num=0, reduced_train_loss=0.322, global_step=1218.0, val_loss=0.0584]  4]\n",
      "Epoch 16:  20%|██        | 21/104 [00:04<00:18,  4.48it/s, loss=0.0314, v_num=0, reduced_train_loss=0.0228, global_step=1220.0, val_loss=0.0764] \n",
      "Epoch 16:  19%|█▉        | 20/104 [00:04<00:18,  4.48it/s, loss=0.07, v_num=0, reduced_train_loss=0.00192, global_step=1219.0, val_loss=0.0584]\n",
      "Epoch 16:  21%|██        | 22/104 [00:04<00:18,  4.48it/s, loss=0.0164, v_num=0, reduced_train_loss=0.000569, global_step=1221.0, val_loss=0.0764]\n",
      "Epoch 16:  20%|██        | 21/104 [00:04<00:18,  4.48it/s, loss=0.0714, v_num=0, reduced_train_loss=0.0453, global_step=1220.0, val_loss=0.0584]\n",
      "Epoch 16:  22%|██▏       | 23/104 [00:05<00:18,  4.47it/s, loss=0.0169, v_num=0, reduced_train_loss=0.0129, global_step=1222.0, val_loss=0.0764]  \n",
      "Epoch 16:  21%|██        | 22/104 [00:04<00:18,  4.48it/s, loss=0.0697, v_num=0, reduced_train_loss=0.00319, global_step=1221.0, val_loss=0.0584]\n",
      "Epoch 16:  22%|██▏       | 23/104 [00:05<00:18,  4.49it/s, loss=0.0697, v_num=0, reduced_train_loss=0.000429, global_step=1222.0, val_loss=0.0584]\n",
      "Epoch 15:  97%|█████████▋| 101/104 [00:20<00:00,  4.96it/s, loss=0.118, v_num=0, reduced_train_loss=0.0754, global_step=1199.0, val_loss=0.148]\n",
      "Epoch 16:  23%|██▎       | 24/104 [00:05<00:17,  4.49it/s, loss=0.0709, v_num=0, reduced_train_loss=0.0257, global_step=1223.0, val_loss=0.0584]  \n",
      "Epoch 15:  99%|█████████▉| 103/104 [00:20<00:00,  5.00it/s, loss=0.118, v_num=0, reduced_train_loss=0.0754, global_step=1199.0, val_loss=0.148]\n",
      "Epoch 15: 100%|██████████| 104/104 [00:20<00:00,  5.03it/s, loss=0.118, v_num=0, reduced_train_loss=0.0754, global_step=1199.0, val_loss=0.148]2023-05-03 22:01:39,757 - root - INFO - val_loss: 0.14052347838878632\n",
      "Epoch 15: 100%|██████████| 104/104 [00:20<00:00,  5.03it/s, loss=0.118, v_num=0, reduced_train_loss=0.0754, global_step=1199.0, val_loss=0.141]\n",
      "Epoch 16:  25%|██▌       | 26/104 [00:05<00:17,  4.47it/s, loss=0.0204, v_num=0, reduced_train_loss=0.00152, global_step=1225.0, val_loss=0.0764]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 22:01:39.757402 139755592410944 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.14052347838878632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16:  72%|███████▏  | 75/104 [00:16<00:06,  4.51it/s, loss=0.1, v_num=0, reduced_train_loss=0.356, global_step=1274.0, val_loss=0.0764]      \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 16:  45%|████▌     | 47/104 [00:10<00:13,  4.31it/s, loss=0.0956, v_num=0, reduced_train_loss=0.112, global_step=1246.0, val_loss=0.141]84]\n",
      "Epoch 16:  72%|███████▏  | 75/104 [00:16<00:06,  4.53it/s, loss=0.0397, v_num=0, reduced_train_loss=0.000733, global_step=1274.0, val_loss=0.0584]\n",
      "Epoch 16:  46%|████▌     | 48/104 [00:11<00:12,  4.31it/s, loss=0.11, v_num=0, reduced_train_loss=0.305, global_step=1247.0, val_loss=0.141]  \n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 16:  74%|███████▍  | 77/104 [00:16<00:05,  4.54it/s, loss=0.1, v_num=0, reduced_train_loss=0.356, global_step=1274.0, val_loss=0.0764]\n",
      "Epoch 16:  73%|███████▎  | 76/104 [00:16<00:06,  4.54it/s, loss=0.0397, v_num=0, reduced_train_loss=0.000733, global_step=1274.0, val_loss=0.0584]\n",
      "Epoch 16:  47%|████▋     | 49/104 [00:11<00:12,  4.32it/s, loss=0.104, v_num=0, reduced_train_loss=0.0185, global_step=1248.0, val_loss=0.141]\n",
      "\n",
      "Epoch 16:  74%|███████▍  | 77/104 [00:16<00:05,  4.56it/s, loss=0.0397, v_num=0, reduced_train_loss=0.000733, global_step=1274.0, val_loss=0.0584]\n",
      "Epoch 16:  77%|███████▋  | 80/104 [00:17<00:05,  4.62it/s, loss=0.1, v_num=0, reduced_train_loss=0.356, global_step=1274.0, val_loss=0.0764]\n",
      "Epoch 16:  48%|████▊     | 50/104 [00:11<00:12,  4.32it/s, loss=0.106, v_num=0, reduced_train_loss=0.104, global_step=1249.0, val_loss=0.141] 584]\n",
      "Validation DataLoader 0:  21%|██        | 6/29 [00:00<00:02,  7.92it/s]\u001b[A\n",
      "Epoch 16:  76%|███████▌  | 79/104 [00:17<00:05,  4.62it/s, loss=0.0397, v_num=0, reduced_train_loss=0.000733, global_step=1274.0, val_loss=0.0584]\n",
      "\n",
      "Epoch 16:  49%|████▉     | 51/104 [00:11<00:12,  4.32it/s, loss=0.115, v_num=0, reduced_train_loss=0.322, global_step=1250.0, val_loss=0.141]0584]\n",
      "Epoch 16:  80%|███████▉  | 83/104 [00:17<00:04,  4.70it/s, loss=0.1, v_num=0, reduced_train_loss=0.356, global_step=1274.0, val_loss=0.0764]\n",
      "Epoch 16:  78%|███████▊  | 81/104 [00:17<00:04,  4.67it/s, loss=0.0397, v_num=0, reduced_train_loss=0.000733, global_step=1274.0, val_loss=0.0584]\n",
      "\n",
      "Epoch 16:  50%|█████     | 52/104 [00:12<00:12,  4.31it/s, loss=0.134, v_num=0, reduced_train_loss=0.373, global_step=1251.0, val_loss=0.141]0584]\n",
      "Epoch 16:  82%|████████▏ | 85/104 [00:17<00:04,  4.75it/s, loss=0.1, v_num=0, reduced_train_loss=0.356, global_step=1274.0, val_loss=0.0764]\n",
      "Epoch 16:  80%|███████▉  | 83/104 [00:17<00:04,  4.72it/s, loss=0.0397, v_num=0, reduced_train_loss=0.000733, global_step=1274.0, val_loss=0.0584]\n",
      "Validation DataLoader 0:  38%|███▊      | 11/29 [00:01<00:02,  8.03it/s]\u001b[A\n",
      "Epoch 16:  51%|█████     | 53/104 [00:12<00:11,  4.32it/s, loss=0.134, v_num=0, reduced_train_loss=0.0068, global_step=1252.0, val_loss=0.141]584]\n",
      "Epoch 16:  84%|████████▎ | 87/104 [00:18<00:03,  4.79it/s, loss=0.1, v_num=0, reduced_train_loss=0.356, global_step=1274.0, val_loss=0.0764]\n",
      "Epoch 16:  82%|████████▏ | 85/104 [00:17<00:03,  4.76it/s, loss=0.0397, v_num=0, reduced_train_loss=0.000733, global_step=1274.0, val_loss=0.0584]\n",
      "Validation DataLoader 0:  45%|████▍     | 13/29 [00:01<00:01,  8.07it/s]\u001b[A\n",
      "Epoch 16:  52%|█████▏    | 54/104 [00:12<00:11,  4.32it/s, loss=0.135, v_num=0, reduced_train_loss=0.0365, global_step=1253.0, val_loss=0.141]584]\n",
      "Epoch 16:  86%|████████▌ | 89/104 [00:18<00:03,  4.84it/s, loss=0.1, v_num=0, reduced_train_loss=0.356, global_step=1274.0, val_loss=0.0764]\n",
      "Epoch 16:  53%|█████▎    | 55/104 [00:12<00:11,  4.32it/s, loss=0.138, v_num=0, reduced_train_loss=0.0551, global_step=1254.0, val_loss=0.141]584]\n",
      "Epoch 16:  87%|████████▋ | 90/104 [00:18<00:02,  4.86it/s, loss=0.1, v_num=0, reduced_train_loss=0.356, global_step=1274.0, val_loss=0.0764]\n",
      "Epoch 16:  85%|████████▍ | 88/104 [00:18<00:03,  4.83it/s, loss=0.0397, v_num=0, reduced_train_loss=0.000733, global_step=1274.0, val_loss=0.0584]\n",
      "Epoch 16:  88%|████████▊ | 91/104 [00:18<00:02,  4.88it/s, loss=0.1, v_num=0, reduced_train_loss=0.356, global_step=1274.0, val_loss=0.0764]\n",
      "Epoch 16:  54%|█████▍    | 56/104 [00:12<00:11,  4.32it/s, loss=0.139, v_num=0, reduced_train_loss=0.0417, global_step=1255.0, val_loss=0.141]584]\n",
      "Epoch 16:  88%|████████▊ | 92/104 [00:18<00:02,  4.91it/s, loss=0.1, v_num=0, reduced_train_loss=0.356, global_step=1274.0, val_loss=0.0764]\n",
      "Epoch 16:  87%|████████▋ | 90/104 [00:18<00:02,  4.88it/s, loss=0.0397, v_num=0, reduced_train_loss=0.000733, global_step=1274.0, val_loss=0.0584]\n",
      "Epoch 16:  89%|████████▉ | 93/104 [00:18<00:02,  4.93it/s, loss=0.1, v_num=0, reduced_train_loss=0.356, global_step=1274.0, val_loss=0.0764]\n",
      "Epoch 16:  55%|█████▍    | 57/104 [00:13<00:10,  4.33it/s, loss=0.139, v_num=0, reduced_train_loss=0.00318, global_step=1256.0, val_loss=0.141]84]\n",
      "Epoch 16:  90%|█████████ | 94/104 [00:18<00:02,  4.95it/s, loss=0.1, v_num=0, reduced_train_loss=0.356, global_step=1274.0, val_loss=0.0764]\n",
      "Epoch 16:  88%|████████▊ | 92/104 [00:18<00:02,  4.92it/s, loss=0.0397, v_num=0, reduced_train_loss=0.000733, global_step=1274.0, val_loss=0.0584]\n",
      "Epoch 16:  91%|█████████▏| 95/104 [00:19<00:01,  4.97it/s, loss=0.1, v_num=0, reduced_train_loss=0.356, global_step=1274.0, val_loss=0.0764]\n",
      "Epoch 16:  56%|█████▌    | 58/104 [00:13<00:10,  4.33it/s, loss=0.148, v_num=0, reduced_train_loss=0.342, global_step=1257.0, val_loss=0.141]  84]\n",
      "Epoch 16:  92%|█████████▏| 96/104 [00:19<00:01,  4.99it/s, loss=0.1, v_num=0, reduced_train_loss=0.356, global_step=1274.0, val_loss=0.0764]\n",
      "Epoch 16:  90%|█████████ | 94/104 [00:18<00:02,  4.97it/s, loss=0.0397, v_num=0, reduced_train_loss=0.000733, global_step=1274.0, val_loss=0.0584]\n",
      "Epoch 16:  93%|█████████▎| 97/104 [00:19<00:01,  5.02it/s, loss=0.1, v_num=0, reduced_train_loss=0.356, global_step=1274.0, val_loss=0.0764]\n",
      "Epoch 16:  57%|█████▋    | 59/104 [00:13<00:10,  4.33it/s, loss=0.148, v_num=0, reduced_train_loss=0.0332, global_step=1258.0, val_loss=0.141]584]\n",
      "Epoch 16:  94%|█████████▍| 98/104 [00:19<00:01,  5.04it/s, loss=0.1, v_num=0, reduced_train_loss=0.356, global_step=1274.0, val_loss=0.0764]\n",
      "Epoch 16:  92%|█████████▏| 96/104 [00:19<00:01,  5.01it/s, loss=0.0397, v_num=0, reduced_train_loss=0.000733, global_step=1274.0, val_loss=0.0584]\n",
      "Epoch 16:  95%|█████████▌| 99/104 [00:19<00:00,  5.06it/s, loss=0.1, v_num=0, reduced_train_loss=0.356, global_step=1274.0, val_loss=0.0764]\n",
      "Epoch 16:  58%|█████▊    | 60/104 [00:13<00:10,  4.33it/s, loss=0.151, v_num=0, reduced_train_loss=0.0944, global_step=1259.0, val_loss=0.141]584]\n",
      "Epoch 16:  96%|█████████▌| 100/104 [00:19<00:00,  5.08it/s, loss=0.1, v_num=0, reduced_train_loss=0.356, global_step=1274.0, val_loss=0.0764]\n",
      "Epoch 16:  94%|█████████▍| 98/104 [00:19<00:01,  5.05it/s, loss=0.0397, v_num=0, reduced_train_loss=0.000733, global_step=1274.0, val_loss=0.0584]\n",
      "Epoch 16:  97%|█████████▋| 101/104 [00:19<00:00,  5.10it/s, loss=0.1, v_num=0, reduced_train_loss=0.356, global_step=1274.0, val_loss=0.0764]\n",
      "Epoch 16:  59%|█████▊    | 61/104 [00:14<00:09,  4.34it/s, loss=0.13, v_num=0, reduced_train_loss=0.0718, global_step=1260.0, val_loss=0.141] 584]\n",
      "Epoch 16:  98%|█████████▊| 102/104 [00:19<00:00,  5.12it/s, loss=0.1, v_num=0, reduced_train_loss=0.356, global_step=1274.0, val_loss=0.0764]\n",
      "Epoch 16:  96%|█████████▌| 100/104 [00:19<00:00,  5.09it/s, loss=0.0397, v_num=0, reduced_train_loss=0.000733, global_step=1274.0, val_loss=0.0584]\n",
      "Epoch 16:  60%|█████▉    | 62/104 [00:14<00:09,  4.34it/s, loss=0.121, v_num=0, reduced_train_loss=0.00556, global_step=1261.0, val_loss=0.141]\n",
      "Epoch 16:  97%|█████████▋| 101/104 [00:19<00:00,  5.11it/s, loss=0.0397, v_num=0, reduced_train_loss=0.000733, global_step=1274.0, val_loss=0.0584]\n",
      "Epoch 16: 100%|██████████| 104/104 [00:20<00:00,  5.17it/s, loss=0.1, v_num=0, reduced_train_loss=0.356, global_step=1274.0, val_loss=0.0764]2023-05-03 22:01:54,092 - root - INFO - val_loss: 0.1006506085395813\n",
      "Epoch 16: 100%|██████████| 104/104 [00:20<00:00,  5.17it/s, loss=0.1, v_num=0, reduced_train_loss=0.356, global_step=1274.0, val_loss=0.101] \n",
      "Epoch 17:   0%|          | 0/104 [00:00<?, ?it/s, loss=0.1, v_num=0, reduced_train_loss=0.356, global_step=1274.0, val_loss=0.101]          \n",
      "Epoch 16:  98%|█████████▊| 102/104 [00:19<00:00,  5.13it/s, loss=0.0397, v_num=0, reduced_train_loss=0.000733, global_step=1274.0, val_loss=0.0584]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 22:01:54.092256 140522389899072 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.1006506085395813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16:  61%|██████    | 63/104 [00:14<00:09,  4.34it/s, loss=0.131, v_num=0, reduced_train_loss=0.198, global_step=1262.0, val_loss=0.141]  \n",
      "Epoch 16:  99%|█████████▉| 103/104 [00:19<00:00,  5.15it/s, loss=0.0397, v_num=0, reduced_train_loss=0.000733, global_step=1274.0, val_loss=0.0584]\n",
      "Epoch 16: 100%|██████████| 104/104 [00:20<00:00,  5.19it/s, loss=0.0397, v_num=0, reduced_train_loss=0.000733, global_step=1274.0, val_loss=0.0584]2023-05-03 22:01:54,355 - root - INFO - val_loss: 0.05217134207487106\n",
      "Epoch 16: 100%|██████████| 104/104 [00:20<00:00,  5.18it/s, loss=0.0397, v_num=0, reduced_train_loss=0.000733, global_step=1274.0, val_loss=0.0522]\n",
      "Epoch 17:   1%|          | 1/104 [00:00<00:27,  3.78it/s, loss=0.1, v_num=0, reduced_train_loss=0.00291, global_step=1275.0, val_loss=0.101]       "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 22:01:54.355755 140649257482048 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.05217134207487106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16:  72%|███████▏  | 75/104 [00:17<00:06,  4.36it/s, loss=0.0955, v_num=0, reduced_train_loss=0.00445, global_step=1274.0, val_loss=0.141]] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 17:  12%|█▏        | 12/104 [00:02<00:20,  4.49it/s, loss=0.0258, v_num=0, reduced_train_loss=0.00912, global_step=1286.0, val_loss=0.0522]\n",
      "Epoch 17:  12%|█▎        | 13/104 [00:02<00:20,  4.50it/s, loss=0.026, v_num=0, reduced_train_loss=0.00341, global_step=1287.0, val_loss=0.0522] \n",
      "Epoch 16:  74%|███████▍  | 77/104 [00:17<00:06,  4.40it/s, loss=0.0955, v_num=0, reduced_train_loss=0.00445, global_step=1274.0, val_loss=0.141]\n",
      "Epoch 17:  13%|█▎        | 14/104 [00:03<00:20,  4.49it/s, loss=0.0252, v_num=0, reduced_train_loss=0.00189, global_step=1288.0, val_loss=0.0522]\n",
      "Epoch 16:  76%|███████▌  | 79/104 [00:17<00:05,  4.45it/s, loss=0.0955, v_num=0, reduced_train_loss=0.00445, global_step=1274.0, val_loss=0.141]\n",
      "Epoch 17:  14%|█▍        | 15/104 [00:03<00:19,  4.49it/s, loss=0.0389, v_num=0, reduced_train_loss=0.276, global_step=1289.0, val_loss=0.0522]  \n",
      "Epoch 16:  78%|███████▊  | 81/104 [00:18<00:05,  4.50it/s, loss=0.0955, v_num=0, reduced_train_loss=0.00445, global_step=1274.0, val_loss=0.141]\n",
      "Epoch 17:  15%|█▌        | 16/104 [00:03<00:19,  4.49it/s, loss=0.0355, v_num=0, reduced_train_loss=0.000455, global_step=1290.0, val_loss=0.0522]\n",
      "Epoch 17:  17%|█▋        | 18/104 [00:04<00:19,  4.48it/s, loss=0.0836, v_num=0, reduced_train_loss=0.00171, global_step=1292.0, val_loss=0.101]\n",
      "Epoch 17:  16%|█▋        | 17/104 [00:03<00:19,  4.50it/s, loss=0.0356, v_num=0, reduced_train_loss=0.00325, global_step=1291.0, val_loss=0.0522] \n",
      "Epoch 17:  17%|█▋        | 18/104 [00:03<00:19,  4.51it/s, loss=0.0346, v_num=0, reduced_train_loss=0.015, global_step=1292.0, val_loss=0.0522]  \n",
      "Epoch 16:  83%|████████▎ | 86/104 [00:18<00:03,  4.62it/s, loss=0.0955, v_num=0, reduced_train_loss=0.00445, global_step=1274.0, val_loss=0.141]\n",
      "Epoch 17:  18%|█▊        | 19/104 [00:04<00:18,  4.51it/s, loss=0.0346, v_num=0, reduced_train_loss=0.00174, global_step=1293.0, val_loss=0.0522]\n",
      "Epoch 16:  85%|████████▍ | 88/104 [00:18<00:03,  4.66it/s, loss=0.0955, v_num=0, reduced_train_loss=0.00445, global_step=1274.0, val_loss=0.141]\n",
      "Epoch 17:  19%|█▉        | 20/104 [00:04<00:18,  4.51it/s, loss=0.0405, v_num=0, reduced_train_loss=0.119, global_step=1294.0, val_loss=0.0522]  \n",
      "Epoch 16:  87%|████████▋ | 90/104 [00:19<00:02,  4.71it/s, loss=0.0955, v_num=0, reduced_train_loss=0.00445, global_step=1274.0, val_loss=0.141]\n",
      "Epoch 17:  20%|██        | 21/104 [00:04<00:18,  4.51it/s, loss=0.0406, v_num=0, reduced_train_loss=0.00124, global_step=1295.0, val_loss=0.0522]\n",
      "Epoch 17:  22%|██▏       | 23/104 [00:05<00:18,  4.49it/s, loss=0.0562, v_num=0, reduced_train_loss=0.023, global_step=1297.0, val_loss=0.101] ]\n",
      "Epoch 17:  21%|██        | 22/104 [00:04<00:18,  4.51it/s, loss=0.0406, v_num=0, reduced_train_loss=0.000864, global_step=1296.0, val_loss=0.0522]\n",
      "Epoch 17:  22%|██▏       | 23/104 [00:05<00:17,  4.52it/s, loss=0.0411, v_num=0, reduced_train_loss=0.0778, global_step=1297.0, val_loss=0.0522]  \n",
      "Epoch 16:  91%|█████████▏| 95/104 [00:19<00:01,  4.82it/s, loss=0.0955, v_num=0, reduced_train_loss=0.00445, global_step=1274.0, val_loss=0.141]\n",
      "Epoch 17:  23%|██▎       | 24/104 [00:05<00:17,  4.51it/s, loss=0.0463, v_num=0, reduced_train_loss=0.120, global_step=1298.0, val_loss=0.0522] \n",
      "Epoch 16:  93%|█████████▎| 97/104 [00:19<00:01,  4.86it/s, loss=0.0955, v_num=0, reduced_train_loss=0.00445, global_step=1274.0, val_loss=0.141]\n",
      "Epoch 17:  24%|██▍       | 25/104 [00:05<00:17,  4.52it/s, loss=0.0464, v_num=0, reduced_train_loss=0.00169, global_step=1299.0, val_loss=0.0522]\n",
      "Epoch 16:  95%|█████████▌| 99/104 [00:20<00:01,  4.90it/s, loss=0.0955, v_num=0, reduced_train_loss=0.00445, global_step=1274.0, val_loss=0.141]\n",
      "Epoch 17:  26%|██▌       | 27/104 [00:06<00:17,  4.49it/s, loss=0.0452, v_num=0, reduced_train_loss=0.245, global_step=1301.0, val_loss=0.101]   \n",
      "Epoch 16:  97%|█████████▋| 101/104 [00:20<00:00,  4.94it/s, loss=0.0955, v_num=0, reduced_train_loss=0.00445, global_step=1274.0, val_loss=0.141]\n",
      "Epoch 17:  27%|██▋       | 28/104 [00:06<00:16,  4.49it/s, loss=0.0441, v_num=0, reduced_train_loss=0.00227, global_step=1302.0, val_loss=0.101] \n",
      "Epoch 16:  99%|█████████▉| 103/104 [00:20<00:00,  4.98it/s, loss=0.0955, v_num=0, reduced_train_loss=0.00445, global_step=1274.0, val_loss=0.141]\n",
      "Epoch 16: 100%|██████████| 104/104 [00:20<00:00,  5.01it/s, loss=0.0955, v_num=0, reduced_train_loss=0.00445, global_step=1274.0, val_loss=0.141]2023-05-03 22:02:00,519 - root - INFO - val_loss: 0.16364814341068268\n",
      "Epoch 16: 100%|██████████| 104/104 [00:20<00:00,  5.01it/s, loss=0.0955, v_num=0, reduced_train_loss=0.00445, global_step=1274.0, val_loss=0.164]\n",
      "Epoch 17:   0%|          | 0/104 [00:00<?, ?it/s, loss=0.0955, v_num=0, reduced_train_loss=0.00445, global_step=1274.0, val_loss=0.164]          "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 22:02:00.519162 139755592410944 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.16364814341068268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17:  72%|███████▏  | 75/104 [00:16<00:06,  4.52it/s, loss=0.0312, v_num=0, reduced_train_loss=0.245, global_step=1349.0, val_loss=0.101]2]  \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 17:  43%|████▎     | 45/104 [00:10<00:13,  4.38it/s, loss=0.076, v_num=0, reduced_train_loss=0.0952, global_step=1319.0, val_loss=0.164] \n",
      "Epoch 17:  72%|███████▏  | 75/104 [00:16<00:06,  4.54it/s, loss=0.0217, v_num=0, reduced_train_loss=0.00551, global_step=1349.0, val_loss=0.0522]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 17:  44%|████▍     | 46/104 [00:10<00:13,  4.38it/s, loss=0.0593, v_num=0, reduced_train_loss=0.0161, global_step=1320.0, val_loss=0.164]\n",
      "Epoch 17:  73%|███████▎  | 76/104 [00:16<00:06,  4.55it/s, loss=0.0217, v_num=0, reduced_train_loss=0.00551, global_step=1349.0, val_loss=0.0522]\n",
      "Epoch 17:  75%|███████▌  | 78/104 [00:16<00:05,  4.59it/s, loss=0.0312, v_num=0, reduced_train_loss=0.245, global_step=1349.0, val_loss=0.101]\n",
      "Epoch 17:  74%|███████▍  | 77/104 [00:16<00:05,  4.58it/s, loss=0.0217, v_num=0, reduced_train_loss=0.00551, global_step=1349.0, val_loss=0.0522]\n",
      "Epoch 17:  45%|████▌     | 47/104 [00:10<00:13,  4.38it/s, loss=0.057, v_num=0, reduced_train_loss=0.00173, global_step=1321.0, val_loss=0.164]\n",
      "Epoch 17:  75%|███████▌  | 78/104 [00:16<00:05,  4.60it/s, loss=0.0217, v_num=0, reduced_train_loss=0.00551, global_step=1349.0, val_loss=0.0522]\n",
      "Epoch 17:  77%|███████▋  | 80/104 [00:17<00:05,  4.64it/s, loss=0.0312, v_num=0, reduced_train_loss=0.245, global_step=1349.0, val_loss=0.101]\n",
      "Epoch 17:  76%|███████▌  | 79/104 [00:17<00:05,  4.63it/s, loss=0.0217, v_num=0, reduced_train_loss=0.00551, global_step=1349.0, val_loss=0.0522]\n",
      "Epoch 17:  46%|████▌     | 48/104 [00:10<00:12,  4.38it/s, loss=0.066, v_num=0, reduced_train_loss=0.213, global_step=1322.0, val_loss=0.164]  \n",
      "Epoch 17:  77%|███████▋  | 80/104 [00:17<00:05,  4.66it/s, loss=0.0217, v_num=0, reduced_train_loss=0.00551, global_step=1349.0, val_loss=0.0522]\n",
      "Epoch 17:  79%|███████▉  | 82/104 [00:17<00:04,  4.69it/s, loss=0.0312, v_num=0, reduced_train_loss=0.245, global_step=1349.0, val_loss=0.101]\n",
      "Epoch 17:  78%|███████▊  | 81/104 [00:17<00:04,  4.68it/s, loss=0.0217, v_num=0, reduced_train_loss=0.00551, global_step=1349.0, val_loss=0.0522]\n",
      "Epoch 17:  47%|████▋     | 49/104 [00:11<00:12,  4.38it/s, loss=0.0751, v_num=0, reduced_train_loss=0.237, global_step=1323.0, val_loss=0.164]\n",
      "Epoch 17:  79%|███████▉  | 82/104 [00:17<00:04,  4.70it/s, loss=0.0217, v_num=0, reduced_train_loss=0.00551, global_step=1349.0, val_loss=0.0522]\n",
      "Epoch 17:  81%|████████  | 84/104 [00:17<00:04,  4.74it/s, loss=0.0312, v_num=0, reduced_train_loss=0.245, global_step=1349.0, val_loss=0.101]\n",
      "Epoch 17:  48%|████▊     | 50/104 [00:11<00:12,  4.38it/s, loss=0.0727, v_num=0, reduced_train_loss=0.00616, global_step=1324.0, val_loss=0.164]]\n",
      "Epoch 17:  82%|████████▏ | 85/104 [00:17<00:03,  4.76it/s, loss=0.0312, v_num=0, reduced_train_loss=0.245, global_step=1349.0, val_loss=0.101]\n",
      "Epoch 17:  81%|████████  | 84/104 [00:17<00:04,  4.75it/s, loss=0.0217, v_num=0, reduced_train_loss=0.00551, global_step=1349.0, val_loss=0.0522]\n",
      "Epoch 17:  49%|████▉     | 51/104 [00:11<00:12,  4.38it/s, loss=0.0782, v_num=0, reduced_train_loss=0.115, global_step=1325.0, val_loss=0.164]  \n",
      "Epoch 17:  82%|████████▏ | 85/104 [00:17<00:03,  4.77it/s, loss=0.0217, v_num=0, reduced_train_loss=0.00551, global_step=1349.0, val_loss=0.0522]\n",
      "Epoch 17:  84%|████████▎ | 87/104 [00:18<00:03,  4.81it/s, loss=0.0312, v_num=0, reduced_train_loss=0.245, global_step=1349.0, val_loss=0.101]\n",
      "Epoch 17:  83%|████████▎ | 86/104 [00:17<00:03,  4.79it/s, loss=0.0217, v_num=0, reduced_train_loss=0.00551, global_step=1349.0, val_loss=0.0522]\n",
      "Epoch 17:  50%|█████     | 52/104 [00:11<00:11,  4.38it/s, loss=0.106, v_num=0, reduced_train_loss=0.569, global_step=1326.0, val_loss=0.164] \n",
      "\n",
      "Epoch 17:  84%|████████▎ | 87/104 [00:18<00:03,  4.82it/s, loss=0.0217, v_num=0, reduced_train_loss=0.00551, global_step=1349.0, val_loss=0.0522]\n",
      "Epoch 17:  87%|████████▋ | 90/104 [00:18<00:02,  4.88it/s, loss=0.0312, v_num=0, reduced_train_loss=0.245, global_step=1349.0, val_loss=0.101]\n",
      "Epoch 17:  51%|█████     | 53/104 [00:12<00:11,  4.38it/s, loss=0.102, v_num=0, reduced_train_loss=0.00245, global_step=1327.0, val_loss=0.164]2]\n",
      "Epoch 17:  88%|████████▊ | 91/104 [00:18<00:02,  4.90it/s, loss=0.0312, v_num=0, reduced_train_loss=0.245, global_step=1349.0, val_loss=0.101]\n",
      "Epoch 17:  86%|████████▌ | 89/104 [00:18<00:03,  4.86it/s, loss=0.0217, v_num=0, reduced_train_loss=0.00551, global_step=1349.0, val_loss=0.0522]\n",
      "Epoch 17:  88%|████████▊ | 92/104 [00:18<00:02,  4.92it/s, loss=0.0312, v_num=0, reduced_train_loss=0.245, global_step=1349.0, val_loss=0.101]\n",
      "Epoch 17:  52%|█████▏    | 54/104 [00:12<00:11,  4.38it/s, loss=0.124, v_num=0, reduced_train_loss=0.462, global_step=1328.0, val_loss=0.164]  2]\n",
      "Epoch 17:  89%|████████▉ | 93/104 [00:18<00:02,  4.95it/s, loss=0.0312, v_num=0, reduced_train_loss=0.245, global_step=1349.0, val_loss=0.101]\n",
      "Epoch 17:  88%|████████▊ | 91/104 [00:18<00:02,  4.90it/s, loss=0.0217, v_num=0, reduced_train_loss=0.00551, global_step=1349.0, val_loss=0.0522]\n",
      "Epoch 17:  90%|█████████ | 94/104 [00:18<00:02,  4.97it/s, loss=0.0312, v_num=0, reduced_train_loss=0.245, global_step=1349.0, val_loss=0.101]\n",
      "Epoch 17:  53%|█████▎    | 55/104 [00:12<00:11,  4.38it/s, loss=0.122, v_num=0, reduced_train_loss=0.00584, global_step=1329.0, val_loss=0.164]2]\n",
      "Epoch 17:  91%|█████████▏| 95/104 [00:19<00:01,  4.99it/s, loss=0.0312, v_num=0, reduced_train_loss=0.245, global_step=1349.0, val_loss=0.101]\n",
      "Epoch 17:  89%|████████▉ | 93/104 [00:18<00:02,  4.94it/s, loss=0.0217, v_num=0, reduced_train_loss=0.00551, global_step=1349.0, val_loss=0.0522]\n",
      "Epoch 17:  92%|█████████▏| 96/104 [00:19<00:01,  5.01it/s, loss=0.0312, v_num=0, reduced_train_loss=0.245, global_step=1349.0, val_loss=0.101]\n",
      "Epoch 17:  54%|█████▍    | 56/104 [00:12<00:10,  4.38it/s, loss=0.121, v_num=0, reduced_train_loss=0.0598, global_step=1330.0, val_loss=0.164] 2]\n",
      "Epoch 17:  93%|█████████▎| 97/104 [00:19<00:01,  5.03it/s, loss=0.0312, v_num=0, reduced_train_loss=0.245, global_step=1349.0, val_loss=0.101]\n",
      "Epoch 17:  91%|█████████▏| 95/104 [00:19<00:01,  4.98it/s, loss=0.0217, v_num=0, reduced_train_loss=0.00551, global_step=1349.0, val_loss=0.0522]\n",
      "Epoch 17:  55%|█████▍    | 57/104 [00:13<00:10,  4.38it/s, loss=0.128, v_num=0, reduced_train_loss=0.327, global_step=1331.0, val_loss=0.164] \n",
      "Epoch 17:  92%|█████████▏| 96/104 [00:19<00:01,  5.00it/s, loss=0.0217, v_num=0, reduced_train_loss=0.00551, global_step=1349.0, val_loss=0.0522]\n",
      "Epoch 17:  95%|█████████▌| 99/104 [00:19<00:00,  5.07it/s, loss=0.0312, v_num=0, reduced_train_loss=0.245, global_step=1349.0, val_loss=0.101]\n",
      "Epoch 17:  93%|█████████▎| 97/104 [00:19<00:01,  5.02it/s, loss=0.0217, v_num=0, reduced_train_loss=0.00551, global_step=1349.0, val_loss=0.0522]\n",
      "Epoch 17:  56%|█████▌    | 58/104 [00:13<00:10,  4.38it/s, loss=0.132, v_num=0, reduced_train_loss=0.109, global_step=1332.0, val_loss=0.164]1]\n",
      "Epoch 17:  94%|█████████▍| 98/104 [00:19<00:01,  5.04it/s, loss=0.0217, v_num=0, reduced_train_loss=0.00551, global_step=1349.0, val_loss=0.0522]\n",
      "Epoch 17:  97%|█████████▋| 101/104 [00:19<00:00,  5.11it/s, loss=0.0312, v_num=0, reduced_train_loss=0.245, global_step=1349.0, val_loss=0.101]\n",
      "Epoch 17:  95%|█████████▌| 99/104 [00:19<00:00,  5.06it/s, loss=0.0217, v_num=0, reduced_train_loss=0.00551, global_step=1349.0, val_loss=0.0522]\n",
      "Epoch 17:  57%|█████▋    | 59/104 [00:13<00:10,  4.38it/s, loss=0.129, v_num=0, reduced_train_loss=0.0217, global_step=1333.0, val_loss=0.164]]\n",
      "Epoch 17:  96%|█████████▌| 100/104 [00:19<00:00,  5.08it/s, loss=0.0217, v_num=0, reduced_train_loss=0.00551, global_step=1349.0, val_loss=0.0522]\n",
      "Epoch 17:  99%|█████████▉| 103/104 [00:19<00:00,  5.15it/s, loss=0.0312, v_num=0, reduced_train_loss=0.245, global_step=1349.0, val_loss=0.101]\n",
      "Epoch 17: 100%|██████████| 104/104 [00:20<00:00,  5.19it/s, loss=0.0312, v_num=0, reduced_train_loss=0.245, global_step=1349.0, val_loss=0.101]2023-05-03 22:02:14,152 - root - INFO - val_loss: 0.10755656659603119\n",
      "Epoch 17: 100%|██████████| 104/104 [00:20<00:00,  5.19it/s, loss=0.0312, v_num=0, reduced_train_loss=0.245, global_step=1349.0, val_loss=0.108]\n",
      "Epoch 18:   0%|          | 0/104 [00:00<?, ?it/s, loss=0.0312, v_num=0, reduced_train_loss=0.245, global_step=1349.0, val_loss=0.108]          "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 22:02:14.152065 140522389899072 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.10755656659603119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17:  58%|█████▊    | 60/104 [00:13<00:10,  4.39it/s, loss=0.132, v_num=0, reduced_train_loss=0.0855, global_step=1334.0, val_loss=0.164]522]\n",
      "Epoch 17:  98%|█████████▊| 102/104 [00:19<00:00,  5.12it/s, loss=0.0217, v_num=0, reduced_train_loss=0.00551, global_step=1349.0, val_loss=0.0522]\n",
      "Epoch 17:  59%|█████▊    | 61/104 [00:13<00:09,  4.39it/s, loss=0.134, v_num=0, reduced_train_loss=0.166, global_step=1335.0, val_loss=0.164] ]22]\n",
      "Epoch 17: 100%|██████████| 104/104 [00:20<00:00,  5.17it/s, loss=0.0217, v_num=0, reduced_train_loss=0.00551, global_step=1349.0, val_loss=0.0522]2023-05-03 22:02:14,486 - root - INFO - val_loss: 0.04125262051820755\n",
      "Epoch 17: 100%|██████████| 104/104 [00:20<00:00,  5.17it/s, loss=0.0217, v_num=0, reduced_train_loss=0.00551, global_step=1349.0, val_loss=0.0413]\n",
      "Epoch 18:   0%|          | 0/104 [00:00<?, ?it/s, loss=0.0217, v_num=0, reduced_train_loss=0.00551, global_step=1349.0, val_loss=0.0413]          "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 22:02:14.486965 140649257482048 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.04125262051820755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17:  72%|███████▏  | 75/104 [00:17<00:06,  4.40it/s, loss=0.106, v_num=0, reduced_train_loss=0.0595, global_step=1349.0, val_loss=0.164]8]3]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 18:  15%|█▌        | 16/104 [00:03<00:19,  4.44it/s, loss=0.0306, v_num=0, reduced_train_loss=0.0776, global_step=1365.0, val_loss=0.108] 3]\n",
      "Epoch 18:  14%|█▍        | 15/104 [00:03<00:20,  4.30it/s, loss=0.0146, v_num=0, reduced_train_loss=0.003, global_step=1364.0, val_loss=0.0413]   \n",
      "Epoch 18:  16%|█▋        | 17/104 [00:03<00:19,  4.44it/s, loss=0.0318, v_num=0, reduced_train_loss=0.025, global_step=1366.0, val_loss=0.108] \n",
      "Epoch 18:  17%|█▋        | 18/104 [00:04<00:19,  4.44it/s, loss=0.032, v_num=0, reduced_train_loss=0.00555, global_step=1367.0, val_loss=0.108]13]\n",
      "Epoch 17:  77%|███████▋  | 80/104 [00:17<00:05,  4.52it/s, loss=0.106, v_num=0, reduced_train_loss=0.0595, global_step=1349.0, val_loss=0.164]\n",
      "Epoch 18:  16%|█▋        | 17/104 [00:03<00:20,  4.31it/s, loss=0.0146, v_num=0, reduced_train_loss=0.0105, global_step=1366.0, val_loss=0.0413]  \n",
      "Epoch 17:  79%|███████▉  | 82/104 [00:17<00:04,  4.57it/s, loss=0.106, v_num=0, reduced_train_loss=0.0595, global_step=1349.0, val_loss=0.164]\n",
      "Epoch 18:  17%|█▋        | 18/104 [00:04<00:19,  4.30it/s, loss=0.0143, v_num=0, reduced_train_loss=0.0303, global_step=1367.0, val_loss=0.0413] \n",
      "Epoch 17:  81%|████████  | 84/104 [00:18<00:04,  4.61it/s, loss=0.106, v_num=0, reduced_train_loss=0.0595, global_step=1349.0, val_loss=0.164]\n",
      "Epoch 18:  18%|█▊        | 19/104 [00:04<00:19,  4.31it/s, loss=0.0137, v_num=0, reduced_train_loss=0.000212, global_step=1368.0, val_loss=0.0413]\n",
      "Epoch 17:  83%|████████▎ | 86/104 [00:18<00:03,  4.66it/s, loss=0.106, v_num=0, reduced_train_loss=0.0595, global_step=1349.0, val_loss=0.164]\n",
      "Epoch 18:  19%|█▉        | 20/104 [00:04<00:19,  4.31it/s, loss=0.0156, v_num=0, reduced_train_loss=0.043, global_step=1369.0, val_loss=0.0413]   \n",
      "Epoch 18:  22%|██▏       | 23/104 [00:05<00:18,  4.45it/s, loss=0.0258, v_num=0, reduced_train_loss=0.118, global_step=1372.0, val_loss=0.108]\n",
      "Epoch 18:  20%|██        | 21/104 [00:04<00:19,  4.31it/s, loss=0.0167, v_num=0, reduced_train_loss=0.0248, global_step=1370.0, val_loss=0.0413]\n",
      "Epoch 18:  21%|██        | 22/104 [00:05<00:18,  4.32it/s, loss=0.0199, v_num=0, reduced_train_loss=0.0674, global_step=1371.0, val_loss=0.0413]\n",
      "Epoch 17:  88%|████████▊ | 91/104 [00:19<00:02,  4.77it/s, loss=0.106, v_num=0, reduced_train_loss=0.0595, global_step=1349.0, val_loss=0.164]\n",
      "Epoch 18:  22%|██▏       | 23/104 [00:05<00:18,  4.32it/s, loss=0.0199, v_num=0, reduced_train_loss=0.000585, global_step=1372.0, val_loss=0.0413]\n",
      "Epoch 17:  89%|████████▉ | 93/104 [00:19<00:02,  4.81it/s, loss=0.106, v_num=0, reduced_train_loss=0.0595, global_step=1349.0, val_loss=0.164]\n",
      "Epoch 18:  23%|██▎       | 24/104 [00:05<00:18,  4.32it/s, loss=0.0202, v_num=0, reduced_train_loss=0.00751, global_step=1373.0, val_loss=0.0413] \n",
      "Epoch 18:  26%|██▌       | 27/104 [00:06<00:17,  4.46it/s, loss=0.0262, v_num=0, reduced_train_loss=0.0191, global_step=1376.0, val_loss=0.108] \n",
      "Epoch 18:  24%|██▍       | 25/104 [00:05<00:18,  4.32it/s, loss=0.0203, v_num=0, reduced_train_loss=0.00104, global_step=1374.0, val_loss=0.0413]\n",
      "Epoch 18:  27%|██▋       | 28/104 [00:06<00:17,  4.46it/s, loss=0.0315, v_num=0, reduced_train_loss=0.107, global_step=1377.0, val_loss=0.108] \n",
      "Epoch 18:  25%|██▌       | 26/104 [00:06<00:18,  4.32it/s, loss=0.0198, v_num=0, reduced_train_loss=0.00179, global_step=1375.0, val_loss=0.0413]\n",
      "Epoch 18:  28%|██▊       | 29/104 [00:06<00:16,  4.46it/s, loss=0.033, v_num=0, reduced_train_loss=0.0296, global_step=1378.0, val_loss=0.108]\n",
      "Epoch 18:  26%|██▌       | 27/104 [00:06<00:17,  4.32it/s, loss=0.0199, v_num=0, reduced_train_loss=0.0132, global_step=1376.0, val_loss=0.0413] \n",
      "Epoch 18:  27%|██▋       | 28/104 [00:06<00:17,  4.32it/s, loss=0.02, v_num=0, reduced_train_loss=0.00267, global_step=1377.0, val_loss=0.0413] \n",
      "Epoch 17:  98%|█████████▊| 102/104 [00:20<00:00,  4.99it/s, loss=0.106, v_num=0, reduced_train_loss=0.0595, global_step=1349.0, val_loss=0.164]\n",
      "Epoch 18:  30%|██▉       | 31/104 [00:06<00:16,  4.46it/s, loss=0.0553, v_num=0, reduced_train_loss=0.0821, global_step=1380.0, val_loss=0.108]\n",
      "Epoch 17: 100%|██████████| 104/104 [00:20<00:00,  5.04it/s, loss=0.106, v_num=0, reduced_train_loss=0.0595, global_step=1349.0, val_loss=0.164]2023-05-03 22:02:21,170 - root - INFO - val_loss: 0.1315620392560959\n",
      "Epoch 17: 100%|██████████| 104/104 [00:20<00:00,  5.04it/s, loss=0.106, v_num=0, reduced_train_loss=0.0595, global_step=1349.0, val_loss=0.132]\n",
      "Epoch 18:  28%|██▊       | 29/104 [00:06<00:17,  4.33it/s, loss=0.0131, v_num=0, reduced_train_loss=0.0016, global_step=1378.0, val_loss=0.0413]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 22:02:21.170073 139755592410944 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.1315620392560959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18:  72%|███████▏  | 75/104 [00:17<00:06,  4.41it/s, loss=0.0175, v_num=0, reduced_train_loss=0.00296, global_step=1424.0, val_loss=0.108]  \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 18:  70%|███████   | 73/104 [00:16<00:07,  4.34it/s, loss=0.0255, v_num=0, reduced_train_loss=0.0055, global_step=1422.0, val_loss=0.0413]\n",
      "Epoch 18:  41%|████▏     | 43/104 [00:10<00:14,  4.21it/s, loss=0.0912, v_num=0, reduced_train_loss=0.0201, global_step=1392.0, val_loss=0.132]]\n",
      "Epoch 18:  71%|███████   | 74/104 [00:17<00:06,  4.34it/s, loss=0.0199, v_num=0, reduced_train_loss=0.0471, global_step=1423.0, val_loss=0.0413]\n",
      "Epoch 18:  42%|████▏     | 44/104 [00:10<00:14,  4.21it/s, loss=0.0981, v_num=0, reduced_train_loss=0.224, global_step=1393.0, val_loss=0.132] ]\n",
      "Epoch 18:  72%|███████▏  | 75/104 [00:17<00:06,  4.34it/s, loss=0.0198, v_num=0, reduced_train_loss=0.00206, global_step=1424.0, val_loss=0.0413]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 18:  43%|████▎     | 45/104 [00:10<00:13,  4.22it/s, loss=0.0983, v_num=0, reduced_train_loss=0.0201, global_step=1394.0, val_loss=0.132]]\n",
      "Epoch 18:  78%|███████▊  | 81/104 [00:17<00:05,  4.55it/s, loss=0.0175, v_num=0, reduced_train_loss=0.00296, global_step=1424.0, val_loss=0.108]\n",
      "Epoch 18:  73%|███████▎  | 76/104 [00:17<00:06,  4.35it/s, loss=0.0198, v_num=0, reduced_train_loss=0.00206, global_step=1424.0, val_loss=0.0413]\n",
      "Epoch 18:  44%|████▍     | 46/104 [00:10<00:13,  4.22it/s, loss=0.119, v_num=0, reduced_train_loss=0.415, global_step=1395.0, val_loss=0.132]  ]\n",
      "Epoch 18:  74%|███████▍  | 77/104 [00:17<00:06,  4.38it/s, loss=0.0198, v_num=0, reduced_train_loss=0.00206, global_step=1424.0, val_loss=0.0413]\n",
      "Epoch 18:  80%|███████▉  | 83/104 [00:18<00:04,  4.60it/s, loss=0.0175, v_num=0, reduced_train_loss=0.00296, global_step=1424.0, val_loss=0.108]\n",
      "Epoch 18:  45%|████▌     | 47/104 [00:11<00:13,  4.23it/s, loss=0.135, v_num=0, reduced_train_loss=0.331, global_step=1396.0, val_loss=0.132]413]\n",
      "Epoch 18:  81%|████████  | 84/104 [00:18<00:04,  4.63it/s, loss=0.0175, v_num=0, reduced_train_loss=0.00296, global_step=1424.0, val_loss=0.108]\n",
      "Epoch 18:  76%|███████▌  | 79/104 [00:17<00:05,  4.43it/s, loss=0.0198, v_num=0, reduced_train_loss=0.00206, global_step=1424.0, val_loss=0.0413]\n",
      "Epoch 18:  82%|████████▏ | 85/104 [00:18<00:04,  4.65it/s, loss=0.0175, v_num=0, reduced_train_loss=0.00296, global_step=1424.0, val_loss=0.108]\n",
      "Epoch 18:  46%|████▌     | 48/104 [00:11<00:13,  4.23it/s, loss=0.135, v_num=0, reduced_train_loss=0.00589, global_step=1397.0, val_loss=0.132]3]\n",
      "Epoch 18:  83%|████████▎ | 86/104 [00:18<00:03,  4.68it/s, loss=0.0175, v_num=0, reduced_train_loss=0.00296, global_step=1424.0, val_loss=0.108]\n",
      "Epoch 18:  78%|███████▊  | 81/104 [00:18<00:05,  4.48it/s, loss=0.0198, v_num=0, reduced_train_loss=0.00206, global_step=1424.0, val_loss=0.0413]\n",
      "Epoch 18:  84%|████████▎ | 87/104 [00:18<00:03,  4.70it/s, loss=0.0175, v_num=0, reduced_train_loss=0.00296, global_step=1424.0, val_loss=0.108]\n",
      "Epoch 18:  47%|████▋     | 49/104 [00:11<00:12,  4.23it/s, loss=0.135, v_num=0, reduced_train_loss=0.102, global_step=1398.0, val_loss=0.132]  3]\n",
      "Epoch 18:  85%|████████▍ | 88/104 [00:18<00:03,  4.72it/s, loss=0.0175, v_num=0, reduced_train_loss=0.00296, global_step=1424.0, val_loss=0.108]\n",
      "Epoch 18:  80%|███████▉  | 83/104 [00:18<00:04,  4.53it/s, loss=0.0198, v_num=0, reduced_train_loss=0.00206, global_step=1424.0, val_loss=0.0413]\n",
      "Epoch 18:  86%|████████▌ | 89/104 [00:18<00:03,  4.75it/s, loss=0.0175, v_num=0, reduced_train_loss=0.00296, global_step=1424.0, val_loss=0.108]\n",
      "Epoch 18:  48%|████▊     | 50/104 [00:11<00:12,  4.24it/s, loss=0.136, v_num=0, reduced_train_loss=0.0154, global_step=1399.0, val_loss=0.132]13]\n",
      "Epoch 18:  87%|████████▋ | 90/104 [00:18<00:02,  4.77it/s, loss=0.0175, v_num=0, reduced_train_loss=0.00296, global_step=1424.0, val_loss=0.108]\n",
      "Epoch 18:  82%|████████▏ | 85/104 [00:18<00:04,  4.57it/s, loss=0.0198, v_num=0, reduced_train_loss=0.00206, global_step=1424.0, val_loss=0.0413]\n",
      "Epoch 18:  88%|████████▊ | 91/104 [00:18<00:02,  4.79it/s, loss=0.0175, v_num=0, reduced_train_loss=0.00296, global_step=1424.0, val_loss=0.108]\n",
      "Epoch 18:  49%|████▉     | 51/104 [00:12<00:12,  4.24it/s, loss=0.137, v_num=0, reduced_train_loss=0.0465, global_step=1400.0, val_loss=0.132]13]\n",
      "Epoch 18:  88%|████████▊ | 92/104 [00:19<00:02,  4.82it/s, loss=0.0175, v_num=0, reduced_train_loss=0.00296, global_step=1424.0, val_loss=0.108]\n",
      "Epoch 18:  84%|████████▎ | 87/104 [00:18<00:03,  4.62it/s, loss=0.0198, v_num=0, reduced_train_loss=0.00206, global_step=1424.0, val_loss=0.0413]\n",
      "Epoch 18:  50%|█████     | 52/104 [00:12<00:12,  4.24it/s, loss=0.15, v_num=0, reduced_train_loss=0.293, global_step=1401.0, val_loss=0.132]  8]\n",
      "Epoch 18:  85%|████████▍ | 88/104 [00:18<00:03,  4.64it/s, loss=0.0198, v_num=0, reduced_train_loss=0.00206, global_step=1424.0, val_loss=0.0413]\n",
      "Epoch 18:  90%|█████████ | 94/104 [00:19<00:02,  4.86it/s, loss=0.0175, v_num=0, reduced_train_loss=0.00296, global_step=1424.0, val_loss=0.108]\n",
      "Epoch 18:  86%|████████▌ | 89/104 [00:19<00:03,  4.66it/s, loss=0.0198, v_num=0, reduced_train_loss=0.00206, global_step=1424.0, val_loss=0.0413]\n",
      "Epoch 18:  51%|█████     | 53/104 [00:12<00:12,  4.24it/s, loss=0.132, v_num=0, reduced_train_loss=0.00201, global_step=1402.0, val_loss=0.132]]\n",
      "Epoch 18:  87%|████████▋ | 90/104 [00:19<00:02,  4.68it/s, loss=0.0198, v_num=0, reduced_train_loss=0.00206, global_step=1424.0, val_loss=0.0413]\n",
      "Epoch 18:  92%|█████████▏| 96/104 [00:19<00:01,  4.90it/s, loss=0.0175, v_num=0, reduced_train_loss=0.00296, global_step=1424.0, val_loss=0.108]\n",
      "Epoch 18:  88%|████████▊ | 91/104 [00:19<00:02,  4.70it/s, loss=0.0198, v_num=0, reduced_train_loss=0.00206, global_step=1424.0, val_loss=0.0413]\n",
      "Epoch 18:  52%|█████▏    | 54/104 [00:12<00:11,  4.25it/s, loss=0.134, v_num=0, reduced_train_loss=0.0941, global_step=1403.0, val_loss=0.132] ]\n",
      "Epoch 18:  88%|████████▊ | 92/104 [00:19<00:02,  4.73it/s, loss=0.0198, v_num=0, reduced_train_loss=0.00206, global_step=1424.0, val_loss=0.0413]\n",
      "Epoch 18:  94%|█████████▍| 98/104 [00:19<00:01,  4.95it/s, loss=0.0175, v_num=0, reduced_train_loss=0.00296, global_step=1424.0, val_loss=0.108]\n",
      "Epoch 18:  89%|████████▉ | 93/104 [00:19<00:02,  4.75it/s, loss=0.0198, v_num=0, reduced_train_loss=0.00206, global_step=1424.0, val_loss=0.0413]\n",
      "Epoch 18:  53%|█████▎    | 55/104 [00:12<00:11,  4.25it/s, loss=0.128, v_num=0, reduced_train_loss=0.0107, global_step=1404.0, val_loss=0.132]8]\n",
      "Epoch 18:  96%|█████████▌| 100/104 [00:20<00:00,  4.99it/s, loss=0.0175, v_num=0, reduced_train_loss=0.00296, global_step=1424.0, val_loss=0.108]\n",
      "Epoch 18:  90%|█████████ | 94/104 [00:19<00:02,  4.77it/s, loss=0.0198, v_num=0, reduced_train_loss=0.00206, global_step=1424.0, val_loss=0.0413]\n",
      "Epoch 18:  97%|█████████▋| 101/104 [00:20<00:00,  5.01it/s, loss=0.0175, v_num=0, reduced_train_loss=0.00296, global_step=1424.0, val_loss=0.108]\n",
      "Epoch 18:  54%|█████▍    | 56/104 [00:13<00:11,  4.25it/s, loss=0.136, v_num=0, reduced_train_loss=0.162, global_step=1405.0, val_loss=0.132] 13]\n",
      "Epoch 18:  98%|█████████▊| 102/104 [00:20<00:00,  5.03it/s, loss=0.0175, v_num=0, reduced_train_loss=0.00296, global_step=1424.0, val_loss=0.108]\n",
      "Epoch 18:  92%|█████████▏| 96/104 [00:19<00:01,  4.81it/s, loss=0.0198, v_num=0, reduced_train_loss=0.00206, global_step=1424.0, val_loss=0.0413]\n",
      "Epoch 18:  55%|█████▍    | 57/104 [00:13<00:11,  4.26it/s, loss=0.121, v_num=0, reduced_train_loss=0.00667, global_step=1406.0, val_loss=0.132]8]\n",
      "Epoch 18:  93%|█████████▎| 97/104 [00:20<00:01,  4.83it/s, loss=0.0198, v_num=0, reduced_train_loss=0.00206, global_step=1424.0, val_loss=0.0413]\n",
      "Epoch 18: 100%|██████████| 104/104 [00:20<00:00,  5.08it/s, loss=0.0175, v_num=0, reduced_train_loss=0.00296, global_step=1424.0, val_loss=0.108]2023-05-03 22:02:34,619 - root - INFO - val_loss: 0.10008056461811066\n",
      "Epoch 18: 100%|██████████| 104/104 [00:20<00:00,  5.08it/s, loss=0.0175, v_num=0, reduced_train_loss=0.00296, global_step=1424.0, val_loss=0.100]\n",
      "Epoch 19:   0%|          | 0/104 [00:00<?, ?it/s, loss=0.0175, v_num=0, reduced_train_loss=0.00296, global_step=1424.0, val_loss=0.100]          \n",
      "Epoch 18:  94%|█████████▍| 98/104 [00:20<00:01,  4.85it/s, loss=0.0198, v_num=0, reduced_train_loss=0.00206, global_step=1424.0, val_loss=0.0413]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 22:02:34.619524 140522389899072 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.10008056461811066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18:  56%|█████▌    | 58/104 [00:13<00:10,  4.26it/s, loss=0.118, v_num=0, reduced_train_loss=0.00474, global_step=1407.0, val_loss=0.132]\n",
      "Epoch 19:   1%|          | 1/104 [00:00<00:27,  3.72it/s, loss=0.015, v_num=0, reduced_train_loss=0.00986, global_step=1425.0, val_loss=0.100] 3]\n",
      "Epoch 18:  57%|█████▋    | 59/104 [00:13<00:10,  4.26it/s, loss=0.113, v_num=0, reduced_train_loss=0.00804, global_step=1408.0, val_loss=0.132]13]\n",
      "Epoch 19:   2%|▏         | 2/104 [00:00<00:24,  4.09it/s, loss=0.0151, v_num=0, reduced_train_loss=0.0055, global_step=1426.0, val_loss=0.100]413]\n",
      "Epoch 18:  58%|█████▊    | 60/104 [00:14<00:10,  4.27it/s, loss=0.111, v_num=0, reduced_train_loss=0.0202, global_step=1409.0, val_loss=0.132] 13]\n",
      "Epoch 19:   3%|▎         | 3/104 [00:00<00:23,  4.23it/s, loss=0.0151, v_num=0, reduced_train_loss=0.000636, global_step=1427.0, val_loss=0.100]3]\n",
      "Epoch 18: 100%|██████████| 104/104 [00:20<00:00,  4.98it/s, loss=0.0198, v_num=0, reduced_train_loss=0.00206, global_step=1424.0, val_loss=0.0413]2023-05-03 22:02:35,385 - root - INFO - val_loss: 0.06310959160327911\n",
      "Epoch 18: 100%|██████████| 104/104 [00:20<00:00,  4.98it/s, loss=0.0198, v_num=0, reduced_train_loss=0.00206, global_step=1424.0, val_loss=0.0631]\n",
      "Epoch 19:   0%|          | 0/104 [00:00<?, ?it/s, loss=0.0198, v_num=0, reduced_train_loss=0.00206, global_step=1424.0, val_loss=0.0631]          "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 22:02:35.385702 140649257482048 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.06310959160327911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18:  72%|███████▏  | 75/104 [00:17<00:06,  4.29it/s, loss=0.0797, v_num=0, reduced_train_loss=0.0326, global_step=1424.0, val_loss=0.132]   \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 19:  18%|█▊        | 19/104 [00:04<00:19,  4.45it/s, loss=0.0204, v_num=0, reduced_train_loss=0.000392, global_step=1443.0, val_loss=0.100]\n",
      "Epoch 19:  15%|█▌        | 16/104 [00:03<00:20,  4.31it/s, loss=0.0105, v_num=0, reduced_train_loss=0.00149, global_step=1439.0, val_loss=0.0631]\n",
      "Epoch 19:  19%|█▉        | 20/104 [00:04<00:18,  4.46it/s, loss=0.0203, v_num=0, reduced_train_loss=0.000377, global_step=1444.0, val_loss=0.100]]\n",
      "Epoch 19:  20%|██        | 21/104 [00:04<00:18,  4.46it/s, loss=0.0199, v_num=0, reduced_train_loss=0.000814, global_step=1445.0, val_loss=0.100] \n",
      "Epoch 18:  77%|███████▋  | 80/104 [00:18<00:05,  4.40it/s, loss=0.0797, v_num=0, reduced_train_loss=0.0326, global_step=1424.0, val_loss=0.132]\n",
      "Epoch 19:  17%|█▋        | 18/104 [00:04<00:19,  4.32it/s, loss=0.0103, v_num=0, reduced_train_loss=0.00892, global_step=1442.0, val_loss=0.0631]\n",
      "Epoch 18:  79%|███████▉  | 82/104 [00:18<00:04,  4.45it/s, loss=0.0797, v_num=0, reduced_train_loss=0.0326, global_step=1424.0, val_loss=0.132]\n",
      "Epoch 19:  18%|█▊        | 19/104 [00:04<00:19,  4.33it/s, loss=0.00803, v_num=0, reduced_train_loss=0.00184, global_step=1443.0, val_loss=0.0631]\n",
      "Epoch 18:  81%|████████  | 84/104 [00:18<00:04,  4.49it/s, loss=0.0797, v_num=0, reduced_train_loss=0.0326, global_step=1424.0, val_loss=0.132]\n",
      "Epoch 19:  19%|█▉        | 20/104 [00:04<00:19,  4.33it/s, loss=0.00837, v_num=0, reduced_train_loss=0.00886, global_step=1444.0, val_loss=0.0631]\n",
      "Epoch 19:  20%|██        | 21/104 [00:04<00:19,  4.33it/s, loss=0.00779, v_num=0, reduced_train_loss=7.39e-5, global_step=1445.0, val_loss=0.0631]\n",
      "Epoch 18:  84%|████████▎ | 87/104 [00:19<00:03,  4.56it/s, loss=0.0797, v_num=0, reduced_train_loss=0.0326, global_step=1424.0, val_loss=0.132]\n",
      "Epoch 19:  21%|██        | 22/104 [00:05<00:18,  4.34it/s, loss=0.00818, v_num=0, reduced_train_loss=0.0103, global_step=1446.0, val_loss=0.0631] \n",
      "Epoch 18:  86%|████████▌ | 89/104 [00:19<00:03,  4.61it/s, loss=0.0797, v_num=0, reduced_train_loss=0.0326, global_step=1424.0, val_loss=0.132]\n",
      "Epoch 19:  22%|██▏       | 23/104 [00:05<00:18,  4.34it/s, loss=0.00819, v_num=0, reduced_train_loss=0.000408, global_step=1447.0, val_loss=0.0631]\n",
      "Epoch 18:  88%|████████▊ | 91/104 [00:19<00:02,  4.65it/s, loss=0.0797, v_num=0, reduced_train_loss=0.0326, global_step=1424.0, val_loss=0.132]\n",
      "Epoch 19:  23%|██▎       | 24/104 [00:05<00:18,  4.35it/s, loss=0.00858, v_num=0, reduced_train_loss=0.00804, global_step=1448.0, val_loss=0.0631] \n",
      "Epoch 19:  28%|██▊       | 29/104 [00:06<00:16,  4.46it/s, loss=0.021, v_num=0, reduced_train_loss=0.0423, global_step=1453.0, val_loss=0.100]  \n",
      "Epoch 19:  24%|██▍       | 25/104 [00:05<00:18,  4.36it/s, loss=0.00682, v_num=0, reduced_train_loss=0.000506, global_step=1449.0, val_loss=0.0631]\n",
      "Epoch 19:  29%|██▉       | 30/104 [00:06<00:16,  4.46it/s, loss=0.0238, v_num=0, reduced_train_loss=0.0562, global_step=1454.0, val_loss=0.100]631]\n",
      "Epoch 18:  92%|█████████▏| 96/104 [00:20<00:01,  4.75it/s, loss=0.0797, v_num=0, reduced_train_loss=0.0326, global_step=1424.0, val_loss=0.132]\n",
      "Epoch 19:  30%|██▉       | 31/104 [00:06<00:16,  4.46it/s, loss=0.0239, v_num=0, reduced_train_loss=0.00186, global_step=1455.0, val_loss=0.100]31]\n",
      "Epoch 18:  94%|█████████▍| 98/104 [00:20<00:01,  4.79it/s, loss=0.0797, v_num=0, reduced_train_loss=0.0326, global_step=1424.0, val_loss=0.132]\n",
      "Epoch 19:  31%|███       | 32/104 [00:07<00:16,  4.46it/s, loss=0.0344, v_num=0, reduced_train_loss=0.212, global_step=1456.0, val_loss=0.100]     \n",
      "Epoch 18:  96%|█████████▌| 100/104 [00:20<00:00,  4.83it/s, loss=0.0797, v_num=0, reduced_train_loss=0.0326, global_step=1424.0, val_loss=0.132]\n",
      "Epoch 19:  32%|███▏      | 33/104 [00:07<00:15,  4.46it/s, loss=0.0341, v_num=0, reduced_train_loss=0.00154, global_step=1457.0, val_loss=0.100]1]\n",
      "Epoch 19:  29%|██▉       | 30/104 [00:06<00:16,  4.38it/s, loss=0.0065, v_num=0, reduced_train_loss=0.00239, global_step=1454.0, val_loss=0.0631] \n",
      "Epoch 19:  33%|███▎      | 34/104 [00:07<00:15,  4.46it/s, loss=0.0293, v_num=0, reduced_train_loss=0.00015, global_step=1458.0, val_loss=0.100]\n",
      "Epoch 18: 100%|██████████| 104/104 [00:21<00:00,  4.92it/s, loss=0.0797, v_num=0, reduced_train_loss=0.0326, global_step=1424.0, val_loss=0.132]2023-05-03 22:02:42,309 - root - INFO - val_loss: 0.1495768278837204\n",
      "Epoch 18: 100%|██████████| 104/104 [00:21<00:00,  4.92it/s, loss=0.0797, v_num=0, reduced_train_loss=0.0326, global_step=1424.0, val_loss=0.150]\n",
      "Epoch 19:   0%|          | 0/104 [00:00<?, ?it/s, loss=0.0797, v_num=0, reduced_train_loss=0.0326, global_step=1424.0, val_loss=0.150]          "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 22:02:42.309551 139755592410944 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.1495768278837204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19:  72%|███████▏  | 75/104 [00:16<00:06,  4.44it/s, loss=0.0612, v_num=0, reduced_train_loss=0.00342, global_step=1499.0, val_loss=0.100]1] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 19:  69%|██████▉   | 72/104 [00:16<00:07,  4.45it/s, loss=0.0254, v_num=0, reduced_train_loss=0.000933, global_step=1496.0, val_loss=0.0631]\n",
      "Epoch 19:  70%|███████   | 73/104 [00:16<00:06,  4.45it/s, loss=0.0412, v_num=0, reduced_train_loss=0.322, global_step=1497.0, val_loss=0.0631]   \n",
      "Epoch 19:  40%|████      | 42/104 [00:09<00:14,  4.36it/s, loss=0.09, v_num=0, reduced_train_loss=0.0494, global_step=1466.0, val_loss=0.150] 0]\n",
      "Epoch 19:  71%|███████   | 74/104 [00:16<00:06,  4.45it/s, loss=0.0414, v_num=0, reduced_train_loss=0.00311, global_step=1498.0, val_loss=0.0631]\n",
      "Epoch 19:  41%|████▏     | 43/104 [00:09<00:13,  4.37it/s, loss=0.088, v_num=0, reduced_train_loss=0.00174, global_step=1467.0, val_loss=0.150]]\n",
      "Epoch 19:  72%|███████▏  | 75/104 [00:16<00:06,  4.45it/s, loss=0.053, v_num=0, reduced_train_loss=0.247, global_step=1499.0, val_loss=0.0631]   \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 19:  42%|████▏     | 44/104 [00:10<00:13,  4.37it/s, loss=0.102, v_num=0, reduced_train_loss=0.291, global_step=1468.0, val_loss=0.150]  ]\n",
      "Epoch 19:  73%|███████▎  | 76/104 [00:17<00:06,  4.46it/s, loss=0.053, v_num=0, reduced_train_loss=0.247, global_step=1499.0, val_loss=0.0631]\n",
      "Epoch 19:  79%|███████▉  | 82/104 [00:17<00:04,  4.60it/s, loss=0.0612, v_num=0, reduced_train_loss=0.00342, global_step=1499.0, val_loss=0.100]\n",
      "Epoch 19:  74%|███████▍  | 77/104 [00:17<00:06,  4.49it/s, loss=0.053, v_num=0, reduced_train_loss=0.247, global_step=1499.0, val_loss=0.0631]\n",
      "Epoch 19:  43%|████▎     | 45/104 [00:10<00:13,  4.37it/s, loss=0.105, v_num=0, reduced_train_loss=0.202, global_step=1469.0, val_loss=0.150]00]\n",
      "Epoch 19:  75%|███████▌  | 78/104 [00:17<00:05,  4.51it/s, loss=0.053, v_num=0, reduced_train_loss=0.247, global_step=1499.0, val_loss=0.0631]\n",
      "Epoch 19:  81%|████████  | 84/104 [00:18<00:04,  4.64it/s, loss=0.0612, v_num=0, reduced_train_loss=0.00342, global_step=1499.0, val_loss=0.100]\n",
      "Epoch 19:  44%|████▍     | 46/104 [00:10<00:13,  4.38it/s, loss=0.105, v_num=0, reduced_train_loss=0.000842, global_step=1470.0, val_loss=0.150]\n",
      "Epoch 19:  82%|████████▏ | 85/104 [00:18<00:04,  4.67it/s, loss=0.0612, v_num=0, reduced_train_loss=0.00342, global_step=1499.0, val_loss=0.100]\n",
      "Epoch 19:  77%|███████▋  | 80/104 [00:17<00:05,  4.56it/s, loss=0.053, v_num=0, reduced_train_loss=0.247, global_step=1499.0, val_loss=0.0631]\n",
      "Epoch 19:  83%|████████▎ | 86/104 [00:18<00:03,  4.69it/s, loss=0.0612, v_num=0, reduced_train_loss=0.00342, global_step=1499.0, val_loss=0.100]\n",
      "Epoch 19:  45%|████▌     | 47/104 [00:10<00:13,  4.38it/s, loss=0.107, v_num=0, reduced_train_loss=0.038, global_step=1471.0, val_loss=0.150]   \n",
      "Epoch 19:  84%|████████▎ | 87/104 [00:18<00:03,  4.71it/s, loss=0.0612, v_num=0, reduced_train_loss=0.00342, global_step=1499.0, val_loss=0.100]\n",
      "Epoch 19:  79%|███████▉  | 82/104 [00:17<00:04,  4.61it/s, loss=0.053, v_num=0, reduced_train_loss=0.247, global_step=1499.0, val_loss=0.0631]\n",
      "Epoch 19:  46%|████▌     | 48/104 [00:10<00:12,  4.38it/s, loss=0.127, v_num=0, reduced_train_loss=0.420, global_step=1472.0, val_loss=0.150]00]\n",
      "Epoch 19:  80%|███████▉  | 83/104 [00:17<00:04,  4.64it/s, loss=0.053, v_num=0, reduced_train_loss=0.247, global_step=1499.0, val_loss=0.0631]\n",
      "Epoch 19:  86%|████████▌ | 89/104 [00:18<00:03,  4.76it/s, loss=0.0612, v_num=0, reduced_train_loss=0.00342, global_step=1499.0, val_loss=0.100]\n",
      "Epoch 19:  81%|████████  | 84/104 [00:18<00:04,  4.66it/s, loss=0.053, v_num=0, reduced_train_loss=0.247, global_step=1499.0, val_loss=0.0631]\n",
      "Epoch 19:  47%|████▋     | 49/104 [00:11<00:12,  4.38it/s, loss=0.138, v_num=0, reduced_train_loss=0.453, global_step=1473.0, val_loss=0.150]00]\n",
      "Epoch 19:  82%|████████▏ | 85/104 [00:18<00:04,  4.68it/s, loss=0.053, v_num=0, reduced_train_loss=0.247, global_step=1499.0, val_loss=0.0631]\n",
      "Epoch 19:  88%|████████▊ | 91/104 [00:18<00:02,  4.80it/s, loss=0.0612, v_num=0, reduced_train_loss=0.00342, global_step=1499.0, val_loss=0.100]\n",
      "Epoch 19:  83%|████████▎ | 86/104 [00:18<00:03,  4.70it/s, loss=0.053, v_num=0, reduced_train_loss=0.247, global_step=1499.0, val_loss=0.0631]\n",
      "Epoch 19:  48%|████▊     | 50/104 [00:11<00:12,  4.38it/s, loss=0.142, v_num=0, reduced_train_loss=0.259, global_step=1474.0, val_loss=0.150]00]\n",
      "Epoch 19:  84%|████████▎ | 87/104 [00:18<00:03,  4.73it/s, loss=0.053, v_num=0, reduced_train_loss=0.247, global_step=1499.0, val_loss=0.0631]\n",
      "Epoch 19:  89%|████████▉ | 93/104 [00:19<00:02,  4.84it/s, loss=0.0612, v_num=0, reduced_train_loss=0.00342, global_step=1499.0, val_loss=0.100]\n",
      "Epoch 19:  49%|████▉     | 51/104 [00:11<00:12,  4.38it/s, loss=0.143, v_num=0, reduced_train_loss=0.0448, global_step=1475.0, val_loss=0.150]\n",
      "Epoch 19:  90%|█████████ | 94/104 [00:19<00:02,  4.86it/s, loss=0.0612, v_num=0, reduced_train_loss=0.00342, global_step=1499.0, val_loss=0.100]\n",
      "Epoch 19:  86%|████████▌ | 89/104 [00:18<00:03,  4.77it/s, loss=0.053, v_num=0, reduced_train_loss=0.247, global_step=1499.0, val_loss=0.0631]\n",
      "Epoch 19:  91%|█████████▏| 95/104 [00:19<00:01,  4.88it/s, loss=0.0612, v_num=0, reduced_train_loss=0.00342, global_step=1499.0, val_loss=0.100]\n",
      "Epoch 19:  50%|█████     | 52/104 [00:11<00:11,  4.39it/s, loss=0.142, v_num=0, reduced_train_loss=0.127, global_step=1476.0, val_loss=0.150] \n",
      "Epoch 19:  92%|█████████▏| 96/104 [00:19<00:01,  4.90it/s, loss=0.0612, v_num=0, reduced_train_loss=0.00342, global_step=1499.0, val_loss=0.100]\n",
      "Epoch 19:  88%|████████▊ | 91/104 [00:18<00:02,  4.82it/s, loss=0.053, v_num=0, reduced_train_loss=0.247, global_step=1499.0, val_loss=0.0631]\n",
      "Epoch 19:  93%|█████████▎| 97/104 [00:19<00:01,  4.92it/s, loss=0.0612, v_num=0, reduced_train_loss=0.00342, global_step=1499.0, val_loss=0.100]\n",
      "Epoch 19:  51%|█████     | 53/104 [00:12<00:11,  4.39it/s, loss=0.14, v_num=0, reduced_train_loss=0.0291, global_step=1477.0, val_loss=0.150]]\n",
      "Epoch 19:  94%|█████████▍| 98/104 [00:19<00:01,  4.93it/s, loss=0.0612, v_num=0, reduced_train_loss=0.00342, global_step=1499.0, val_loss=0.100]\n",
      "Epoch 19:  52%|█████▏    | 54/104 [00:12<00:11,  4.39it/s, loss=0.148, v_num=0, reduced_train_loss=0.159, global_step=1478.0, val_loss=0.150]]\n",
      "Epoch 19:  95%|█████████▌| 99/104 [00:20<00:01,  4.95it/s, loss=0.0612, v_num=0, reduced_train_loss=0.00342, global_step=1499.0, val_loss=0.100]\n",
      "Epoch 19:  90%|█████████ | 94/104 [00:19<00:02,  4.88it/s, loss=0.053, v_num=0, reduced_train_loss=0.247, global_step=1499.0, val_loss=0.0631]\n",
      "Epoch 19:  91%|█████████▏| 95/104 [00:19<00:01,  4.90it/s, loss=0.053, v_num=0, reduced_train_loss=0.247, global_step=1499.0, val_loss=0.0631]\n",
      "Epoch 19:  53%|█████▎    | 55/104 [00:12<00:11,  4.39it/s, loss=0.152, v_num=0, reduced_train_loss=0.0961, global_step=1479.0, val_loss=0.150]00]\n",
      "Epoch 19:  92%|█████████▏| 96/104 [00:19<00:01,  4.93it/s, loss=0.053, v_num=0, reduced_train_loss=0.247, global_step=1499.0, val_loss=0.0631]\n",
      "Epoch 19:  97%|█████████▋| 101/104 [00:20<00:00,  4.98it/s, loss=0.0612, v_num=0, reduced_train_loss=0.00342, global_step=1499.0, val_loss=0.100]\n",
      "Epoch 19:  93%|█████████▎| 97/104 [00:19<00:01,  4.95it/s, loss=0.053, v_num=0, reduced_train_loss=0.247, global_step=1499.0, val_loss=0.0631]\n",
      "Epoch 19:  54%|█████▍    | 56/104 [00:12<00:10,  4.39it/s, loss=0.15, v_num=0, reduced_train_loss=0.00168, global_step=1480.0, val_loss=0.150]00]\n",
      "Epoch 19:  94%|█████████▍| 98/104 [00:19<00:01,  4.97it/s, loss=0.053, v_num=0, reduced_train_loss=0.247, global_step=1499.0, val_loss=0.0631]\n",
      "Epoch 19:  99%|█████████▉| 103/104 [00:20<00:00,  5.01it/s, loss=0.0612, v_num=0, reduced_train_loss=0.00342, global_step=1499.0, val_loss=0.100]\n",
      "Epoch 19:  95%|█████████▌| 99/104 [00:19<00:01,  4.99it/s, loss=0.053, v_num=0, reduced_train_loss=0.247, global_step=1499.0, val_loss=0.0631]\n",
      "Epoch 19: 100%|██████████| 104/104 [00:20<00:00,  5.04it/s, loss=0.0612, v_num=0, reduced_train_loss=0.00342, global_step=1499.0, val_loss=0.100]2023-05-03 22:02:55,275 - root - INFO - val_loss: 0.12376294285058975\n",
      "Epoch 19: 100%|██████████| 104/104 [00:20<00:00,  5.04it/s, loss=0.0612, v_num=0, reduced_train_loss=0.00342, global_step=1499.0, val_loss=0.124]\n",
      "Epoch 19:  55%|█████▍    | 57/104 [00:12<00:10,  4.39it/s, loss=0.154, v_num=0, reduced_train_loss=0.0744, global_step=1481.0, val_loss=0.150]   \n",
      "Epoch 19:  96%|█████████▌| 100/104 [00:19<00:00,  5.01it/s, loss=0.053, v_num=0, reduced_train_loss=0.247, global_step=1499.0, val_loss=0.0631]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 22:02:55.275534 140522389899072 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.12376294285058975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20:   1%|          | 1/104 [00:00<00:28,  3.57it/s, loss=0.0612, v_num=0, reduced_train_loss=0.00079, global_step=1500.0, val_loss=0.124]\n",
      "Epoch 19:  98%|█████████▊| 102/104 [00:20<00:00,  5.04it/s, loss=0.053, v_num=0, reduced_train_loss=0.247, global_step=1499.0, val_loss=0.0631]\n",
      "Epoch 20:   2%|▏         | 2/104 [00:00<00:25,  3.97it/s, loss=0.0611, v_num=0, reduced_train_loss=0.00253, global_step=1501.0, val_loss=0.124]\n",
      "Epoch 19: 100%|██████████| 104/104 [00:20<00:00,  5.10it/s, loss=0.053, v_num=0, reduced_train_loss=0.247, global_step=1499.0, val_loss=0.0631]2023-05-03 22:02:55,796 - root - INFO - val_loss: 0.06662103533744812\n",
      "Epoch 19: 100%|██████████| 104/104 [00:20<00:00,  5.10it/s, loss=0.053, v_num=0, reduced_train_loss=0.247, global_step=1499.0, val_loss=0.0666]\n",
      "Epoch 20:   0%|          | 0/104 [00:00<?, ?it/s, loss=0.053, v_num=0, reduced_train_loss=0.247, global_step=1499.0, val_loss=0.0666]          "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 22:02:55.796039 140649257482048 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.06662103533744812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19:  72%|███████▏  | 75/104 [00:16<00:06,  4.42it/s, loss=0.0615, v_num=0, reduced_train_loss=0.00203, global_step=1499.0, val_loss=0.150]]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 20:  17%|█▋        | 18/104 [00:04<00:19,  4.34it/s, loss=0.0222, v_num=0, reduced_train_loss=0.00198, global_step=1517.0, val_loss=0.124]]\n",
      "Epoch 19:  73%|███████▎  | 76/104 [00:17<00:06,  4.43it/s, loss=0.0615, v_num=0, reduced_train_loss=0.00203, global_step=1499.0, val_loss=0.150]\n",
      "Epoch 20:  18%|█▊        | 19/104 [00:04<00:19,  4.34it/s, loss=0.0217, v_num=0, reduced_train_loss=0.00223, global_step=1518.0, val_loss=0.124]]\n",
      "Epoch 19:  75%|███████▌  | 78/104 [00:17<00:05,  4.49it/s, loss=0.0615, v_num=0, reduced_train_loss=0.00203, global_step=1499.0, val_loss=0.150]\n",
      "Epoch 20:  19%|█▉        | 20/104 [00:04<00:19,  4.32it/s, loss=0.0215, v_num=0, reduced_train_loss=0.000222, global_step=1519.0, val_loss=0.124]]\n",
      "Epoch 20:  18%|█▊        | 19/104 [00:04<00:19,  4.46it/s, loss=0.0535, v_num=0, reduced_train_loss=0.000814, global_step=1518.0, val_loss=0.0666]\n",
      "Epoch 20:  20%|██        | 21/104 [00:04<00:19,  4.32it/s, loss=0.0219, v_num=0, reduced_train_loss=0.00921, global_step=1520.0, val_loss=0.124] \n",
      "Epoch 20:  19%|█▉        | 20/104 [00:04<00:18,  4.46it/s, loss=0.0412, v_num=0, reduced_train_loss=0.00152, global_step=1519.0, val_loss=0.0666] \n",
      "Epoch 20:  21%|██        | 22/104 [00:05<00:18,  4.32it/s, loss=0.0219, v_num=0, reduced_train_loss=0.000914, global_step=1521.0, val_loss=0.124]\n",
      "Epoch 20:  20%|██        | 21/104 [00:04<00:18,  4.46it/s, loss=0.0307, v_num=0, reduced_train_loss=0.00752, global_step=1520.0, val_loss=0.0666]\n",
      "Epoch 20:  22%|██▏       | 23/104 [00:05<00:18,  4.32it/s, loss=0.021, v_num=0, reduced_train_loss=0.000122, global_step=1522.0, val_loss=0.124] \n",
      "Epoch 20:  21%|██        | 22/104 [00:04<00:18,  4.46it/s, loss=0.0305, v_num=0, reduced_train_loss=0.000182, global_step=1521.0, val_loss=0.0666]\n",
      "Epoch 20:  23%|██▎       | 24/104 [00:05<00:18,  4.32it/s, loss=0.02, v_num=0, reduced_train_loss=0.0115, global_step=1523.0, val_loss=0.124]   \n",
      "Epoch 20:  22%|██▏       | 23/104 [00:05<00:18,  4.46it/s, loss=0.0383, v_num=0, reduced_train_loss=0.165, global_step=1522.0, val_loss=0.0666]   \n",
      "Epoch 20:  23%|██▎       | 24/104 [00:05<00:17,  4.46it/s, loss=0.0383, v_num=0, reduced_train_loss=0.165, global_step=1522.0, val_loss=0.0666]4]\n",
      "Epoch 20:  25%|██▌       | 26/104 [00:06<00:18,  4.32it/s, loss=0.0271, v_num=0, reduced_train_loss=0.228, global_step=1525.0, val_loss=0.124]   \n",
      "Epoch 20:  24%|██▍       | 25/104 [00:05<00:17,  4.46it/s, loss=0.023, v_num=0, reduced_train_loss=0.000276, global_step=1524.0, val_loss=0.0666]\n",
      "Epoch 20:  26%|██▌       | 27/104 [00:06<00:17,  4.33it/s, loss=0.0272, v_num=0, reduced_train_loss=0.000955, global_step=1526.0, val_loss=0.124]\n",
      "Epoch 20:  25%|██▌       | 26/104 [00:05<00:17,  4.47it/s, loss=0.0211, v_num=0, reduced_train_loss=0.001, global_step=1525.0, val_loss=0.0666]  \n",
      "Epoch 20:  27%|██▋       | 28/104 [00:06<00:17,  4.33it/s, loss=0.0267, v_num=0, reduced_train_loss=0.00175, global_step=1527.0, val_loss=0.124] \n",
      "Epoch 20:  26%|██▌       | 27/104 [00:06<00:17,  4.47it/s, loss=0.0239, v_num=0, reduced_train_loss=0.058, global_step=1526.0, val_loss=0.0666]]\n",
      "Epoch 20:  28%|██▊       | 29/104 [00:06<00:17,  4.33it/s, loss=0.0283, v_num=0, reduced_train_loss=0.0411, global_step=1528.0, val_loss=0.124] \n",
      "Epoch 20:  27%|██▋       | 28/104 [00:06<00:17,  4.47it/s, loss=0.0239, v_num=0, reduced_train_loss=0.00153, global_step=1527.0, val_loss=0.0666]\n",
      "Epoch 20:  29%|██▉       | 30/104 [00:06<00:17,  4.33it/s, loss=0.0381, v_num=0, reduced_train_loss=0.216, global_step=1529.0, val_loss=0.124] ]\n",
      "Epoch 20:  28%|██▊       | 29/104 [00:06<00:16,  4.47it/s, loss=0.027, v_num=0, reduced_train_loss=0.0615, global_step=1528.0, val_loss=0.0666]  \n",
      "Epoch 20:  29%|██▉       | 30/104 [00:06<00:16,  4.47it/s, loss=0.027, v_num=0, reduced_train_loss=0.00305, global_step=1529.0, val_loss=0.0666]]\n",
      "Epoch 19:  97%|█████████▋| 101/104 [00:20<00:00,  5.00it/s, loss=0.0615, v_num=0, reduced_train_loss=0.00203, global_step=1499.0, val_loss=0.150]\n",
      "Epoch 20:  30%|██▉       | 31/104 [00:06<00:16,  4.47it/s, loss=0.0271, v_num=0, reduced_train_loss=0.00433, global_step=1530.0, val_loss=0.0666]\n",
      "Epoch 19:  99%|█████████▉| 103/104 [00:20<00:00,  5.04it/s, loss=0.0615, v_num=0, reduced_train_loss=0.00203, global_step=1499.0, val_loss=0.150]\n",
      "Epoch 19: 100%|██████████| 104/104 [00:20<00:00,  5.07it/s, loss=0.0615, v_num=0, reduced_train_loss=0.00203, global_step=1499.0, val_loss=0.150]2023-05-03 22:03:02,819 - root - INFO - val_loss: 0.1222435012459755\n",
      "Epoch 19: 100%|██████████| 104/104 [00:20<00:00,  5.07it/s, loss=0.0615, v_num=0, reduced_train_loss=0.00203, global_step=1499.0, val_loss=0.122]\n",
      "Epoch 20:  32%|███▏      | 33/104 [00:07<00:16,  4.34it/s, loss=0.0275, v_num=0, reduced_train_loss=0.00291, global_step=1532.0, val_loss=0.124] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 22:03:02.819531 139755592410944 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.1222435012459755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20:  72%|███████▏  | 75/104 [00:17<00:06,  4.36it/s, loss=0.0453, v_num=0, reduced_train_loss=0.00767, global_step=1574.0, val_loss=0.124]6]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 20:  72%|███████▏  | 75/104 [00:16<00:06,  4.46it/s, loss=0.0289, v_num=0, reduced_train_loss=0.00487, global_step=1574.0, val_loss=0.0666] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 20:  73%|███████▎  | 76/104 [00:17<00:06,  4.37it/s, loss=0.0453, v_num=0, reduced_train_loss=0.00767, global_step=1574.0, val_loss=0.124]\n",
      "Epoch 20:  73%|███████▎  | 76/104 [00:17<00:06,  4.47it/s, loss=0.0289, v_num=0, reduced_train_loss=0.00487, global_step=1574.0, val_loss=0.0666]\n",
      "Epoch 20:  41%|████▏     | 43/104 [00:10<00:14,  4.30it/s, loss=0.0489, v_num=0, reduced_train_loss=0.107, global_step=1542.0, val_loss=0.122] ]\n",
      "Epoch 20:  74%|███████▍  | 77/104 [00:17<00:06,  4.50it/s, loss=0.0289, v_num=0, reduced_train_loss=0.00487, global_step=1574.0, val_loss=0.0666]\n",
      "Epoch 20:  75%|███████▌  | 78/104 [00:17<00:05,  4.42it/s, loss=0.0453, v_num=0, reduced_train_loss=0.00767, global_step=1574.0, val_loss=0.124]\n",
      "Epoch 20:  75%|███████▌  | 78/104 [00:17<00:05,  4.52it/s, loss=0.0289, v_num=0, reduced_train_loss=0.00487, global_step=1574.0, val_loss=0.0666]\n",
      "Epoch 20:  42%|████▏     | 44/104 [00:10<00:13,  4.30it/s, loss=0.0501, v_num=0, reduced_train_loss=0.0341, global_step=1543.0, val_loss=0.122]]\n",
      "Epoch 20:  76%|███████▌  | 79/104 [00:17<00:05,  4.55it/s, loss=0.0289, v_num=0, reduced_train_loss=0.00487, global_step=1574.0, val_loss=0.0666]\n",
      "Epoch 20:  43%|████▎     | 45/104 [00:10<00:13,  4.30it/s, loss=0.0572, v_num=0, reduced_train_loss=0.145, global_step=1544.0, val_loss=0.122] ]\n",
      "Epoch 20:  77%|███████▋  | 80/104 [00:17<00:05,  4.57it/s, loss=0.0289, v_num=0, reduced_train_loss=0.00487, global_step=1574.0, val_loss=0.0666]\n",
      "Epoch 20:  78%|███████▊  | 81/104 [00:18<00:05,  4.49it/s, loss=0.0453, v_num=0, reduced_train_loss=0.00767, global_step=1574.0, val_loss=0.124]\n",
      "Epoch 20:  78%|███████▊  | 81/104 [00:17<00:05,  4.60it/s, loss=0.0289, v_num=0, reduced_train_loss=0.00487, global_step=1574.0, val_loss=0.0666]\n",
      "Epoch 20:  44%|████▍     | 46/104 [00:10<00:13,  4.30it/s, loss=0.0626, v_num=0, reduced_train_loss=0.119, global_step=1545.0, val_loss=0.122]4]\n",
      "Validation DataLoader 0:  24%|██▍       | 7/29 [00:00<00:02,  7.71it/s]\n",
      "Epoch 20:  80%|███████▉  | 83/104 [00:18<00:04,  4.54it/s, loss=0.0453, v_num=0, reduced_train_loss=0.00767, global_step=1574.0, val_loss=0.124]]\n",
      "Epoch 20:  81%|████████  | 84/104 [00:18<00:04,  4.56it/s, loss=0.0453, v_num=0, reduced_train_loss=0.00767, global_step=1574.0, val_loss=0.124]\n",
      "Epoch 20:  45%|████▌     | 47/104 [00:10<00:13,  4.30it/s, loss=0.0742, v_num=0, reduced_train_loss=0.234, global_step=1546.0, val_loss=0.122]66]\n",
      "\n",
      "Epoch 20:  81%|████████  | 84/104 [00:18<00:04,  4.66it/s, loss=0.0289, v_num=0, reduced_train_loss=0.00487, global_step=1574.0, val_loss=0.0666]\n",
      "Epoch 20:  83%|████████▎ | 86/104 [00:18<00:03,  4.61it/s, loss=0.0453, v_num=0, reduced_train_loss=0.00767, global_step=1574.0, val_loss=0.124]\n",
      "Epoch 20:  46%|████▌     | 48/104 [00:11<00:13,  4.30it/s, loss=0.0949, v_num=0, reduced_train_loss=0.417, global_step=1547.0, val_loss=0.122]66]\n",
      "\n",
      "Epoch 20:  84%|████████▎ | 87/104 [00:18<00:03,  4.63it/s, loss=0.0453, v_num=0, reduced_train_loss=0.00767, global_step=1574.0, val_loss=0.124]]\n",
      "\n",
      "Epoch 20:  47%|████▋     | 49/104 [00:11<00:12,  4.30it/s, loss=0.102, v_num=0, reduced_train_loss=0.149, global_step=1548.0, val_loss=0.122] 66]\n",
      "Epoch 20:  86%|████████▌ | 89/104 [00:19<00:03,  4.68it/s, loss=0.0453, v_num=0, reduced_train_loss=0.00767, global_step=1574.0, val_loss=0.124]\n",
      "Epoch 20:  85%|████████▍ | 88/104 [00:18<00:03,  4.75it/s, loss=0.0289, v_num=0, reduced_train_loss=0.00487, global_step=1574.0, val_loss=0.0666]\n",
      "\n",
      "Epoch 20:  48%|████▊     | 50/104 [00:11<00:12,  4.30it/s, loss=0.0909, v_num=0, reduced_train_loss=0.0237, global_step=1549.0, val_loss=0.122]6]\n",
      "Epoch 20:  88%|████████▊ | 91/104 [00:19<00:02,  4.72it/s, loss=0.0453, v_num=0, reduced_train_loss=0.00767, global_step=1574.0, val_loss=0.124]\n",
      "Epoch 20:  87%|████████▋ | 90/104 [00:18<00:02,  4.80it/s, loss=0.0289, v_num=0, reduced_train_loss=0.00487, global_step=1574.0, val_loss=0.0666]\n",
      "Epoch 20:  88%|████████▊ | 92/104 [00:19<00:02,  4.74it/s, loss=0.0453, v_num=0, reduced_train_loss=0.00767, global_step=1574.0, val_loss=0.124]\n",
      "Epoch 20:  49%|████▉     | 51/104 [00:11<00:12,  4.30it/s, loss=0.0784, v_num=0, reduced_train_loss=0.0759, global_step=1550.0, val_loss=0.122]6]\n",
      "Epoch 20:  89%|████████▉ | 93/104 [00:19<00:02,  4.76it/s, loss=0.0453, v_num=0, reduced_train_loss=0.00767, global_step=1574.0, val_loss=0.124]\n",
      "Epoch 20:  88%|████████▊ | 92/104 [00:19<00:02,  4.84it/s, loss=0.0289, v_num=0, reduced_train_loss=0.00487, global_step=1574.0, val_loss=0.0666]\n",
      "Epoch 20:  50%|█████     | 52/104 [00:12<00:12,  4.30it/s, loss=0.0784, v_num=0, reduced_train_loss=0.0759, global_step=1550.0, val_loss=0.122]]\n",
      "Epoch 20:  50%|█████     | 52/104 [00:12<00:12,  4.30it/s, loss=0.0858, v_num=0, reduced_train_loss=0.150, global_step=1551.0, val_loss=0.122] 6]\n",
      "Epoch 20:  91%|█████████▏| 95/104 [00:19<00:01,  4.81it/s, loss=0.0453, v_num=0, reduced_train_loss=0.00767, global_step=1574.0, val_loss=0.124]\n",
      "Epoch 20:  51%|█████     | 53/104 [00:12<00:11,  4.30it/s, loss=0.0884, v_num=0, reduced_train_loss=0.0625, global_step=1552.0, val_loss=0.122]6]\n",
      "Epoch 20:  92%|█████████▏| 96/104 [00:19<00:01,  4.83it/s, loss=0.0453, v_num=0, reduced_train_loss=0.00767, global_step=1574.0, val_loss=0.124]\n",
      "Epoch 20:  91%|█████████▏| 95/104 [00:19<00:01,  4.90it/s, loss=0.0289, v_num=0, reduced_train_loss=0.00487, global_step=1574.0, val_loss=0.0666]\n",
      "\n",
      "Epoch 20:  52%|█████▏    | 54/104 [00:12<00:11,  4.30it/s, loss=0.0862, v_num=0, reduced_train_loss=0.00536, global_step=1553.0, val_loss=0.122]]\n",
      "Epoch 20:  94%|█████████▍| 98/104 [00:20<00:01,  4.87it/s, loss=0.0453, v_num=0, reduced_train_loss=0.00767, global_step=1574.0, val_loss=0.124]\n",
      "Epoch 20:  93%|█████████▎| 97/104 [00:19<00:01,  4.94it/s, loss=0.0289, v_num=0, reduced_train_loss=0.00487, global_step=1574.0, val_loss=0.0666]\n",
      "\n",
      "Epoch 20:  53%|█████▎    | 55/104 [00:12<00:11,  4.30it/s, loss=0.0877, v_num=0, reduced_train_loss=0.040, global_step=1554.0, val_loss=0.122]  ]\n",
      "\n",
      "Epoch 20:  95%|█████████▌| 99/104 [00:19<00:01,  4.98it/s, loss=0.0289, v_num=0, reduced_train_loss=0.00487, global_step=1574.0, val_loss=0.0666]\n",
      "Epoch 20:  97%|█████████▋| 101/104 [00:20<00:00,  4.92it/s, loss=0.0453, v_num=0, reduced_train_loss=0.00767, global_step=1574.0, val_loss=0.124]\n",
      "Epoch 20:  54%|█████▍    | 56/104 [00:13<00:11,  4.30it/s, loss=0.0869, v_num=0, reduced_train_loss=0.00426, global_step=1555.0, val_loss=0.122]6]\n",
      "Validation DataLoader 0:  93%|█████████▎| 27/29 [00:03<00:00,  8.00it/s]\u001b[A\n",
      "Epoch 20:  97%|█████████▋| 101/104 [00:20<00:00,  5.02it/s, loss=0.0289, v_num=0, reduced_train_loss=0.00487, global_step=1574.0, val_loss=0.0666]\n",
      "Validation DataLoader 0:  97%|█████████▋| 28/29 [00:03<00:00,  8.01it/s]\u001b[A\n",
      "Epoch 20:  55%|█████▍    | 57/104 [00:13<00:10,  4.30it/s, loss=0.0867, v_num=0, reduced_train_loss=0.037, global_step=1556.0, val_loss=0.122]  6]\n",
      "Epoch 20: 100%|██████████| 104/104 [00:20<00:00,  5.00it/s, loss=0.0453, v_num=0, reduced_train_loss=0.00767, global_step=1574.0, val_loss=0.124]2023-05-03 22:03:16,100 - root - INFO - val_loss: 0.08515377342700958\n",
      "Epoch 20: 100%|██████████| 104/104 [00:20<00:00,  5.00it/s, loss=0.0453, v_num=0, reduced_train_loss=0.00767, global_step=1574.0, val_loss=0.0852]\n",
      "Epoch 21:   0%|          | 0/104 [00:00<?, ?it/s, loss=0.0453, v_num=0, reduced_train_loss=0.00767, global_step=1574.0, val_loss=0.0852]          \n",
      "Epoch 20:  99%|█████████▉| 103/104 [00:20<00:00,  5.06it/s, loss=0.0289, v_num=0, reduced_train_loss=0.00487, global_step=1574.0, val_loss=0.0666]\n",
      "Epoch 20: 100%|██████████| 104/104 [00:20<00:00,  5.09it/s, loss=0.0289, v_num=0, reduced_train_loss=0.00487, global_step=1574.0, val_loss=0.0666]2023-05-03 22:03:16,223 - root - INFO - val_loss: 0.058985598385334015\n",
      "Epoch 20: 100%|██████████| 104/104 [00:20<00:00,  5.09it/s, loss=0.0289, v_num=0, reduced_train_loss=0.00487, global_step=1574.0, val_loss=0.059] \n",
      "Epoch 21:   0%|          | 0/104 [00:00<?, ?it/s, loss=0.0289, v_num=0, reduced_train_loss=0.00487, global_step=1574.0, val_loss=0.059]          "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 22:03:16.100631 140522389899072 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.08515377342700958\n",
      "I0503 22:03:16.223928 140649257482048 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.058985598385334015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20:  72%|███████▏  | 75/104 [00:17<00:06,  4.28it/s, loss=0.0899, v_num=0, reduced_train_loss=0.00336, global_step=1574.0, val_loss=0.122]] \n",
      "Epoch 21:  17%|█▋        | 18/104 [00:04<00:19,  4.32it/s, loss=0.00879, v_num=0, reduced_train_loss=0.000545, global_step=1592.0, val_loss=0.059]\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 21:  18%|█▊        | 19/104 [00:04<00:19,  4.33it/s, loss=0.0151, v_num=0, reduced_train_loss=0.0316, global_step=1593.0, val_loss=0.0852] \n",
      "Epoch 21:  18%|█▊        | 19/104 [00:04<00:19,  4.32it/s, loss=0.00878, v_num=0, reduced_train_loss=0.000244, global_step=1593.0, val_loss=0.059]\n",
      "Epoch 21:  19%|█▉        | 20/104 [00:04<00:19,  4.33it/s, loss=0.0148, v_num=0, reduced_train_loss=0.00184, global_step=1594.0, val_loss=0.0852]\n",
      "Epoch 21:  19%|█▉        | 20/104 [00:04<00:19,  4.32it/s, loss=0.00856, v_num=0, reduced_train_loss=0.000602, global_step=1594.0, val_loss=0.059]\n",
      "Epoch 21:  20%|██        | 21/104 [00:04<00:19,  4.34it/s, loss=0.0159, v_num=0, reduced_train_loss=0.0229, global_step=1595.0, val_loss=0.0852] \n",
      "Epoch 21:  21%|██        | 22/104 [00:05<00:18,  4.34it/s, loss=0.0216, v_num=0, reduced_train_loss=0.131, global_step=1596.0, val_loss=0.0852] ] \n",
      "Epoch 21:  21%|██        | 22/104 [00:05<00:18,  4.33it/s, loss=0.0213, v_num=0, reduced_train_loss=0.247, global_step=1596.0, val_loss=0.059]   \n",
      "Epoch 21:  22%|██▏       | 23/104 [00:05<00:18,  4.34it/s, loss=0.0225, v_num=0, reduced_train_loss=0.069, global_step=1597.0, val_loss=0.0852]]\n",
      "Epoch 21:  22%|██▏       | 23/104 [00:05<00:18,  4.33it/s, loss=0.0209, v_num=0, reduced_train_loss=0.00301, global_step=1597.0, val_loss=0.059]\n",
      "Epoch 21:  23%|██▎       | 24/104 [00:05<00:18,  4.34it/s, loss=0.0223, v_num=0, reduced_train_loss=0.00575, global_step=1598.0, val_loss=0.0852]\n",
      "Epoch 21:  23%|██▎       | 24/104 [00:05<00:18,  4.33it/s, loss=0.0274, v_num=0, reduced_train_loss=0.133, global_step=1598.0, val_loss=0.059]  \n",
      "Epoch 21:  24%|██▍       | 25/104 [00:05<00:18,  4.35it/s, loss=0.0227, v_num=0, reduced_train_loss=0.00851, global_step=1599.0, val_loss=0.0852]\n",
      "Epoch 21:  25%|██▌       | 26/104 [00:05<00:17,  4.35it/s, loss=0.0226, v_num=0, reduced_train_loss=0.000939, global_step=1600.0, val_loss=0.0852]\n",
      "Epoch 20:  85%|████████▍ | 88/104 [00:19<00:03,  4.57it/s, loss=0.0899, v_num=0, reduced_train_loss=0.00336, global_step=1574.0, val_loss=0.122]\n",
      "Epoch 21:  26%|██▌       | 27/104 [00:06<00:17,  4.35it/s, loss=0.0225, v_num=0, reduced_train_loss=0.0096, global_step=1601.0, val_loss=0.0852]  \n",
      "Epoch 21:  26%|██▌       | 27/104 [00:06<00:17,  4.34it/s, loss=0.028, v_num=0, reduced_train_loss=0.00825, global_step=1601.0, val_loss=0.059]]\n",
      "Epoch 21:  27%|██▋       | 28/104 [00:06<00:17,  4.35it/s, loss=0.0228, v_num=0, reduced_train_loss=0.0094, global_step=1602.0, val_loss=0.0852]\n",
      "Epoch 21:  27%|██▋       | 28/104 [00:06<00:17,  4.34it/s, loss=0.028, v_num=0, reduced_train_loss=0.00495, global_step=1602.0, val_loss=0.059]]\n",
      "Epoch 21:  28%|██▊       | 29/104 [00:06<00:17,  4.36it/s, loss=0.0228, v_num=0, reduced_train_loss=0.000444, global_step=1603.0, val_loss=0.0852]\n",
      "Epoch 21:  28%|██▊       | 29/104 [00:06<00:17,  4.34it/s, loss=0.0279, v_num=0, reduced_train_loss=0.00173, global_step=1603.0, val_loss=0.059]\n",
      "Epoch 21:  29%|██▉       | 30/104 [00:06<00:16,  4.36it/s, loss=0.039, v_num=0, reduced_train_loss=0.325, global_step=1604.0, val_loss=0.0852]    \n",
      "Epoch 21:  30%|██▉       | 31/104 [00:07<00:16,  4.36it/s, loss=0.0327, v_num=0, reduced_train_loss=0.00234, global_step=1605.0, val_loss=0.0852]\n",
      "Epoch 20:  93%|█████████▎| 97/104 [00:20<00:01,  4.76it/s, loss=0.0899, v_num=0, reduced_train_loss=0.00336, global_step=1574.0, val_loss=0.122]\n",
      "Epoch 21:  31%|███       | 32/104 [00:07<00:16,  4.36it/s, loss=0.033, v_num=0, reduced_train_loss=0.00926, global_step=1606.0, val_loss=0.0852] \n",
      "Epoch 21:  31%|███       | 32/104 [00:07<00:16,  4.34it/s, loss=0.0271, v_num=0, reduced_train_loss=0.000134, global_step=1606.0, val_loss=0.059]\n",
      "Epoch 21:  32%|███▏      | 33/104 [00:07<00:16,  4.36it/s, loss=0.0331, v_num=0, reduced_train_loss=0.00574, global_step=1607.0, val_loss=0.0852]\n",
      "Epoch 21:  32%|███▏      | 33/104 [00:07<00:16,  4.35it/s, loss=0.0289, v_num=0, reduced_train_loss=0.0366, global_step=1607.0, val_loss=0.059]  \n",
      "Epoch 21:  33%|███▎      | 34/104 [00:07<00:16,  4.36it/s, loss=0.0391, v_num=0, reduced_train_loss=0.136, global_step=1608.0, val_loss=0.0852]  \n",
      "Epoch 20:  99%|█████████▉| 103/104 [00:21<00:00,  4.87it/s, loss=0.0899, v_num=0, reduced_train_loss=0.00336, global_step=1574.0, val_loss=0.122]\n",
      "Epoch 20: 100%|██████████| 104/104 [00:21<00:00,  4.90it/s, loss=0.0899, v_num=0, reduced_train_loss=0.00336, global_step=1574.0, val_loss=0.122]2023-05-03 22:03:24,043 - root - INFO - val_loss: 0.09656734764575958\n",
      "Epoch 20: 100%|██████████| 104/104 [00:21<00:00,  4.90it/s, loss=0.0899, v_num=0, reduced_train_loss=0.00336, global_step=1574.0, val_loss=0.0966]\n",
      "Epoch 21:   0%|          | 0/104 [00:00<?, ?it/s, loss=0.0899, v_num=0, reduced_train_loss=0.00336, global_step=1574.0, val_loss=0.0966]          "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 22:03:24.043118 139755592410944 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.09656734764575958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21:  72%|███████▏  | 75/104 [00:17<00:06,  4.39it/s, loss=0.0282, v_num=0, reduced_train_loss=0.000323, global_step=1649.0, val_loss=0.059] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 21:  72%|███████▏  | 75/104 [00:17<00:06,  4.34it/s, loss=0.0453, v_num=0, reduced_train_loss=0.111, global_step=1649.0, val_loss=0.0852]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 21:  38%|███▊      | 40/104 [00:09<00:15,  4.21it/s, loss=0.0755, v_num=0, reduced_train_loss=0.0181, global_step=1614.0, val_loss=0.0966] \n",
      "Epoch 21:  73%|███████▎  | 76/104 [00:17<00:06,  4.35it/s, loss=0.0453, v_num=0, reduced_train_loss=0.111, global_step=1649.0, val_loss=0.0852]\n",
      "Epoch 21:  74%|███████▍  | 77/104 [00:17<00:06,  4.43it/s, loss=0.0282, v_num=0, reduced_train_loss=0.000323, global_step=1649.0, val_loss=0.059]\n",
      "Epoch 21:  74%|███████▍  | 77/104 [00:17<00:06,  4.38it/s, loss=0.0453, v_num=0, reduced_train_loss=0.111, global_step=1649.0, val_loss=0.0852]\n",
      "Epoch 21:  39%|███▉      | 41/104 [00:09<00:14,  4.21it/s, loss=0.0875, v_num=0, reduced_train_loss=0.260, global_step=1615.0, val_loss=0.0966] ]\n",
      "Epoch 21:  75%|███████▌  | 78/104 [00:17<00:05,  4.40it/s, loss=0.0453, v_num=0, reduced_train_loss=0.111, global_step=1649.0, val_loss=0.0852]\n",
      "Epoch 21:  76%|███████▌  | 79/104 [00:17<00:05,  4.48it/s, loss=0.0282, v_num=0, reduced_train_loss=0.000323, global_step=1649.0, val_loss=0.059]\n",
      "Epoch 21:  76%|███████▌  | 79/104 [00:17<00:05,  4.43it/s, loss=0.0453, v_num=0, reduced_train_loss=0.111, global_step=1649.0, val_loss=0.0852]\n",
      "Epoch 21:  40%|████      | 42/104 [00:09<00:14,  4.21it/s, loss=0.0848, v_num=0, reduced_train_loss=0.0253, global_step=1616.0, val_loss=0.0966]]\n",
      "Epoch 21:  77%|███████▋  | 80/104 [00:17<00:05,  4.45it/s, loss=0.0453, v_num=0, reduced_train_loss=0.111, global_step=1649.0, val_loss=0.0852]\n",
      "Epoch 21:  78%|███████▊  | 81/104 [00:17<00:05,  4.53it/s, loss=0.0282, v_num=0, reduced_train_loss=0.000323, global_step=1649.0, val_loss=0.059]\n",
      "Epoch 21:  78%|███████▊  | 81/104 [00:18<00:05,  4.48it/s, loss=0.0453, v_num=0, reduced_train_loss=0.111, global_step=1649.0, val_loss=0.0852]\n",
      "Epoch 21:  41%|████▏     | 43/104 [00:10<00:14,  4.21it/s, loss=0.0768, v_num=0, reduced_train_loss=0.195, global_step=1617.0, val_loss=0.0966] ]\n",
      "Epoch 21:  79%|███████▉  | 82/104 [00:18<00:04,  4.50it/s, loss=0.0453, v_num=0, reduced_train_loss=0.111, global_step=1649.0, val_loss=0.0852]\n",
      "Epoch 21:  80%|███████▉  | 83/104 [00:18<00:04,  4.57it/s, loss=0.0282, v_num=0, reduced_train_loss=0.000323, global_step=1649.0, val_loss=0.059]\n",
      "Epoch 21:  42%|████▏     | 44/104 [00:10<00:14,  4.21it/s, loss=0.0768, v_num=0, reduced_train_loss=0.00336, global_step=1618.0, val_loss=0.0966]\n",
      "Epoch 21:  81%|████████  | 84/104 [00:18<00:04,  4.60it/s, loss=0.0282, v_num=0, reduced_train_loss=0.000323, global_step=1649.0, val_loss=0.059]\n",
      "Epoch 21:  81%|████████  | 84/104 [00:18<00:04,  4.55it/s, loss=0.0453, v_num=0, reduced_train_loss=0.111, global_step=1649.0, val_loss=0.0852]\n",
      "Epoch 21:  82%|████████▏ | 85/104 [00:18<00:04,  4.62it/s, loss=0.0282, v_num=0, reduced_train_loss=0.000323, global_step=1649.0, val_loss=0.059]\n",
      "Epoch 21:  43%|████▎     | 45/104 [00:10<00:14,  4.21it/s, loss=0.0779, v_num=0, reduced_train_loss=0.0809, global_step=1619.0, val_loss=0.0966] \n",
      "Epoch 21:  83%|████████▎ | 86/104 [00:18<00:03,  4.64it/s, loss=0.0282, v_num=0, reduced_train_loss=0.000323, global_step=1649.0, val_loss=0.059]\n",
      "Epoch 21:  83%|████████▎ | 86/104 [00:18<00:03,  4.60it/s, loss=0.0453, v_num=0, reduced_train_loss=0.111, global_step=1649.0, val_loss=0.0852]\n",
      "Epoch 21:  84%|████████▎ | 87/104 [00:18<00:03,  4.66it/s, loss=0.0282, v_num=0, reduced_train_loss=0.000323, global_step=1649.0, val_loss=0.059]\n",
      "Epoch 21:  44%|████▍     | 46/104 [00:10<00:13,  4.21it/s, loss=0.0819, v_num=0, reduced_train_loss=0.163, global_step=1620.0, val_loss=0.0966] \n",
      "Epoch 21:  85%|████████▍ | 88/104 [00:18<00:03,  4.68it/s, loss=0.0282, v_num=0, reduced_train_loss=0.000323, global_step=1649.0, val_loss=0.059]\n",
      "Epoch 21:  85%|████████▍ | 88/104 [00:18<00:03,  4.64it/s, loss=0.0453, v_num=0, reduced_train_loss=0.111, global_step=1649.0, val_loss=0.0852]\n",
      "Epoch 21:  86%|████████▌ | 89/104 [00:18<00:03,  4.71it/s, loss=0.0282, v_num=0, reduced_train_loss=0.000323, global_step=1649.0, val_loss=0.059]\n",
      "Epoch 21:  45%|████▌     | 47/104 [00:11<00:13,  4.22it/s, loss=0.0818, v_num=0, reduced_train_loss=0.020, global_step=1621.0, val_loss=0.0966]\n",
      "Epoch 21:  87%|████████▋ | 90/104 [00:19<00:02,  4.73it/s, loss=0.0282, v_num=0, reduced_train_loss=0.000323, global_step=1649.0, val_loss=0.059]\n",
      "Epoch 21:  87%|████████▋ | 90/104 [00:19<00:02,  4.69it/s, loss=0.0453, v_num=0, reduced_train_loss=0.111, global_step=1649.0, val_loss=0.0852]\n",
      "Epoch 21:  88%|████████▊ | 91/104 [00:19<00:02,  4.75it/s, loss=0.0282, v_num=0, reduced_train_loss=0.000323, global_step=1649.0, val_loss=0.059]\n",
      "Epoch 21:  46%|████▌     | 48/104 [00:11<00:13,  4.22it/s, loss=0.104, v_num=0, reduced_train_loss=0.441, global_step=1622.0, val_loss=0.0966] \n",
      "Epoch 21:  88%|████████▊ | 92/104 [00:19<00:02,  4.77it/s, loss=0.0282, v_num=0, reduced_train_loss=0.000323, global_step=1649.0, val_loss=0.059]\n",
      "Epoch 21:  88%|████████▊ | 92/104 [00:19<00:02,  4.73it/s, loss=0.0453, v_num=0, reduced_train_loss=0.111, global_step=1649.0, val_loss=0.0852]\n",
      "Epoch 21:  47%|████▋     | 49/104 [00:11<00:13,  4.22it/s, loss=0.103, v_num=0, reduced_train_loss=0.00473, global_step=1623.0, val_loss=0.0966]]\n",
      "Epoch 21:  89%|████████▉ | 93/104 [00:19<00:02,  4.75it/s, loss=0.0453, v_num=0, reduced_train_loss=0.111, global_step=1649.0, val_loss=0.0852]\n",
      "Epoch 21:  90%|█████████ | 94/104 [00:19<00:02,  4.81it/s, loss=0.0282, v_num=0, reduced_train_loss=0.000323, global_step=1649.0, val_loss=0.059]\n",
      "Epoch 21:  90%|█████████ | 94/104 [00:19<00:02,  4.77it/s, loss=0.0453, v_num=0, reduced_train_loss=0.111, global_step=1649.0, val_loss=0.0852]\n",
      "Epoch 21:  48%|████▊     | 50/104 [00:11<00:12,  4.22it/s, loss=0.104, v_num=0, reduced_train_loss=0.042, global_step=1624.0, val_loss=0.0966]  ]\n",
      "Epoch 21:  91%|█████████▏| 95/104 [00:19<00:01,  4.79it/s, loss=0.0453, v_num=0, reduced_train_loss=0.111, global_step=1649.0, val_loss=0.0852]\n",
      "Epoch 21:  92%|█████████▏| 96/104 [00:19<00:01,  4.85it/s, loss=0.0282, v_num=0, reduced_train_loss=0.000323, global_step=1649.0, val_loss=0.059]\n",
      "Epoch 21:  92%|█████████▏| 96/104 [00:19<00:01,  4.81it/s, loss=0.0453, v_num=0, reduced_train_loss=0.111, global_step=1649.0, val_loss=0.0852]\n",
      "Epoch 21:  49%|████▉     | 51/104 [00:12<00:12,  4.22it/s, loss=0.1, v_num=0, reduced_train_loss=0.00409, global_step=1625.0, val_loss=0.0966]59]\n",
      "Epoch 21:  93%|█████████▎| 97/104 [00:20<00:01,  4.84it/s, loss=0.0453, v_num=0, reduced_train_loss=0.111, global_step=1649.0, val_loss=0.0852]\n",
      "Epoch 21:  94%|█████████▍| 98/104 [00:20<00:01,  4.89it/s, loss=0.0282, v_num=0, reduced_train_loss=0.000323, global_step=1649.0, val_loss=0.059]\n",
      "Epoch 21:  94%|█████████▍| 98/104 [00:20<00:01,  4.86it/s, loss=0.0453, v_num=0, reduced_train_loss=0.111, global_step=1649.0, val_loss=0.0852]\n",
      "Epoch 21:  50%|█████     | 52/104 [00:12<00:12,  4.21it/s, loss=0.0809, v_num=0, reduced_train_loss=0.0637, global_step=1626.0, val_loss=0.0966]]\n",
      "Epoch 21:  95%|█████████▌| 99/104 [00:20<00:01,  4.88it/s, loss=0.0453, v_num=0, reduced_train_loss=0.111, global_step=1649.0, val_loss=0.0852]\n",
      "Epoch 21:  96%|█████████▌| 100/104 [00:20<00:00,  4.93it/s, loss=0.0282, v_num=0, reduced_train_loss=0.000323, global_step=1649.0, val_loss=0.059]\n",
      "Epoch 21:  51%|█████     | 53/104 [00:12<00:12,  4.21it/s, loss=0.0789, v_num=0, reduced_train_loss=0.0192, global_step=1627.0, val_loss=0.0966]\n",
      "Epoch 21:  97%|█████████▋| 101/104 [00:20<00:00,  4.95it/s, loss=0.0282, v_num=0, reduced_train_loss=0.000323, global_step=1649.0, val_loss=0.059]\n",
      "Epoch 21:  97%|█████████▋| 101/104 [00:20<00:00,  4.91it/s, loss=0.0453, v_num=0, reduced_train_loss=0.111, global_step=1649.0, val_loss=0.0852]\n",
      "Epoch 21:  98%|█████████▊| 102/104 [00:20<00:00,  4.97it/s, loss=0.0282, v_num=0, reduced_train_loss=0.000323, global_step=1649.0, val_loss=0.059]\n",
      "Epoch 21:  52%|█████▏    | 54/104 [00:12<00:11,  4.21it/s, loss=0.0705, v_num=0, reduced_train_loss=0.0016, global_step=1628.0, val_loss=0.0966]\n",
      "Epoch 21:  99%|█████████▉| 103/104 [00:20<00:00,  4.99it/s, loss=0.0282, v_num=0, reduced_train_loss=0.000323, global_step=1649.0, val_loss=0.059]\n",
      "Epoch 21:  99%|█████████▉| 103/104 [00:20<00:00,  4.95it/s, loss=0.0453, v_num=0, reduced_train_loss=0.111, global_step=1649.0, val_loss=0.0852]\n",
      "Epoch 21: 100%|██████████| 104/104 [00:20<00:00,  5.02it/s, loss=0.0282, v_num=0, reduced_train_loss=0.000323, global_step=1649.0, val_loss=0.059]2023-05-03 22:03:36,949 - root - INFO - val_loss: 0.06703179329633713\n",
      "Epoch 21: 100%|██████████| 104/104 [00:20<00:00,  5.02it/s, loss=0.0282, v_num=0, reduced_train_loss=0.000323, global_step=1649.0, val_loss=0.067]\n",
      "Epoch 22:   0%|          | 0/104 [00:00<?, ?it/s, loss=0.0282, v_num=0, reduced_train_loss=0.000323, global_step=1649.0, val_loss=0.067]          \n",
      "Epoch 21: 100%|██████████| 104/104 [00:20<00:00,  4.99it/s, loss=0.0453, v_num=0, reduced_train_loss=0.111, global_step=1649.0, val_loss=0.0852]2023-05-03 22:03:36,967 - root - INFO - val_loss: 0.12073282152414322\n",
      "Epoch 21: 100%|██████████| 104/104 [00:20<00:00,  4.98it/s, loss=0.0453, v_num=0, reduced_train_loss=0.111, global_step=1649.0, val_loss=0.121] \n",
      "Epoch 22:   0%|          | 0/104 [00:00<?, ?it/s, loss=0.0453, v_num=0, reduced_train_loss=0.111, global_step=1649.0, val_loss=0.121]          "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 22:03:36.949040 140649257482048 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.06703179329633713\n",
      "I0503 22:03:36.967534 140522389899072 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.12073282152414322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21:  72%|███████▏  | 75/104 [00:17<00:06,  4.23it/s, loss=0.129, v_num=0, reduced_train_loss=0.00104, global_step=1649.0, val_loss=0.0966] \n",
      "Epoch 22:  20%|██        | 21/104 [00:04<00:19,  4.31it/s, loss=0.0151, v_num=0, reduced_train_loss=0.000223, global_step=1670.0, val_loss=0.067]\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 22:  20%|██        | 21/104 [00:04<00:19,  4.22it/s, loss=0.018, v_num=0, reduced_train_loss=0.0169, global_step=1670.0, val_loss=0.121]\n",
      "Epoch 22:  21%|██        | 22/104 [00:05<00:19,  4.31it/s, loss=0.0152, v_num=0, reduced_train_loss=0.00456, global_step=1671.0, val_loss=0.067] \n",
      "Epoch 22:  21%|██        | 22/104 [00:05<00:19,  4.21it/s, loss=0.018, v_num=0, reduced_train_loss=0.00143, global_step=1671.0, val_loss=0.121]]\n",
      "Epoch 22:  22%|██▏       | 23/104 [00:05<00:18,  4.32it/s, loss=0.0152, v_num=0, reduced_train_loss=0.000811, global_step=1672.0, val_loss=0.067]\n",
      "Epoch 22:  22%|██▏       | 23/104 [00:05<00:19,  4.22it/s, loss=0.018, v_num=0, reduced_train_loss=0.000436, global_step=1672.0, val_loss=0.121]\n",
      "Epoch 22:  23%|██▎       | 24/104 [00:05<00:18,  4.32it/s, loss=0.0154, v_num=0, reduced_train_loss=0.00433, global_step=1673.0, val_loss=0.067] \n",
      "Epoch 22:  24%|██▍       | 25/104 [00:05<00:18,  4.32it/s, loss=0.0153, v_num=0, reduced_train_loss=0.000177, global_step=1674.0, val_loss=0.067]\n",
      "Epoch 21:  79%|███████▉  | 82/104 [00:18<00:05,  4.38it/s, loss=0.129, v_num=0, reduced_train_loss=0.00104, global_step=1649.0, val_loss=0.0966]\n",
      "Epoch 22:  25%|██▌       | 26/104 [00:06<00:18,  4.33it/s, loss=0.0153, v_num=0, reduced_train_loss=0.000284, global_step=1675.0, val_loss=0.067]\n",
      "Epoch 22:  25%|██▌       | 26/104 [00:06<00:18,  4.23it/s, loss=0.0277, v_num=0, reduced_train_loss=0.00136, global_step=1675.0, val_loss=0.121]\n",
      "Epoch 22:  26%|██▌       | 27/104 [00:06<00:17,  4.34it/s, loss=0.0153, v_num=0, reduced_train_loss=0.000453, global_step=1676.0, val_loss=0.067]\n",
      "Epoch 22:  26%|██▌       | 27/104 [00:06<00:18,  4.24it/s, loss=0.0397, v_num=0, reduced_train_loss=0.241, global_step=1676.0, val_loss=0.121]  \n",
      "Epoch 22:  27%|██▋       | 28/104 [00:06<00:17,  4.35it/s, loss=0.0149, v_num=0, reduced_train_loss=0.000308, global_step=1677.0, val_loss=0.067]\n",
      "Epoch 22:  28%|██▊       | 29/104 [00:06<00:17,  4.35it/s, loss=0.0149, v_num=0, reduced_train_loss=0.000258, global_step=1678.0, val_loss=0.067]\n",
      "Epoch 21:  86%|████████▌ | 89/104 [00:19<00:03,  4.55it/s, loss=0.129, v_num=0, reduced_train_loss=0.00104, global_step=1649.0, val_loss=0.0966]\n",
      "Epoch 22:  29%|██▉       | 30/104 [00:06<00:16,  4.36it/s, loss=0.0148, v_num=0, reduced_train_loss=0.000306, global_step=1679.0, val_loss=0.067]\n",
      "Epoch 21:  88%|████████▊ | 91/104 [00:19<00:02,  4.59it/s, loss=0.129, v_num=0, reduced_train_loss=0.00104, global_step=1649.0, val_loss=0.0966]\n",
      "Epoch 22:  30%|██▉       | 31/104 [00:07<00:16,  4.36it/s, loss=0.00329, v_num=0, reduced_train_loss=0.000308, global_step=1680.0, val_loss=0.067]\n",
      "Epoch 21:  89%|████████▉ | 93/104 [00:20<00:02,  4.63it/s, loss=0.129, v_num=0, reduced_train_loss=0.00104, global_step=1649.0, val_loss=0.0966]\n",
      "Epoch 22:  31%|███       | 32/104 [00:07<00:16,  4.37it/s, loss=0.00301, v_num=0, reduced_train_loss=0.000189, global_step=1681.0, val_loss=0.067]\n",
      "Epoch 21:  91%|█████████▏| 95/104 [00:20<00:01,  4.68it/s, loss=0.129, v_num=0, reduced_train_loss=0.00104, global_step=1649.0, val_loss=0.0966]\n",
      "Epoch 22:  32%|███▏      | 33/104 [00:07<00:16,  4.37it/s, loss=0.0105, v_num=0, reduced_train_loss=0.150, global_step=1682.0, val_loss=0.067]    \n",
      "Epoch 22:  32%|███▏      | 33/104 [00:07<00:16,  4.25it/s, loss=0.048, v_num=0, reduced_train_loss=0.145, global_step=1682.0, val_loss=0.121]   7]\n",
      "Epoch 21:  94%|█████████▍| 98/104 [00:20<00:01,  4.74it/s, loss=0.129, v_num=0, reduced_train_loss=0.00104, global_step=1649.0, val_loss=0.0966]\n",
      "Epoch 22:  33%|███▎      | 34/104 [00:07<00:16,  4.26it/s, loss=0.0482, v_num=0, reduced_train_loss=0.0177, global_step=1683.0, val_loss=0.121]   \n",
      "Epoch 21:  96%|█████████▌| 100/104 [00:20<00:00,  4.78it/s, loss=0.129, v_num=0, reduced_train_loss=0.00104, global_step=1649.0, val_loss=0.0966]\n",
      "Epoch 22:  34%|███▎      | 35/104 [00:08<00:16,  4.27it/s, loss=0.0481, v_num=0, reduced_train_loss=0.000321, global_step=1684.0, val_loss=0.121]\n",
      "Epoch 21:  98%|█████████▊| 102/104 [00:21<00:00,  4.82it/s, loss=0.129, v_num=0, reduced_train_loss=0.00104, global_step=1649.0, val_loss=0.0966]\n",
      "Epoch 22:  36%|███▌      | 37/104 [00:08<00:15,  4.39it/s, loss=0.0102, v_num=0, reduced_train_loss=0.00123, global_step=1686.0, val_loss=0.067]]\n",
      "Epoch 22:  35%|███▍      | 36/104 [00:08<00:15,  4.27it/s, loss=0.0481, v_num=0, reduced_train_loss=0.00103, global_step=1685.0, val_loss=0.121] 2023-05-03 22:03:45,414 - root - INFO - val_loss: 0.10958194732666016\n",
      "Epoch 21: 100%|██████████| 104/104 [00:21<00:00,  4.87it/s, loss=0.129, v_num=0, reduced_train_loss=0.00104, global_step=1649.0, val_loss=0.110] \n",
      "Epoch 22:   0%|          | 0/104 [00:00<?, ?it/s, loss=0.129, v_num=0, reduced_train_loss=0.00104, global_step=1649.0, val_loss=0.110]          "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 22:03:45.414415 139755592410944 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.10958194732666016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22:  72%|███████▏  | 75/104 [00:16<00:06,  4.46it/s, loss=0.0408, v_num=0, reduced_train_loss=0.000156, global_step=1724.0, val_loss=0.067]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 22:  35%|███▍      | 36/104 [00:08<00:15,  4.28it/s, loss=0.0619, v_num=0, reduced_train_loss=0.0695, global_step=1685.0, val_loss=0.110] \n",
      "Epoch 22:  36%|███▌      | 37/104 [00:08<00:15,  4.28it/s, loss=0.0618, v_num=0, reduced_train_loss=0.00377, global_step=1686.0, val_loss=0.110]]\n",
      "Epoch 22:  74%|███████▍  | 77/104 [00:17<00:06,  4.49it/s, loss=0.0408, v_num=0, reduced_train_loss=0.000156, global_step=1724.0, val_loss=0.067]\n",
      "Epoch 22:  72%|███████▏  | 75/104 [00:17<00:06,  4.35it/s, loss=0.0373, v_num=0, reduced_train_loss=0.035, global_step=1724.0, val_loss=0.121]  ]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 22:  37%|███▋      | 38/104 [00:08<00:15,  4.28it/s, loss=0.0602, v_num=0, reduced_train_loss=0.00904, global_step=1687.0, val_loss=0.110]\n",
      "Epoch 22:  76%|███████▌  | 79/104 [00:17<00:05,  4.55it/s, loss=0.0408, v_num=0, reduced_train_loss=0.000156, global_step=1724.0, val_loss=0.067]\n",
      "Epoch 22:  73%|███████▎  | 76/104 [00:17<00:06,  4.36it/s, loss=0.0373, v_num=0, reduced_train_loss=0.035, global_step=1724.0, val_loss=0.121]\n",
      "Epoch 22:  38%|███▊      | 39/104 [00:09<00:15,  4.28it/s, loss=0.0607, v_num=0, reduced_train_loss=0.0305, global_step=1688.0, val_loss=0.110] ]\n",
      "Epoch 22:  74%|███████▍  | 77/104 [00:17<00:06,  4.39it/s, loss=0.0373, v_num=0, reduced_train_loss=0.035, global_step=1724.0, val_loss=0.121]\n",
      "Epoch 22:  78%|███████▊  | 81/104 [00:17<00:05,  4.60it/s, loss=0.0408, v_num=0, reduced_train_loss=0.000156, global_step=1724.0, val_loss=0.067]\n",
      "Epoch 22:  75%|███████▌  | 78/104 [00:17<00:05,  4.41it/s, loss=0.0373, v_num=0, reduced_train_loss=0.035, global_step=1724.0, val_loss=0.121]\n",
      "Epoch 22:  38%|███▊      | 40/104 [00:09<00:14,  4.28it/s, loss=0.0535, v_num=0, reduced_train_loss=0.00974, global_step=1689.0, val_loss=0.110]]\n",
      "Epoch 22:  76%|███████▌  | 79/104 [00:17<00:05,  4.44it/s, loss=0.0373, v_num=0, reduced_train_loss=0.035, global_step=1724.0, val_loss=0.121]\n",
      "Epoch 22:  80%|███████▉  | 83/104 [00:17<00:04,  4.65it/s, loss=0.0408, v_num=0, reduced_train_loss=0.000156, global_step=1724.0, val_loss=0.067]\n",
      "Epoch 22:  77%|███████▋  | 80/104 [00:17<00:05,  4.46it/s, loss=0.0373, v_num=0, reduced_train_loss=0.035, global_step=1724.0, val_loss=0.121]\n",
      "Epoch 22:  39%|███▉      | 41/104 [00:09<00:14,  4.28it/s, loss=0.0535, v_num=0, reduced_train_loss=0.0002, global_step=1690.0, val_loss=0.110] ]\n",
      "Epoch 22:  78%|███████▊  | 81/104 [00:18<00:05,  4.49it/s, loss=0.0373, v_num=0, reduced_train_loss=0.035, global_step=1724.0, val_loss=0.121]\n",
      "Epoch 22:  82%|████████▏ | 85/104 [00:18<00:04,  4.70it/s, loss=0.0408, v_num=0, reduced_train_loss=0.000156, global_step=1724.0, val_loss=0.067]\n",
      "Epoch 22:  79%|███████▉  | 82/104 [00:18<00:04,  4.51it/s, loss=0.0373, v_num=0, reduced_train_loss=0.035, global_step=1724.0, val_loss=0.121]\n",
      "Epoch 22:  40%|████      | 42/104 [00:09<00:14,  4.28it/s, loss=0.0633, v_num=0, reduced_train_loss=0.201, global_step=1691.0, val_loss=0.110] 7]\n",
      "Epoch 22:  80%|███████▉  | 83/104 [00:18<00:04,  4.54it/s, loss=0.0373, v_num=0, reduced_train_loss=0.035, global_step=1724.0, val_loss=0.121]\n",
      "Epoch 22:  84%|████████▎ | 87/104 [00:18<00:03,  4.74it/s, loss=0.0408, v_num=0, reduced_train_loss=0.000156, global_step=1724.0, val_loss=0.067]\n",
      "Epoch 22:  81%|████████  | 84/104 [00:18<00:04,  4.56it/s, loss=0.0373, v_num=0, reduced_train_loss=0.035, global_step=1724.0, val_loss=0.121]\n",
      "Epoch 22:  41%|████▏     | 43/104 [00:10<00:14,  4.29it/s, loss=0.063, v_num=0, reduced_train_loss=0.000756, global_step=1692.0, val_loss=0.110]]\n",
      "Epoch 22:  82%|████████▏ | 85/104 [00:18<00:04,  4.59it/s, loss=0.0373, v_num=0, reduced_train_loss=0.035, global_step=1724.0, val_loss=0.121]\n",
      "Epoch 22:  86%|████████▌ | 89/104 [00:18<00:03,  4.79it/s, loss=0.0408, v_num=0, reduced_train_loss=0.000156, global_step=1724.0, val_loss=0.067]\n",
      "Epoch 22:  83%|████████▎ | 86/104 [00:18<00:03,  4.61it/s, loss=0.0373, v_num=0, reduced_train_loss=0.035, global_step=1724.0, val_loss=0.121]\n",
      "Epoch 22:  42%|████▏     | 44/104 [00:10<00:13,  4.29it/s, loss=0.0561, v_num=0, reduced_train_loss=0.000421, global_step=1693.0, val_loss=0.110]\n",
      "Epoch 22:  84%|████████▎ | 87/104 [00:18<00:03,  4.64it/s, loss=0.0373, v_num=0, reduced_train_loss=0.035, global_step=1724.0, val_loss=0.121]\n",
      "Epoch 22:  88%|████████▊ | 91/104 [00:18<00:02,  4.84it/s, loss=0.0408, v_num=0, reduced_train_loss=0.000156, global_step=1724.0, val_loss=0.067]\n",
      "Epoch 22:  85%|████████▍ | 88/104 [00:18<00:03,  4.66it/s, loss=0.0373, v_num=0, reduced_train_loss=0.035, global_step=1724.0, val_loss=0.121]\n",
      "Epoch 22:  43%|████▎     | 45/104 [00:10<00:13,  4.29it/s, loss=0.0526, v_num=0, reduced_train_loss=0.0115, global_step=1694.0, val_loss=0.110]  \n",
      "Epoch 22:  86%|████████▌ | 89/104 [00:19<00:03,  4.68it/s, loss=0.0373, v_num=0, reduced_train_loss=0.035, global_step=1724.0, val_loss=0.121]\n",
      "Epoch 22:  89%|████████▉ | 93/104 [00:19<00:02,  4.88it/s, loss=0.0408, v_num=0, reduced_train_loss=0.000156, global_step=1724.0, val_loss=0.067]\n",
      "Epoch 22:  87%|████████▋ | 90/104 [00:19<00:02,  4.71it/s, loss=0.0373, v_num=0, reduced_train_loss=0.035, global_step=1724.0, val_loss=0.121]\n",
      "Epoch 22:  44%|████▍     | 46/104 [00:10<00:13,  4.29it/s, loss=0.0598, v_num=0, reduced_train_loss=0.150, global_step=1695.0, val_loss=0.110] 7]\n",
      "Epoch 22:  88%|████████▊ | 91/104 [00:19<00:02,  4.73it/s, loss=0.0373, v_num=0, reduced_train_loss=0.035, global_step=1724.0, val_loss=0.121]\n",
      "Epoch 22:  91%|█████████▏| 95/104 [00:19<00:01,  4.92it/s, loss=0.0408, v_num=0, reduced_train_loss=0.000156, global_step=1724.0, val_loss=0.067]\n",
      "Epoch 22:  88%|████████▊ | 92/104 [00:19<00:02,  4.75it/s, loss=0.0373, v_num=0, reduced_train_loss=0.035, global_step=1724.0, val_loss=0.121]\n",
      "Epoch 22:  45%|████▌     | 47/104 [00:10<00:13,  4.29it/s, loss=0.0597, v_num=0, reduced_train_loss=0.000818, global_step=1696.0, val_loss=0.110]\n",
      "Epoch 22:  89%|████████▉ | 93/104 [00:19<00:02,  4.77it/s, loss=0.0373, v_num=0, reduced_train_loss=0.035, global_step=1724.0, val_loss=0.121]\n",
      "Epoch 22:  93%|█████████▎| 97/104 [00:19<00:01,  4.97it/s, loss=0.0408, v_num=0, reduced_train_loss=0.000156, global_step=1724.0, val_loss=0.067]\n",
      "Epoch 22:  90%|█████████ | 94/104 [00:19<00:02,  4.79it/s, loss=0.0373, v_num=0, reduced_train_loss=0.035, global_step=1724.0, val_loss=0.121]\n",
      "Epoch 22:  46%|████▌     | 48/104 [00:11<00:13,  4.29it/s, loss=0.0596, v_num=0, reduced_train_loss=0.120, global_step=1697.0, val_loss=0.110]   \n",
      "Epoch 22:  91%|█████████▏| 95/104 [00:19<00:01,  4.82it/s, loss=0.0373, v_num=0, reduced_train_loss=0.035, global_step=1724.0, val_loss=0.121]\n",
      "Epoch 22:  95%|█████████▌| 99/104 [00:19<00:00,  5.01it/s, loss=0.0408, v_num=0, reduced_train_loss=0.000156, global_step=1724.0, val_loss=0.067]\n",
      "Epoch 22:  92%|█████████▏| 96/104 [00:19<00:01,  4.84it/s, loss=0.0373, v_num=0, reduced_train_loss=0.035, global_step=1724.0, val_loss=0.121]\n",
      "Epoch 22:  47%|████▋     | 49/104 [00:11<00:12,  4.29it/s, loss=0.0642, v_num=0, reduced_train_loss=0.404, global_step=1698.0, val_loss=0.110]067]\n",
      "Epoch 22:  93%|█████████▎| 97/104 [00:19<00:01,  4.86it/s, loss=0.0373, v_num=0, reduced_train_loss=0.035, global_step=1724.0, val_loss=0.121]\n",
      "Epoch 22:  97%|█████████▋| 101/104 [00:20<00:00,  5.05it/s, loss=0.0408, v_num=0, reduced_train_loss=0.000156, global_step=1724.0, val_loss=0.067]\n",
      "Epoch 22:  48%|████▊     | 50/104 [00:11<00:12,  4.29it/s, loss=0.0642, v_num=0, reduced_train_loss=0.404, global_step=1698.0, val_loss=0.110]\n",
      "Epoch 22:  48%|████▊     | 50/104 [00:11<00:12,  4.29it/s, loss=0.0641, v_num=0, reduced_train_loss=0.00726, global_step=1699.0, val_loss=0.110]7]\n",
      "Epoch 22:  95%|█████████▌| 99/104 [00:20<00:01,  4.90it/s, loss=0.0373, v_num=0, reduced_train_loss=0.035, global_step=1724.0, val_loss=0.121]\n",
      "Epoch 22:  99%|█████████▉| 103/104 [00:20<00:00,  5.09it/s, loss=0.0408, v_num=0, reduced_train_loss=0.000156, global_step=1724.0, val_loss=0.067]\n",
      "Epoch 22: 100%|██████████| 104/104 [00:20<00:00,  5.12it/s, loss=0.0408, v_num=0, reduced_train_loss=0.000156, global_step=1724.0, val_loss=0.067]2023-05-03 22:03:57,270 - root - INFO - val_loss: 0.05105328932404518\n",
      "Epoch 22: 100%|██████████| 104/104 [00:20<00:00,  5.12it/s, loss=0.0408, v_num=0, reduced_train_loss=0.000156, global_step=1724.0, val_loss=0.0511]\n",
      "Epoch 23:   0%|          | 0/104 [00:00<?, ?it/s, loss=0.0408, v_num=0, reduced_train_loss=0.000156, global_step=1724.0, val_loss=0.0511]          \n",
      "Epoch 22:  49%|████▉     | 51/104 [00:11<00:12,  4.29it/s, loss=0.0572, v_num=0, reduced_train_loss=0.000564, global_step=1700.0, val_loss=0.110]\n",
      "Epoch 22:  97%|█████████▋| 101/104 [00:20<00:00,  4.94it/s, loss=0.0373, v_num=0, reduced_train_loss=0.035, global_step=1724.0, val_loss=0.121]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 22:03:57.270401 140649257482048 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.05105328932404518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 22:  50%|█████     | 52/104 [00:12<00:12,  4.29it/s, loss=0.0574, v_num=0, reduced_train_loss=0.00829, global_step=1701.0, val_loss=0.110] \n",
      "Epoch 22:  99%|█████████▉| 103/104 [00:20<00:00,  4.98it/s, loss=0.0373, v_num=0, reduced_train_loss=0.035, global_step=1724.0, val_loss=0.121]\n",
      "Epoch 22: 100%|██████████| 104/104 [00:20<00:00,  5.02it/s, loss=0.0373, v_num=0, reduced_train_loss=0.035, global_step=1724.0, val_loss=0.121]2023-05-03 22:03:57,709 - root - INFO - val_loss: 0.07210730016231537\n",
      "Epoch 22: 100%|██████████| 104/104 [00:20<00:00,  5.01it/s, loss=0.0373, v_num=0, reduced_train_loss=0.035, global_step=1724.0, val_loss=0.0721]\n",
      "Epoch 23:   0%|          | 0/104 [00:00<?, ?it/s, loss=0.0373, v_num=0, reduced_train_loss=0.035, global_step=1724.0, val_loss=0.0721]          "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 22:03:57.709430 140522389899072 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.07210730016231537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22:  72%|███████▏  | 75/104 [00:17<00:06,  4.19it/s, loss=0.11, v_num=0, reduced_train_loss=0.0947, global_step=1724.0, val_loss=0.110] ]   \n",
      "Epoch 23:  24%|██▍       | 25/104 [00:06<00:19,  4.13it/s, loss=0.0297, v_num=0, reduced_train_loss=0.00012, global_step=1749.0, val_loss=0.0511]\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 23:  25%|██▌       | 26/104 [00:06<00:18,  4.14it/s, loss=0.0297, v_num=0, reduced_train_loss=0.00146, global_step=1750.0, val_loss=0.0511]]\n",
      "Epoch 23:  24%|██▍       | 25/104 [00:06<00:18,  4.16it/s, loss=0.0319, v_num=0, reduced_train_loss=0.256, global_step=1749.0, val_loss=0.0721]   \n",
      "Epoch 23:  26%|██▌       | 27/104 [00:06<00:18,  4.15it/s, loss=0.0298, v_num=0, reduced_train_loss=0.000693, global_step=1751.0, val_loss=0.0511]\n",
      "Epoch 23:  25%|██▌       | 26/104 [00:06<00:18,  4.17it/s, loss=0.0318, v_num=0, reduced_train_loss=0.000581, global_step=1750.0, val_loss=0.0721]\n",
      "Epoch 23:  27%|██▋       | 28/104 [00:06<00:18,  4.15it/s, loss=0.0348, v_num=0, reduced_train_loss=0.101, global_step=1752.0, val_loss=0.0511]   \n",
      "Epoch 23:  28%|██▊       | 29/104 [00:06<00:18,  4.16it/s, loss=0.0348, v_num=0, reduced_train_loss=0.0013, global_step=1753.0, val_loss=0.0511]  \n",
      "Epoch 22:  79%|███████▉  | 82/104 [00:18<00:05,  4.35it/s, loss=0.11, v_num=0, reduced_train_loss=0.0947, global_step=1724.0, val_loss=0.110]\n",
      "Epoch 23:  29%|██▉       | 30/104 [00:07<00:17,  4.17it/s, loss=0.0165, v_num=0, reduced_train_loss=0.0372, global_step=1754.0, val_loss=0.0511]]\n",
      "Epoch 23:  28%|██▊       | 29/104 [00:06<00:17,  4.20it/s, loss=0.0309, v_num=0, reduced_train_loss=0.000553, global_step=1753.0, val_loss=0.0721]\n",
      "Epoch 23:  30%|██▉       | 31/104 [00:07<00:17,  4.17it/s, loss=0.0161, v_num=0, reduced_train_loss=0.00278, global_step=1755.0, val_loss=0.0511]\n",
      "Epoch 23:  29%|██▉       | 30/104 [00:07<00:17,  4.21it/s, loss=0.0486, v_num=0, reduced_train_loss=0.383, global_step=1754.0, val_loss=0.0721]   \n",
      "Epoch 23:  31%|███       | 32/104 [00:07<00:17,  4.18it/s, loss=0.016, v_num=0, reduced_train_loss=0.00038, global_step=1756.0, val_loss=0.0511] \n",
      "Epoch 23:  30%|██▉       | 31/104 [00:07<00:17,  4.22it/s, loss=0.0481, v_num=0, reduced_train_loss=0.00462, global_step=1755.0, val_loss=0.0721]\n",
      "Epoch 23:  32%|███▏      | 33/104 [00:07<00:16,  4.19it/s, loss=0.016, v_num=0, reduced_train_loss=0.000276, global_step=1757.0, val_loss=0.0511]\n",
      "Epoch 23:  31%|███       | 32/104 [00:07<00:17,  4.22it/s, loss=0.0376, v_num=0, reduced_train_loss=0.00449, global_step=1756.0, val_loss=0.0721]\n",
      "Epoch 23:  33%|███▎      | 34/104 [00:08<00:16,  4.19it/s, loss=0.015, v_num=0, reduced_train_loss=0.00546, global_step=1758.0, val_loss=0.0511] \n",
      "Epoch 23:  34%|███▎      | 35/104 [00:08<00:16,  4.20it/s, loss=0.015, v_num=0, reduced_train_loss=0.00111, global_step=1759.0, val_loss=0.0511] \n",
      "Epoch 23:  33%|███▎      | 34/104 [00:08<00:16,  4.24it/s, loss=0.0352, v_num=0, reduced_train_loss=0.00579, global_step=1758.0, val_loss=0.0721]\n",
      "Epoch 23:  35%|███▍      | 36/104 [00:08<00:16,  4.20it/s, loss=0.0189, v_num=0, reduced_train_loss=0.0786, global_step=1760.0, val_loss=0.0511]\n",
      "Epoch 23:  34%|███▎      | 35/104 [00:08<00:16,  4.24it/s, loss=0.0352, v_num=0, reduced_train_loss=0.00115, global_step=1759.0, val_loss=0.0721]\n",
      "Epoch 23:  36%|███▌      | 37/104 [00:08<00:15,  4.21it/s, loss=0.0188, v_num=0, reduced_train_loss=0.000217, global_step=1761.0, val_loss=0.0511]\n",
      "Epoch 23:  35%|███▍      | 36/104 [00:08<00:15,  4.25it/s, loss=0.0344, v_num=0, reduced_train_loss=0.00149, global_step=1760.0, val_loss=0.0721]\n",
      "Epoch 23:  37%|███▋      | 38/104 [00:09<00:15,  4.21it/s, loss=0.0185, v_num=0, reduced_train_loss=0.000256, global_step=1762.0, val_loss=0.0511]\n",
      "Epoch 23:  36%|███▌      | 37/104 [00:08<00:15,  4.26it/s, loss=0.036, v_num=0, reduced_train_loss=0.0317, global_step=1761.0, val_loss=0.0721]  \n",
      "Epoch 23:  38%|███▊      | 39/104 [00:09<00:15,  4.22it/s, loss=0.0185, v_num=0, reduced_train_loss=0.000274, global_step=1763.0, val_loss=0.0511]\n",
      "Epoch 23:  37%|███▋      | 38/104 [00:08<00:15,  4.26it/s, loss=0.0361, v_num=0, reduced_train_loss=0.00178, global_step=1762.0, val_loss=0.0721]\n",
      "Epoch 23:  38%|███▊      | 40/104 [00:09<00:15,  4.22it/s, loss=0.0168, v_num=0, reduced_train_loss=0.000242, global_step=1764.0, val_loss=0.0511]\n",
      "Epoch 23:  38%|███▊      | 39/104 [00:09<00:15,  4.26it/s, loss=0.0362, v_num=0, reduced_train_loss=0.00299, global_step=1763.0, val_loss=0.0721]\n",
      "Epoch 22: 100%|██████████| 104/104 [00:21<00:00,  4.84it/s, loss=0.11, v_num=0, reduced_train_loss=0.0947, global_step=1724.0, val_loss=0.110]2023-05-03 22:04:06,927 - root - INFO - val_loss: 0.10228592157363892\n",
      "Epoch 22: 100%|██████████| 104/104 [00:21<00:00,  4.84it/s, loss=0.11, v_num=0, reduced_train_loss=0.0947, global_step=1724.0, val_loss=0.102]\n",
      "Epoch 23:  39%|███▉      | 41/104 [00:09<00:14,  4.22it/s, loss=0.0163, v_num=0, reduced_train_loss=0.000273, global_step=1765.0, val_loss=0.0511]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 22:04:06.927616 139755592410944 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.10228592157363892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23:  72%|███████▏  | 75/104 [00:17<00:06,  4.29it/s, loss=0.0333, v_num=0, reduced_train_loss=0.000475, global_step=1799.0, val_loss=0.0511]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 23:  72%|███████▏  | 75/104 [00:17<00:06,  4.36it/s, loss=0.0405, v_num=0, reduced_train_loss=0.0434, global_step=1799.0, val_loss=0.0721]\n",
      "Epoch 23:  33%|███▎      | 34/104 [00:08<00:16,  4.24it/s, loss=0.0345, v_num=0, reduced_train_loss=0.000503, global_step=1758.0, val_loss=0.102]\n",
      "Epoch 23:  73%|███████▎  | 76/104 [00:17<00:06,  4.30it/s, loss=0.0333, v_num=0, reduced_train_loss=0.000475, global_step=1799.0, val_loss=0.0511]\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 23:  74%|███████▍  | 77/104 [00:17<00:06,  4.32it/s, loss=0.0333, v_num=0, reduced_train_loss=0.000475, global_step=1799.0, val_loss=0.0511]\n",
      "Epoch 23:  34%|███▎      | 35/104 [00:08<00:16,  4.25it/s, loss=0.0395, v_num=0, reduced_train_loss=0.109, global_step=1759.0, val_loss=0.102]   \n",
      "Epoch 23:  75%|███████▌  | 78/104 [00:17<00:05,  4.35it/s, loss=0.0333, v_num=0, reduced_train_loss=0.000475, global_step=1799.0, val_loss=0.0511]\n",
      "Epoch 23:  74%|███████▍  | 77/104 [00:17<00:06,  4.40it/s, loss=0.0405, v_num=0, reduced_train_loss=0.0434, global_step=1799.0, val_loss=0.0721]\n",
      "Epoch 23:  76%|███████▌  | 79/104 [00:18<00:05,  4.37it/s, loss=0.0333, v_num=0, reduced_train_loss=0.000475, global_step=1799.0, val_loss=0.0511]\n",
      "Epoch 23:  35%|███▍      | 36/104 [00:08<00:16,  4.25it/s, loss=0.043, v_num=0, reduced_train_loss=0.0764, global_step=1760.0, val_loss=0.102]1]\n",
      "Epoch 23:  77%|███████▋  | 80/104 [00:18<00:05,  4.40it/s, loss=0.0333, v_num=0, reduced_train_loss=0.000475, global_step=1799.0, val_loss=0.0511]\n",
      "Epoch 23:  76%|███████▌  | 79/104 [00:17<00:05,  4.45it/s, loss=0.0405, v_num=0, reduced_train_loss=0.0434, global_step=1799.0, val_loss=0.0721]\n",
      "\n",
      "Epoch 23:  36%|███▌      | 37/104 [00:08<00:15,  4.25it/s, loss=0.0427, v_num=0, reduced_train_loss=0.0934, global_step=1761.0, val_loss=0.102]11]\n",
      "Epoch 23:  78%|███████▊  | 81/104 [00:17<00:05,  4.50it/s, loss=0.0405, v_num=0, reduced_train_loss=0.0434, global_step=1799.0, val_loss=0.0721]\n",
      "Epoch 23:  79%|███████▉  | 82/104 [00:18<00:04,  4.45it/s, loss=0.0333, v_num=0, reduced_train_loss=0.000475, global_step=1799.0, val_loss=0.0511]\n",
      "Epoch 23:  79%|███████▉  | 82/104 [00:18<00:04,  4.53it/s, loss=0.0405, v_num=0, reduced_train_loss=0.0434, global_step=1799.0, val_loss=0.0721]\n",
      "Epoch 23:  37%|███▋      | 38/104 [00:08<00:15,  4.25it/s, loss=0.0499, v_num=0, reduced_train_loss=0.150, global_step=1762.0, val_loss=0.102] 11]\n",
      "Epoch 23:  80%|███████▉  | 83/104 [00:18<00:04,  4.55it/s, loss=0.0405, v_num=0, reduced_train_loss=0.0434, global_step=1799.0, val_loss=0.0721]\n",
      "Epoch 23:  81%|████████  | 84/104 [00:18<00:04,  4.49it/s, loss=0.0333, v_num=0, reduced_train_loss=0.000475, global_step=1799.0, val_loss=0.0511]\n",
      "Epoch 23:  81%|████████  | 84/104 [00:18<00:04,  4.58it/s, loss=0.0405, v_num=0, reduced_train_loss=0.0434, global_step=1799.0, val_loss=0.0721]\n",
      "Epoch 23:  38%|███▊      | 39/104 [00:09<00:15,  4.25it/s, loss=0.0503, v_num=0, reduced_train_loss=0.00851, global_step=1763.0, val_loss=0.102]1]\n",
      "Epoch 23:  82%|████████▏ | 85/104 [00:18<00:04,  4.60it/s, loss=0.0405, v_num=0, reduced_train_loss=0.0434, global_step=1799.0, val_loss=0.0721]\n",
      "Epoch 23:  83%|████████▎ | 86/104 [00:18<00:03,  4.54it/s, loss=0.0333, v_num=0, reduced_train_loss=0.000475, global_step=1799.0, val_loss=0.0511]\n",
      "Epoch 23:  38%|███▊      | 40/104 [00:09<00:15,  4.25it/s, loss=0.05, v_num=0, reduced_train_loss=0.00334, global_step=1764.0, val_loss=0.102]  \n",
      "Epoch 23:  84%|████████▎ | 87/104 [00:19<00:03,  4.56it/s, loss=0.0333, v_num=0, reduced_train_loss=0.000475, global_step=1799.0, val_loss=0.0511]\n",
      "Epoch 23:  84%|████████▎ | 87/104 [00:18<00:03,  4.65it/s, loss=0.0405, v_num=0, reduced_train_loss=0.0434, global_step=1799.0, val_loss=0.0721]\n",
      "Epoch 23:  85%|████████▍ | 88/104 [00:19<00:03,  4.58it/s, loss=0.0333, v_num=0, reduced_train_loss=0.000475, global_step=1799.0, val_loss=0.0511]\n",
      "Epoch 23:  39%|███▉      | 41/104 [00:09<00:14,  4.25it/s, loss=0.0502, v_num=0, reduced_train_loss=0.0127, global_step=1765.0, val_loss=0.102]]\n",
      "Epoch 23:  86%|████████▌ | 89/104 [00:19<00:03,  4.61it/s, loss=0.0333, v_num=0, reduced_train_loss=0.000475, global_step=1799.0, val_loss=0.0511]\n",
      "Epoch 23:  86%|████████▌ | 89/104 [00:18<00:03,  4.70it/s, loss=0.0405, v_num=0, reduced_train_loss=0.0434, global_step=1799.0, val_loss=0.0721]\n",
      "Epoch 23:  87%|████████▋ | 90/104 [00:19<00:03,  4.63it/s, loss=0.0333, v_num=0, reduced_train_loss=0.000475, global_step=1799.0, val_loss=0.0511]\n",
      "Epoch 23:  40%|████      | 42/104 [00:09<00:14,  4.25it/s, loss=0.0506, v_num=0, reduced_train_loss=0.00802, global_step=1766.0, val_loss=0.102]\n",
      "Epoch 23:  88%|████████▊ | 91/104 [00:19<00:02,  4.65it/s, loss=0.0333, v_num=0, reduced_train_loss=0.000475, global_step=1799.0, val_loss=0.0511]\n",
      "Epoch 23:  88%|████████▊ | 91/104 [00:19<00:02,  4.74it/s, loss=0.0405, v_num=0, reduced_train_loss=0.0434, global_step=1799.0, val_loss=0.0721]\n",
      "Epoch 23:  88%|████████▊ | 92/104 [00:19<00:02,  4.67it/s, loss=0.0333, v_num=0, reduced_train_loss=0.000475, global_step=1799.0, val_loss=0.0511]\n",
      "Epoch 23:  41%|████▏     | 43/104 [00:10<00:14,  4.26it/s, loss=0.0576, v_num=0, reduced_train_loss=0.144, global_step=1767.0, val_loss=0.102]  \n",
      "Epoch 23:  89%|████████▉ | 93/104 [00:19<00:02,  4.69it/s, loss=0.0333, v_num=0, reduced_train_loss=0.000475, global_step=1799.0, val_loss=0.0511]\n",
      "Epoch 23:  89%|████████▉ | 93/104 [00:19<00:02,  4.78it/s, loss=0.0405, v_num=0, reduced_train_loss=0.0434, global_step=1799.0, val_loss=0.0721]\n",
      "Epoch 23:  42%|████▏     | 44/104 [00:10<00:14,  4.26it/s, loss=0.0643, v_num=0, reduced_train_loss=0.142, global_step=1768.0, val_loss=0.102]511]\n",
      "Epoch 23:  90%|█████████ | 94/104 [00:19<00:02,  4.81it/s, loss=0.0405, v_num=0, reduced_train_loss=0.0434, global_step=1799.0, val_loss=0.0721]\n",
      "Epoch 23:  91%|█████████▏| 95/104 [00:20<00:01,  4.73it/s, loss=0.0333, v_num=0, reduced_train_loss=0.000475, global_step=1799.0, val_loss=0.0511]\n",
      "Epoch 23:  91%|█████████▏| 95/104 [00:19<00:01,  4.83it/s, loss=0.0405, v_num=0, reduced_train_loss=0.0434, global_step=1799.0, val_loss=0.0721]\n",
      "Epoch 23:  43%|████▎     | 45/104 [00:10<00:13,  4.26it/s, loss=0.067, v_num=0, reduced_train_loss=0.0602, global_step=1769.0, val_loss=0.102]511]\n",
      "Epoch 23:  92%|█████████▏| 96/104 [00:19<00:01,  4.85it/s, loss=0.0405, v_num=0, reduced_train_loss=0.0434, global_step=1799.0, val_loss=0.0721]\n",
      "Epoch 23:  93%|█████████▎| 97/104 [00:20<00:01,  4.78it/s, loss=0.0333, v_num=0, reduced_train_loss=0.000475, global_step=1799.0, val_loss=0.0511]\n",
      "Epoch 23:  93%|█████████▎| 97/104 [00:19<00:01,  4.87it/s, loss=0.0405, v_num=0, reduced_train_loss=0.0434, global_step=1799.0, val_loss=0.0721]\n",
      "Epoch 23:  44%|████▍     | 46/104 [00:10<00:13,  4.26it/s, loss=0.0529, v_num=0, reduced_train_loss=0.00213, global_step=1770.0, val_loss=0.102]1]\n",
      "Epoch 23:  94%|█████████▍| 98/104 [00:20<00:01,  4.89it/s, loss=0.0405, v_num=0, reduced_train_loss=0.0434, global_step=1799.0, val_loss=0.0721]\n",
      "Epoch 23:  95%|█████████▌| 99/104 [00:20<00:01,  4.82it/s, loss=0.0333, v_num=0, reduced_train_loss=0.000475, global_step=1799.0, val_loss=0.0511]\n",
      "Epoch 23:  95%|█████████▌| 99/104 [00:20<00:01,  4.91it/s, loss=0.0405, v_num=0, reduced_train_loss=0.0434, global_step=1799.0, val_loss=0.0721]\n",
      "Epoch 23:  45%|████▌     | 47/104 [00:11<00:13,  4.26it/s, loss=0.0456, v_num=0, reduced_train_loss=0.00824, global_step=1771.0, val_loss=0.102]11]\n",
      "Epoch 23:  96%|█████████▌| 100/104 [00:20<00:00,  4.93it/s, loss=0.0405, v_num=0, reduced_train_loss=0.0434, global_step=1799.0, val_loss=0.0721]\n",
      "Epoch 23:  97%|█████████▋| 101/104 [00:20<00:00,  4.85it/s, loss=0.0333, v_num=0, reduced_train_loss=0.000475, global_step=1799.0, val_loss=0.0511]\n",
      "Epoch 23:  46%|████▌     | 48/104 [00:11<00:13,  4.26it/s, loss=0.0476, v_num=0, reduced_train_loss=0.054, global_step=1772.0, val_loss=0.102]  ]\n",
      "Epoch 23:  98%|█████████▊| 102/104 [00:20<00:00,  4.87it/s, loss=0.0333, v_num=0, reduced_train_loss=0.000475, global_step=1799.0, val_loss=0.0511]\n",
      "Epoch 23:  98%|█████████▊| 102/104 [00:20<00:00,  4.97it/s, loss=0.0405, v_num=0, reduced_train_loss=0.0434, global_step=1799.0, val_loss=0.0721]\n",
      "Epoch 23:  99%|█████████▉| 103/104 [00:21<00:00,  4.89it/s, loss=0.0333, v_num=0, reduced_train_loss=0.000475, global_step=1799.0, val_loss=0.0511]\n",
      "Epoch 23:  99%|█████████▉| 103/104 [00:20<00:00,  4.98it/s, loss=0.0405, v_num=0, reduced_train_loss=0.0434, global_step=1799.0, val_loss=0.0721]\n",
      "Epoch 23: 100%|██████████| 104/104 [00:21<00:00,  4.92it/s, loss=0.0333, v_num=0, reduced_train_loss=0.000475, global_step=1799.0, val_loss=0.0511]2023-05-03 22:04:18,396 - root - INFO - val_loss: 0.10322674363851547\n",
      "Epoch 23: 100%|██████████| 104/104 [00:21<00:00,  4.92it/s, loss=0.0333, v_num=0, reduced_train_loss=0.000475, global_step=1799.0, val_loss=0.103] \n",
      "Epoch 23:  47%|████▋     | 49/104 [00:11<00:12,  4.27it/s, loss=0.0505, v_num=0, reduced_train_loss=0.059, global_step=1773.0, val_loss=0.102]    \n",
      "Epoch 23: 100%|██████████| 104/104 [00:20<00:00,  5.02it/s, loss=0.0405, v_num=0, reduced_train_loss=0.0434, global_step=1799.0, val_loss=0.0721]2023-05-03 22:04:18,452 - root - INFO - val_loss: 0.06687038391828537\n",
      "Epoch 23: 100%|██████████| 104/104 [00:20<00:00,  5.02it/s, loss=0.0405, v_num=0, reduced_train_loss=0.0434, global_step=1799.0, val_loss=0.0669]\n",
      "Epoch 24:   0%|          | 0/104 [00:00<?, ?it/s, loss=0.0405, v_num=0, reduced_train_loss=0.0434, global_step=1799.0, val_loss=0.0669]          "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 22:04:18.396652 140649257482048 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.10322674363851547\n",
      "I0503 22:04:18.452578 140522389899072 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.06687038391828537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23:  72%|███████▏  | 75/104 [00:17<00:06,  4.30it/s, loss=0.0575, v_num=0, reduced_train_loss=0.0495, global_step=1799.0, val_loss=0.102]9] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 24:  26%|██▌       | 27/104 [00:06<00:17,  4.43it/s, loss=0.0185, v_num=0, reduced_train_loss=0.000379, global_step=1826.0, val_loss=0.0669]\n",
      "Epoch 24:  27%|██▋       | 28/104 [00:06<00:17,  4.46it/s, loss=0.0173, v_num=0, reduced_train_loss=0.00103, global_step=1827.0, val_loss=0.103] \n",
      "Epoch 24:  27%|██▋       | 28/104 [00:06<00:17,  4.43it/s, loss=0.0198, v_num=0, reduced_train_loss=0.0281, global_step=1827.0, val_loss=0.0669]  \n",
      "Epoch 24:  28%|██▊       | 29/104 [00:06<00:16,  4.46it/s, loss=0.0173, v_num=0, reduced_train_loss=0.000424, global_step=1828.0, val_loss=0.103]\n",
      "Epoch 24:  28%|██▊       | 29/104 [00:06<00:16,  4.43it/s, loss=0.0227, v_num=0, reduced_train_loss=0.0769, global_step=1828.0, val_loss=0.0669]\n",
      "Epoch 24:  29%|██▉       | 30/104 [00:06<00:16,  4.46it/s, loss=0.0116, v_num=0, reduced_train_loss=0.00197, global_step=1829.0, val_loss=0.103] \n",
      "Epoch 24:  29%|██▉       | 30/104 [00:06<00:16,  4.43it/s, loss=0.021, v_num=0, reduced_train_loss=0.000303, global_step=1829.0, val_loss=0.0669]\n",
      "Epoch 24:  30%|██▉       | 31/104 [00:06<00:16,  4.46it/s, loss=0.0116, v_num=0, reduced_train_loss=0.000443, global_step=1830.0, val_loss=0.103]\n",
      "Epoch 24:  30%|██▉       | 31/104 [00:06<00:16,  4.43it/s, loss=0.0211, v_num=0, reduced_train_loss=0.00207, global_step=1830.0, val_loss=0.0669]\n",
      "Epoch 24:  31%|███       | 32/104 [00:07<00:16,  4.47it/s, loss=0.0116, v_num=0, reduced_train_loss=0.000263, global_step=1831.0, val_loss=0.103]\n",
      "Epoch 24:  32%|███▏      | 33/104 [00:07<00:15,  4.47it/s, loss=0.00778, v_num=0, reduced_train_loss=0.00236, global_step=1832.0, val_loss=0.103]\n",
      "Epoch 24:  32%|███▏      | 33/104 [00:07<00:16,  4.43it/s, loss=0.0222, v_num=0, reduced_train_loss=0.0181, global_step=1832.0, val_loss=0.0669] \n",
      "Epoch 24:  33%|███▎      | 34/104 [00:07<00:15,  4.47it/s, loss=0.0081, v_num=0, reduced_train_loss=0.00652, global_step=1833.0, val_loss=0.103] \n",
      "Epoch 24:  33%|███▎      | 34/104 [00:07<00:15,  4.43it/s, loss=0.0222, v_num=0, reduced_train_loss=0.000186, global_step=1833.0, val_loss=0.0669]\n",
      "Epoch 24:  34%|███▎      | 35/104 [00:07<00:15,  4.47it/s, loss=0.00798, v_num=0, reduced_train_loss=0.000362, global_step=1834.0, val_loss=0.103]\n",
      "Epoch 24:  34%|███▎      | 35/104 [00:07<00:15,  4.43it/s, loss=0.0351, v_num=0, reduced_train_loss=0.257, global_step=1834.0, val_loss=0.0669]   \n",
      "Epoch 24:  35%|███▍      | 36/104 [00:08<00:15,  4.47it/s, loss=0.00895, v_num=0, reduced_train_loss=0.0195, global_step=1835.0, val_loss=0.103]  \n",
      "Epoch 24:  35%|███▍      | 36/104 [00:08<00:15,  4.42it/s, loss=0.0298, v_num=0, reduced_train_loss=0.000504, global_step=1835.0, val_loss=0.0669]\n",
      "Epoch 24:  36%|███▌      | 37/104 [00:08<00:14,  4.47it/s, loss=0.00869, v_num=0, reduced_train_loss=0.000598, global_step=1836.0, val_loss=0.103]\n",
      "Epoch 24:  36%|███▌      | 37/104 [00:08<00:15,  4.42it/s, loss=0.0299, v_num=0, reduced_train_loss=0.00281, global_step=1836.0, val_loss=0.0669] \n",
      "Epoch 24:  37%|███▋      | 38/104 [00:08<00:14,  4.47it/s, loss=0.00872, v_num=0, reduced_train_loss=0.00132, global_step=1837.0, val_loss=0.103] \n",
      "Epoch 24:  38%|███▊      | 39/104 [00:08<00:14,  4.47it/s, loss=0.00872, v_num=0, reduced_train_loss=8.35e-5, global_step=1838.0, val_loss=0.103]\n",
      "Epoch 23:  93%|█████████▎| 97/104 [00:20<00:01,  4.80it/s, loss=0.0575, v_num=0, reduced_train_loss=0.0495, global_step=1799.0, val_loss=0.102]\n",
      "Epoch 24:  38%|███▊      | 40/104 [00:08<00:14,  4.47it/s, loss=0.00875, v_num=0, reduced_train_loss=0.000585, global_step=1839.0, val_loss=0.103]\n",
      "Epoch 23:  95%|█████████▌| 99/104 [00:20<00:01,  4.84it/s, loss=0.0575, v_num=0, reduced_train_loss=0.0495, global_step=1799.0, val_loss=0.102]\n",
      "Epoch 24:  39%|███▉      | 41/104 [00:09<00:14,  4.47it/s, loss=0.00873, v_num=0, reduced_train_loss=5.53e-5, global_step=1840.0, val_loss=0.103] \n",
      "Epoch 23:  97%|█████████▋| 101/104 [00:20<00:00,  4.88it/s, loss=0.0575, v_num=0, reduced_train_loss=0.0495, global_step=1799.0, val_loss=0.102]\n",
      "Epoch 24:  40%|████      | 42/104 [00:09<00:13,  4.47it/s, loss=0.0155, v_num=0, reduced_train_loss=0.135, global_step=1841.0, val_loss=0.103]    \n",
      "Epoch 23:  99%|█████████▉| 103/104 [00:20<00:00,  4.92it/s, loss=0.0575, v_num=0, reduced_train_loss=0.0495, global_step=1799.0, val_loss=0.102]\n",
      "Epoch 23: 100%|██████████| 104/104 [00:20<00:00,  4.95it/s, loss=0.0575, v_num=0, reduced_train_loss=0.0495, global_step=1799.0, val_loss=0.102]2023-05-03 22:04:27,929 - root - INFO - val_loss: 0.07757751643657684\n",
      "Epoch 23: 100%|██████████| 104/104 [00:20<00:00,  4.95it/s, loss=0.0575, v_num=0, reduced_train_loss=0.0495, global_step=1799.0, val_loss=0.0776]\n",
      "Epoch 24:  40%|████      | 42/104 [00:09<00:14,  4.41it/s, loss=0.0379, v_num=0, reduced_train_loss=0.000522, global_step=1841.0, val_loss=0.0669]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 22:04:27.929568 139755592410944 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.07757751643657684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24:  72%|███████▏  | 75/104 [00:16<00:06,  4.42it/s, loss=0.0167, v_num=0, reduced_train_loss=0.000293, global_step=1874.0, val_loss=0.103] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 24:  72%|███████▏  | 75/104 [00:17<00:06,  4.39it/s, loss=0.127, v_num=0, reduced_train_loss=0.0132, global_step=1874.0, val_loss=0.0669]]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24:  73%|███████▎  | 76/104 [00:17<00:06,  4.43it/s, loss=0.0167, v_num=0, reduced_train_loss=0.000293, global_step=1874.0, val_loss=0.103]\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 24:  74%|███████▍  | 77/104 [00:17<00:06,  4.45it/s, loss=0.0167, v_num=0, reduced_train_loss=0.000293, global_step=1874.0, val_loss=0.103]\n",
      "Epoch 24:  33%|███▎      | 34/104 [00:07<00:16,  4.36it/s, loss=0.0789, v_num=0, reduced_train_loss=0.0199, global_step=1833.0, val_loss=0.0776]\n",
      "Epoch 24:  75%|███████▌  | 78/104 [00:17<00:05,  4.48it/s, loss=0.0167, v_num=0, reduced_train_loss=0.000293, global_step=1874.0, val_loss=0.103]\n",
      "Epoch 24:  74%|███████▍  | 77/104 [00:17<00:06,  4.43it/s, loss=0.127, v_num=0, reduced_train_loss=0.0132, global_step=1874.0, val_loss=0.0669]\n",
      "Epoch 24:  34%|███▎      | 35/104 [00:08<00:15,  4.36it/s, loss=0.0789, v_num=0, reduced_train_loss=0.0199, global_step=1833.0, val_loss=0.0776]]\n",
      "Epoch 24:  34%|███▎      | 35/104 [00:08<00:15,  4.36it/s, loss=0.0633, v_num=0, reduced_train_loss=0.00475, global_step=1834.0, val_loss=0.0776]\n",
      "Epoch 24:  77%|███████▋  | 80/104 [00:17<00:05,  4.53it/s, loss=0.0167, v_num=0, reduced_train_loss=0.000293, global_step=1874.0, val_loss=0.103]\n",
      "Epoch 24:  35%|███▍      | 36/104 [00:08<00:15,  4.36it/s, loss=0.0552, v_num=0, reduced_train_loss=0.00253, global_step=1835.0, val_loss=0.0776]\n",
      "Epoch 24:  78%|███████▊  | 81/104 [00:17<00:05,  4.55it/s, loss=0.0167, v_num=0, reduced_train_loss=0.000293, global_step=1874.0, val_loss=0.103]\n",
      "Epoch 24:  77%|███████▋  | 80/104 [00:17<00:05,  4.51it/s, loss=0.127, v_num=0, reduced_train_loss=0.0132, global_step=1874.0, val_loss=0.0669]\n",
      "Epoch 24:  78%|███████▊  | 81/104 [00:17<00:05,  4.53it/s, loss=0.127, v_num=0, reduced_train_loss=0.0132, global_step=1874.0, val_loss=0.0669]\n",
      "Epoch 24:  36%|███▌      | 37/104 [00:08<00:15,  4.37it/s, loss=0.0543, v_num=0, reduced_train_loss=0.00878, global_step=1836.0, val_loss=0.0776]\n",
      "Epoch 24:  79%|███████▉  | 82/104 [00:17<00:04,  4.56it/s, loss=0.127, v_num=0, reduced_train_loss=0.0132, global_step=1874.0, val_loss=0.0669]\n",
      "Epoch 24:  80%|███████▉  | 83/104 [00:18<00:04,  4.60it/s, loss=0.0167, v_num=0, reduced_train_loss=0.000293, global_step=1874.0, val_loss=0.103]\n",
      "Epoch 24:  80%|███████▉  | 83/104 [00:18<00:04,  4.58it/s, loss=0.127, v_num=0, reduced_train_loss=0.0132, global_step=1874.0, val_loss=0.0669]\n",
      "Epoch 24:  37%|███▋      | 38/104 [00:08<00:15,  4.37it/s, loss=0.0545, v_num=0, reduced_train_loss=0.0125, global_step=1837.0, val_loss=0.0776] \n",
      "Epoch 24:  81%|████████  | 84/104 [00:18<00:04,  4.60it/s, loss=0.127, v_num=0, reduced_train_loss=0.0132, global_step=1874.0, val_loss=0.0669]\n",
      "Epoch 24:  82%|████████▏ | 85/104 [00:18<00:04,  4.64it/s, loss=0.0167, v_num=0, reduced_train_loss=0.000293, global_step=1874.0, val_loss=0.103]\n",
      "Epoch 24:  82%|████████▏ | 85/104 [00:18<00:04,  4.63it/s, loss=0.127, v_num=0, reduced_train_loss=0.0132, global_step=1874.0, val_loss=0.0669]\n",
      "Epoch 24:  38%|███▊      | 39/104 [00:08<00:14,  4.37it/s, loss=0.0542, v_num=0, reduced_train_loss=0.00563, global_step=1838.0, val_loss=0.0776]\n",
      "Epoch 24:  83%|████████▎ | 86/104 [00:18<00:03,  4.65it/s, loss=0.127, v_num=0, reduced_train_loss=0.0132, global_step=1874.0, val_loss=0.0669]\n",
      "Epoch 24:  84%|████████▎ | 87/104 [00:18<00:03,  4.69it/s, loss=0.0167, v_num=0, reduced_train_loss=0.000293, global_step=1874.0, val_loss=0.103]\n",
      "Epoch 24:  38%|███▊      | 40/104 [00:09<00:14,  4.37it/s, loss=0.0428, v_num=0, reduced_train_loss=0.00535, global_step=1839.0, val_loss=0.0776]\n",
      "Epoch 24:  85%|████████▍ | 88/104 [00:18<00:03,  4.71it/s, loss=0.0167, v_num=0, reduced_train_loss=0.000293, global_step=1874.0, val_loss=0.103]\n",
      "Epoch 24:  85%|████████▍ | 88/104 [00:18<00:03,  4.69it/s, loss=0.127, v_num=0, reduced_train_loss=0.0132, global_step=1874.0, val_loss=0.0669]\n",
      "Epoch 24:  39%|███▉      | 41/104 [00:09<00:14,  4.37it/s, loss=0.0429, v_num=0, reduced_train_loss=0.00392, global_step=1840.0, val_loss=0.0776]\n",
      "Epoch 24:  86%|████████▌ | 89/104 [00:18<00:03,  4.72it/s, loss=0.127, v_num=0, reduced_train_loss=0.0132, global_step=1874.0, val_loss=0.0669]\n",
      "Epoch 24:  87%|████████▋ | 90/104 [00:18<00:02,  4.75it/s, loss=0.0167, v_num=0, reduced_train_loss=0.000293, global_step=1874.0, val_loss=0.103]\n",
      "Epoch 24:  87%|████████▋ | 90/104 [00:18<00:02,  4.74it/s, loss=0.127, v_num=0, reduced_train_loss=0.0132, global_step=1874.0, val_loss=0.0669]\n",
      "Epoch 24:  40%|████      | 42/104 [00:09<00:14,  4.37it/s, loss=0.0421, v_num=0, reduced_train_loss=0.0111, global_step=1841.0, val_loss=0.0776] \n",
      "Epoch 24:  88%|████████▊ | 91/104 [00:19<00:02,  4.76it/s, loss=0.127, v_num=0, reduced_train_loss=0.0132, global_step=1874.0, val_loss=0.0669]\n",
      "Epoch 24:  88%|████████▊ | 92/104 [00:19<00:02,  4.79it/s, loss=0.0167, v_num=0, reduced_train_loss=0.000293, global_step=1874.0, val_loss=0.103]\n",
      "Epoch 24:  88%|████████▊ | 92/104 [00:19<00:02,  4.78it/s, loss=0.127, v_num=0, reduced_train_loss=0.0132, global_step=1874.0, val_loss=0.0669]\n",
      "Epoch 24:  41%|████▏     | 43/104 [00:09<00:13,  4.37it/s, loss=0.0422, v_num=0, reduced_train_loss=0.0015, global_step=1842.0, val_loss=0.0776]]\n",
      "Epoch 24:  89%|████████▉ | 93/104 [00:19<00:02,  4.80it/s, loss=0.127, v_num=0, reduced_train_loss=0.0132, global_step=1874.0, val_loss=0.0669]\n",
      "Epoch 24:  90%|█████████ | 94/104 [00:19<00:02,  4.83it/s, loss=0.0167, v_num=0, reduced_train_loss=0.000293, global_step=1874.0, val_loss=0.103]\n",
      "Epoch 24:  90%|█████████ | 94/104 [00:19<00:02,  4.82it/s, loss=0.127, v_num=0, reduced_train_loss=0.0132, global_step=1874.0, val_loss=0.0669]\n",
      "Epoch 24:  42%|████▏     | 44/104 [00:10<00:13,  4.37it/s, loss=0.0422, v_num=0, reduced_train_loss=0.00229, global_step=1843.0, val_loss=0.0776]\n",
      "Epoch 24:  91%|█████████▏| 95/104 [00:19<00:01,  4.84it/s, loss=0.127, v_num=0, reduced_train_loss=0.0132, global_step=1874.0, val_loss=0.0669]\n",
      "Epoch 24:  92%|█████████▏| 96/104 [00:19<00:01,  4.87it/s, loss=0.0167, v_num=0, reduced_train_loss=0.000293, global_step=1874.0, val_loss=0.103]\n",
      "Epoch 24:  43%|████▎     | 45/104 [00:10<00:13,  4.38it/s, loss=0.0446, v_num=0, reduced_train_loss=0.104, global_step=1844.0, val_loss=0.0776]  \n",
      "Epoch 24:  93%|█████████▎| 97/104 [00:19<00:01,  4.89it/s, loss=0.0167, v_num=0, reduced_train_loss=0.000293, global_step=1874.0, val_loss=0.103]\n",
      "Epoch 24:  93%|█████████▎| 97/104 [00:19<00:01,  4.89it/s, loss=0.127, v_num=0, reduced_train_loss=0.0132, global_step=1874.0, val_loss=0.0669]\n",
      "Epoch 24:  94%|█████████▍| 98/104 [00:19<00:01,  4.91it/s, loss=0.0167, v_num=0, reduced_train_loss=0.000293, global_step=1874.0, val_loss=0.103]\n",
      "Epoch 24:  44%|████▍     | 46/104 [00:10<00:13,  4.38it/s, loss=0.0457, v_num=0, reduced_train_loss=0.0257, global_step=1845.0, val_loss=0.0776]\n",
      "Epoch 24:  95%|█████████▌| 99/104 [00:20<00:01,  4.93it/s, loss=0.0167, v_num=0, reduced_train_loss=0.000293, global_step=1874.0, val_loss=0.103]\n",
      "Epoch 24:  95%|█████████▌| 99/104 [00:20<00:01,  4.93it/s, loss=0.127, v_num=0, reduced_train_loss=0.0132, global_step=1874.0, val_loss=0.0669]\n",
      "Epoch 24:  45%|████▌     | 47/104 [00:10<00:13,  4.38it/s, loss=0.0511, v_num=0, reduced_train_loss=0.109, global_step=1846.0, val_loss=0.0776] 3]\n",
      "Epoch 24:  96%|█████████▌| 100/104 [00:20<00:00,  4.94it/s, loss=0.127, v_num=0, reduced_train_loss=0.0132, global_step=1874.0, val_loss=0.0669]\n",
      "Epoch 24:  97%|█████████▋| 101/104 [00:20<00:00,  4.97it/s, loss=0.0167, v_num=0, reduced_train_loss=0.000293, global_step=1874.0, val_loss=0.103]\n",
      "Epoch 24:  97%|█████████▋| 101/104 [00:20<00:00,  4.96it/s, loss=0.127, v_num=0, reduced_train_loss=0.0132, global_step=1874.0, val_loss=0.0669]\n",
      "Epoch 24:  46%|████▌     | 48/104 [00:10<00:12,  4.37it/s, loss=0.0329, v_num=0, reduced_train_loss=0.00246, global_step=1847.0, val_loss=0.0776]]\n",
      "Epoch 24:  98%|█████████▊| 102/104 [00:20<00:00,  4.98it/s, loss=0.127, v_num=0, reduced_train_loss=0.0132, global_step=1874.0, val_loss=0.0669]\n",
      "Epoch 24:  99%|█████████▉| 103/104 [00:20<00:00,  5.00it/s, loss=0.0167, v_num=0, reduced_train_loss=0.000293, global_step=1874.0, val_loss=0.103]\n",
      "Epoch 24: 100%|██████████| 104/104 [00:20<00:00,  5.04it/s, loss=0.0167, v_num=0, reduced_train_loss=0.000293, global_step=1874.0, val_loss=0.103]\n",
      "Epoch 24:  99%|█████████▉| 103/104 [00:20<00:00,  5.00it/s, loss=0.127, v_num=0, reduced_train_loss=0.0132, global_step=1874.0, val_loss=0.0669]2023-05-03 22:04:39,055 - root - INFO - val_loss: 0.06608252972364426\n",
      "Epoch 24: 100%|██████████| 104/104 [00:20<00:00,  5.03it/s, loss=0.0167, v_num=0, reduced_train_loss=0.000293, global_step=1874.0, val_loss=0.0661]\n",
      "Epoch 25:   0%|          | 0/104 [00:00<?, ?it/s, loss=0.0167, v_num=0, reduced_train_loss=0.000293, global_step=1874.0, val_loss=0.0661]          "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 22:04:39.055444 140649257482048 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.06608252972364426\n",
      "I0503 22:04:39.119512 140522389899072 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.10501304268836975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 24: 100%|██████████| 104/104 [00:20<00:00,  5.03it/s, loss=0.127, v_num=0, reduced_train_loss=0.0132, global_step=1874.0, val_loss=0.0669]2023-05-03 22:04:39,119 - root - INFO - val_loss: 0.10501304268836975\n",
      "Epoch 24: 100%|██████████| 104/104 [00:20<00:00,  5.03it/s, loss=0.127, v_num=0, reduced_train_loss=0.0132, global_step=1874.0, val_loss=0.105] \n",
      "Epoch 24:  72%|███████▏  | 75/104 [00:17<00:06,  4.35it/s, loss=0.0823, v_num=0, reduced_train_loss=0.213, global_step=1874.0, val_loss=0.0776]   \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 25:  26%|██▌       | 27/104 [00:06<00:17,  4.36it/s, loss=0.0317, v_num=0, reduced_train_loss=0.0254, global_step=1901.0, val_loss=0.105]  \n",
      "Epoch 25:  26%|██▌       | 27/104 [00:06<00:18,  4.26it/s, loss=0.0735, v_num=0, reduced_train_loss=0.202, global_step=1901.0, val_loss=0.0661]  \n",
      "Epoch 25:  27%|██▋       | 28/104 [00:06<00:17,  4.35it/s, loss=0.0438, v_num=0, reduced_train_loss=0.255, global_step=1902.0, val_loss=0.105] \n",
      "Epoch 25:  27%|██▋       | 28/104 [00:06<00:17,  4.26it/s, loss=0.0711, v_num=0, reduced_train_loss=0.000315, global_step=1902.0, val_loss=0.0661]\n",
      "Epoch 25:  28%|██▊       | 29/104 [00:06<00:17,  4.35it/s, loss=0.0438, v_num=0, reduced_train_loss=9.87e-5, global_step=1903.0, val_loss=0.105]\n",
      "Epoch 25:  28%|██▊       | 29/104 [00:06<00:17,  4.26it/s, loss=0.071, v_num=0, reduced_train_loss=0.000383, global_step=1903.0, val_loss=0.0661] \n",
      "Epoch 25:  29%|██▉       | 30/104 [00:07<00:17,  4.26it/s, loss=0.0846, v_num=0, reduced_train_loss=0.273, global_step=1904.0, val_loss=0.0661]  \n",
      "Epoch 24:  79%|███████▉  | 82/104 [00:18<00:04,  4.51it/s, loss=0.0823, v_num=0, reduced_train_loss=0.213, global_step=1874.0, val_loss=0.0776]\n",
      "Epoch 25:  30%|██▉       | 31/104 [00:07<00:17,  4.27it/s, loss=0.0365, v_num=0, reduced_train_loss=0.000257, global_step=1905.0, val_loss=0.0661]\n",
      "Epoch 25:  31%|███       | 32/104 [00:07<00:16,  4.36it/s, loss=0.0494, v_num=0, reduced_train_loss=0.00119, global_step=1906.0, val_loss=0.105]\n",
      "Epoch 25:  31%|███       | 32/104 [00:07<00:16,  4.27it/s, loss=0.0365, v_num=0, reduced_train_loss=0.000434, global_step=1906.0, val_loss=0.0661]\n",
      "Epoch 25:  32%|███▏      | 33/104 [00:07<00:16,  4.36it/s, loss=0.0241, v_num=0, reduced_train_loss=0.00198, global_step=1907.0, val_loss=0.105]\n",
      "Epoch 25:  32%|███▏      | 33/104 [00:07<00:16,  4.28it/s, loss=0.0365, v_num=0, reduced_train_loss=0.000373, global_step=1907.0, val_loss=0.0661]\n",
      "Epoch 25:  33%|███▎      | 34/104 [00:07<00:16,  4.36it/s, loss=0.0424, v_num=0, reduced_train_loss=0.367, global_step=1908.0, val_loss=0.105]  \n",
      "Epoch 25:  33%|███▎      | 34/104 [00:07<00:16,  4.28it/s, loss=0.0371, v_num=0, reduced_train_loss=0.0154, global_step=1908.0, val_loss=0.0661]  \n",
      "Epoch 25:  34%|███▎      | 35/104 [00:08<00:15,  4.36it/s, loss=0.0425, v_num=0, reduced_train_loss=0.00257, global_step=1909.0, val_loss=0.105]\n",
      "Epoch 25:  34%|███▎      | 35/104 [00:08<00:16,  4.28it/s, loss=0.0397, v_num=0, reduced_train_loss=0.0516, global_step=1909.0, val_loss=0.0661]\n",
      "Epoch 25:  35%|███▍      | 36/104 [00:08<00:15,  4.28it/s, loss=0.0411, v_num=0, reduced_train_loss=0.0428, global_step=1910.0, val_loss=0.0661]\n",
      "Epoch 24:  89%|████████▉ | 93/104 [00:19<00:02,  4.76it/s, loss=0.0823, v_num=0, reduced_train_loss=0.213, global_step=1874.0, val_loss=0.0776]\n",
      "Epoch 25:  36%|███▌      | 37/104 [00:08<00:15,  4.29it/s, loss=0.0413, v_num=0, reduced_train_loss=0.00405, global_step=1911.0, val_loss=0.0661]\n",
      "Epoch 25:  37%|███▋      | 38/104 [00:08<00:15,  4.36it/s, loss=0.0426, v_num=0, reduced_train_loss=0.000517, global_step=1912.0, val_loss=0.105]\n",
      "Epoch 25:  37%|███▋      | 38/104 [00:08<00:15,  4.29it/s, loss=0.0413, v_num=0, reduced_train_loss=0.000444, global_step=1912.0, val_loss=0.0661]\n",
      "Epoch 25:  38%|███▊      | 39/104 [00:08<00:14,  4.36it/s, loss=0.0395, v_num=0, reduced_train_loss=0.000129, global_step=1913.0, val_loss=0.105]\n",
      "Epoch 25:  38%|███▊      | 39/104 [00:09<00:15,  4.29it/s, loss=0.0413, v_num=0, reduced_train_loss=0.000613, global_step=1913.0, val_loss=0.0661]\n",
      "Epoch 25:  38%|███▊      | 40/104 [00:09<00:14,  4.36it/s, loss=0.0399, v_num=0, reduced_train_loss=0.00675, global_step=1914.0, val_loss=0.105] \n",
      "Epoch 25:  38%|███▊      | 40/104 [00:09<00:14,  4.29it/s, loss=0.0413, v_num=0, reduced_train_loss=0.000277, global_step=1914.0, val_loss=0.0661]\n",
      "Epoch 25:  39%|███▉      | 41/104 [00:09<00:14,  4.36it/s, loss=0.0493, v_num=0, reduced_train_loss=0.189, global_step=1915.0, val_loss=0.105]  \n",
      "Epoch 25:  39%|███▉      | 41/104 [00:09<00:14,  4.29it/s, loss=0.0411, v_num=0, reduced_train_loss=0.000817, global_step=1915.0, val_loss=0.0661]\n",
      "Epoch 25:  40%|████      | 42/104 [00:09<00:14,  4.36it/s, loss=0.0499, v_num=0, reduced_train_loss=0.0134, global_step=1916.0, val_loss=0.105]]\n",
      "Epoch 24: 100%|██████████| 104/104 [00:20<00:00,  4.99it/s, loss=0.0823, v_num=0, reduced_train_loss=0.213, global_step=1874.0, val_loss=0.0776]2023-05-03 22:04:48,798 - root - INFO - val_loss: 0.0865817442536354\n",
      "Epoch 24: 100%|██████████| 104/104 [00:20<00:00,  4.98it/s, loss=0.0823, v_num=0, reduced_train_loss=0.213, global_step=1874.0, val_loss=0.0866]\n",
      "Epoch 25:  40%|████      | 42/104 [00:09<00:14,  4.29it/s, loss=0.0302, v_num=0, reduced_train_loss=0.00171, global_step=1916.0, val_loss=0.0661] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 22:04:48.798007 139755592410944 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.0865817442536354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25:  31%|███       | 32/104 [00:07<00:16,  4.31it/s, loss=0.0562, v_num=0, reduced_train_loss=0.00227, global_step=1906.0, val_loss=0.0866]]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 25:  72%|███████▏  | 75/104 [00:17<00:06,  4.33it/s, loss=0.0134, v_num=0, reduced_train_loss=0.00152, global_step=1949.0, val_loss=0.0661] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 25:  32%|███▏      | 33/104 [00:07<00:16,  4.31it/s, loss=0.0506, v_num=0, reduced_train_loss=0.00055, global_step=1907.0, val_loss=0.0866]\n",
      "Epoch 25:  74%|███████▍  | 77/104 [00:17<00:06,  4.42it/s, loss=0.0246, v_num=0, reduced_train_loss=0.00934, global_step=1949.0, val_loss=0.105]\n",
      "Epoch 25:  73%|███████▎  | 76/104 [00:17<00:06,  4.34it/s, loss=0.0134, v_num=0, reduced_train_loss=0.00152, global_step=1949.0, val_loss=0.0661]\n",
      "Epoch 25:  33%|███▎      | 34/104 [00:07<00:16,  4.31it/s, loss=0.0506, v_num=0, reduced_train_loss=0.00055, global_step=1907.0, val_loss=0.0866]\n",
      "Epoch 25:  74%|███████▍  | 77/104 [00:17<00:06,  4.37it/s, loss=0.0134, v_num=0, reduced_train_loss=0.00152, global_step=1949.0, val_loss=0.0661]\n",
      "Epoch 25:  76%|███████▌  | 79/104 [00:17<00:05,  4.47it/s, loss=0.0246, v_num=0, reduced_train_loss=0.00934, global_step=1949.0, val_loss=0.105]\n",
      "Epoch 25:  75%|███████▌  | 78/104 [00:17<00:05,  4.39it/s, loss=0.0134, v_num=0, reduced_train_loss=0.00152, global_step=1949.0, val_loss=0.0661]\n",
      "Epoch 25:  34%|███▎      | 35/104 [00:08<00:15,  4.31it/s, loss=0.0484, v_num=0, reduced_train_loss=0.000423, global_step=1909.0, val_loss=0.0866]\n",
      "Epoch 25:  76%|███████▌  | 79/104 [00:17<00:05,  4.42it/s, loss=0.0134, v_num=0, reduced_train_loss=0.00152, global_step=1949.0, val_loss=0.0661]\n",
      "Epoch 25:  78%|███████▊  | 81/104 [00:17<00:05,  4.52it/s, loss=0.0246, v_num=0, reduced_train_loss=0.00934, global_step=1949.0, val_loss=0.105]\n",
      "Epoch 25:  35%|███▍      | 36/104 [00:08<00:15,  4.31it/s, loss=0.0429, v_num=0, reduced_train_loss=0.0187, global_step=1910.0, val_loss=0.0866]  \n",
      "Epoch 25:  79%|███████▉  | 82/104 [00:18<00:04,  4.55it/s, loss=0.0246, v_num=0, reduced_train_loss=0.00934, global_step=1949.0, val_loss=0.105]\n",
      "Epoch 25:  78%|███████▊  | 81/104 [00:18<00:05,  4.47it/s, loss=0.0134, v_num=0, reduced_train_loss=0.00152, global_step=1949.0, val_loss=0.0661]\n",
      "Epoch 25:  80%|███████▉  | 83/104 [00:18<00:04,  4.57it/s, loss=0.0246, v_num=0, reduced_train_loss=0.00934, global_step=1949.0, val_loss=0.105]\n",
      "Epoch 25:  36%|███▌      | 37/104 [00:08<00:15,  4.32it/s, loss=0.0604, v_num=0, reduced_train_loss=0.414, global_step=1911.0, val_loss=0.0866] ]\n",
      "Epoch 25:  81%|████████  | 84/104 [00:18<00:04,  4.59it/s, loss=0.0246, v_num=0, reduced_train_loss=0.00934, global_step=1949.0, val_loss=0.105]\n",
      "Epoch 25:  80%|███████▉  | 83/104 [00:18<00:04,  4.51it/s, loss=0.0134, v_num=0, reduced_train_loss=0.00152, global_step=1949.0, val_loss=0.0661]\n",
      "Epoch 25:  82%|████████▏ | 85/104 [00:18<00:04,  4.62it/s, loss=0.0246, v_num=0, reduced_train_loss=0.00934, global_step=1949.0, val_loss=0.105]\n",
      "Epoch 25:  37%|███▋      | 38/104 [00:08<00:15,  4.32it/s, loss=0.0549, v_num=0, reduced_train_loss=0.00248, global_step=1912.0, val_loss=0.0866]\n",
      "Epoch 25:  83%|████████▎ | 86/104 [00:18<00:03,  4.64it/s, loss=0.0246, v_num=0, reduced_train_loss=0.00934, global_step=1949.0, val_loss=0.105]\n",
      "Epoch 25:  82%|████████▏ | 85/104 [00:18<00:04,  4.56it/s, loss=0.0134, v_num=0, reduced_train_loss=0.00152, global_step=1949.0, val_loss=0.0661]\n",
      "Epoch 25:  84%|████████▎ | 87/104 [00:18<00:03,  4.66it/s, loss=0.0246, v_num=0, reduced_train_loss=0.00934, global_step=1949.0, val_loss=0.105]\n",
      "Epoch 25:  38%|███▊      | 39/104 [00:09<00:15,  4.31it/s, loss=0.0525, v_num=0, reduced_train_loss=0.0093, global_step=1913.0, val_loss=0.0866] \n",
      "Epoch 25:  85%|████████▍ | 88/104 [00:18<00:03,  4.68it/s, loss=0.0246, v_num=0, reduced_train_loss=0.00934, global_step=1949.0, val_loss=0.105]\n",
      "Epoch 25:  84%|████████▎ | 87/104 [00:18<00:03,  4.60it/s, loss=0.0134, v_num=0, reduced_train_loss=0.00152, global_step=1949.0, val_loss=0.0661]\n",
      "Epoch 25:  38%|███▊      | 40/104 [00:09<00:14,  4.31it/s, loss=0.0589, v_num=0, reduced_train_loss=0.132, global_step=1914.0, val_loss=0.0866] \n",
      "Epoch 25:  85%|████████▍ | 88/104 [00:19<00:03,  4.63it/s, loss=0.0134, v_num=0, reduced_train_loss=0.00152, global_step=1949.0, val_loss=0.0661]\n",
      "Epoch 25:  87%|████████▋ | 90/104 [00:19<00:02,  4.73it/s, loss=0.0246, v_num=0, reduced_train_loss=0.00934, global_step=1949.0, val_loss=0.105]\n",
      "Epoch 25:  86%|████████▌ | 89/104 [00:19<00:03,  4.65it/s, loss=0.0134, v_num=0, reduced_train_loss=0.00152, global_step=1949.0, val_loss=0.0661]\n",
      "Epoch 25:  39%|███▉      | 41/104 [00:09<00:14,  4.31it/s, loss=0.0591, v_num=0, reduced_train_loss=0.00334, global_step=1915.0, val_loss=0.0866]\n",
      "Epoch 25:  87%|████████▋ | 90/104 [00:19<00:02,  4.67it/s, loss=0.0134, v_num=0, reduced_train_loss=0.00152, global_step=1949.0, val_loss=0.0661]\n",
      "Epoch 25:  88%|████████▊ | 92/104 [00:19<00:02,  4.77it/s, loss=0.0246, v_num=0, reduced_train_loss=0.00934, global_step=1949.0, val_loss=0.105]\n",
      "Epoch 25:  88%|████████▊ | 91/104 [00:19<00:02,  4.69it/s, loss=0.0134, v_num=0, reduced_train_loss=0.00152, global_step=1949.0, val_loss=0.0661]\n",
      "Epoch 25:  40%|████      | 42/104 [00:09<00:14,  4.32it/s, loss=0.0579, v_num=0, reduced_train_loss=0.00523, global_step=1916.0, val_loss=0.0866]\n",
      "Epoch 25:  88%|████████▊ | 92/104 [00:19<00:02,  4.71it/s, loss=0.0134, v_num=0, reduced_train_loss=0.00152, global_step=1949.0, val_loss=0.0661]\n",
      "Epoch 25:  90%|█████████ | 94/104 [00:19<00:02,  4.81it/s, loss=0.0246, v_num=0, reduced_train_loss=0.00934, global_step=1949.0, val_loss=0.105]\n",
      "Epoch 25:  41%|████▏     | 43/104 [00:09<00:14,  4.32it/s, loss=0.0482, v_num=0, reduced_train_loss=0.00151, global_step=1917.0, val_loss=0.0866]\n",
      "Epoch 25:  91%|█████████▏| 95/104 [00:19<00:01,  4.83it/s, loss=0.0246, v_num=0, reduced_train_loss=0.00934, global_step=1949.0, val_loss=0.105]\n",
      "Epoch 25:  90%|█████████ | 94/104 [00:19<00:02,  4.75it/s, loss=0.0134, v_num=0, reduced_train_loss=0.00152, global_step=1949.0, val_loss=0.0661]\n",
      "Epoch 25:  92%|█████████▏| 96/104 [00:19<00:01,  4.85it/s, loss=0.0246, v_num=0, reduced_train_loss=0.00934, global_step=1949.0, val_loss=0.105]\n",
      "Epoch 25:  42%|████▏     | 44/104 [00:10<00:13,  4.32it/s, loss=0.0497, v_num=0, reduced_train_loss=0.0301, global_step=1918.0, val_loss=0.0866] \n",
      "Epoch 25:  93%|█████████▎| 97/104 [00:19<00:01,  4.87it/s, loss=0.0246, v_num=0, reduced_train_loss=0.00934, global_step=1949.0, val_loss=0.105]\n",
      "Epoch 25:  92%|█████████▏| 96/104 [00:20<00:01,  4.79it/s, loss=0.0134, v_num=0, reduced_train_loss=0.00152, global_step=1949.0, val_loss=0.0661]\n",
      "Epoch 25:  94%|█████████▍| 98/104 [00:20<00:01,  4.89it/s, loss=0.0246, v_num=0, reduced_train_loss=0.00934, global_step=1949.0, val_loss=0.105]\n",
      "Epoch 25:  43%|████▎     | 45/104 [00:10<00:13,  4.32it/s, loss=0.0498, v_num=0, reduced_train_loss=0.0034, global_step=1919.0, val_loss=0.0866]]\n",
      "Epoch 25:  95%|█████████▌| 99/104 [00:20<00:01,  4.91it/s, loss=0.0246, v_num=0, reduced_train_loss=0.00934, global_step=1949.0, val_loss=0.105]\n",
      "Epoch 25:  94%|█████████▍| 98/104 [00:20<00:01,  4.83it/s, loss=0.0134, v_num=0, reduced_train_loss=0.00152, global_step=1949.0, val_loss=0.0661]\n",
      "Epoch 25:  44%|████▍     | 46/104 [00:10<00:13,  4.32it/s, loss=0.0499, v_num=0, reduced_train_loss=0.020, global_step=1920.0, val_loss=0.0866] ]\n",
      "Epoch 25:  95%|█████████▌| 99/104 [00:20<00:01,  4.85it/s, loss=0.0134, v_num=0, reduced_train_loss=0.00152, global_step=1949.0, val_loss=0.0661]\n",
      "Epoch 25:  97%|█████████▋| 101/104 [00:20<00:00,  4.95it/s, loss=0.0246, v_num=0, reduced_train_loss=0.00934, global_step=1949.0, val_loss=0.105]\n",
      "Epoch 25:  96%|█████████▌| 100/104 [00:20<00:00,  4.87it/s, loss=0.0134, v_num=0, reduced_train_loss=0.00152, global_step=1949.0, val_loss=0.0661]\n",
      "Epoch 25:  45%|████▌     | 47/104 [00:10<00:13,  4.32it/s, loss=0.0504, v_num=0, reduced_train_loss=0.0166, global_step=1921.0, val_loss=0.0866]]\n",
      "Epoch 25:  97%|█████████▋| 101/104 [00:20<00:00,  4.89it/s, loss=0.0134, v_num=0, reduced_train_loss=0.00152, global_step=1949.0, val_loss=0.0661]\n",
      "Epoch 25:  99%|█████████▉| 103/104 [00:20<00:00,  4.99it/s, loss=0.0246, v_num=0, reduced_train_loss=0.00934, global_step=1949.0, val_loss=0.105]\n",
      "Epoch 25: 100%|██████████| 104/104 [00:20<00:00,  5.02it/s, loss=0.0246, v_num=0, reduced_train_loss=0.00934, global_step=1949.0, val_loss=0.105]2023-05-03 22:04:59,836 - root - INFO - val_loss: 0.08488143235445023\n",
      "Epoch 25: 100%|██████████| 104/104 [00:20<00:00,  5.02it/s, loss=0.0246, v_num=0, reduced_train_loss=0.00934, global_step=1949.0, val_loss=0.0849]\n",
      "Epoch 26:   0%|          | 0/104 [00:00<?, ?it/s, loss=0.0246, v_num=0, reduced_train_loss=0.00934, global_step=1949.0, val_loss=0.0849]          \n",
      "Epoch 25:  98%|█████████▊| 102/104 [00:20<00:00,  4.91it/s, loss=0.0134, v_num=0, reduced_train_loss=0.00152, global_step=1949.0, val_loss=0.0661]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 22:04:59.836790 140522389899072 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.08488143235445023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25:  46%|████▌     | 48/104 [00:11<00:12,  4.32it/s, loss=0.0517, v_num=0, reduced_train_loss=0.0465, global_step=1922.0, val_loss=0.0866]\n",
      "Epoch 25:  99%|█████████▉| 103/104 [00:20<00:00,  4.93it/s, loss=0.0134, v_num=0, reduced_train_loss=0.00152, global_step=1949.0, val_loss=0.0661]\n",
      "Epoch 25: 100%|██████████| 104/104 [00:20<00:00,  4.96it/s, loss=0.0134, v_num=0, reduced_train_loss=0.00152, global_step=1949.0, val_loss=0.0661]2023-05-03 22:05:00,037 - root - INFO - val_loss: 0.07081040740013123\n",
      "Epoch 25: 100%|██████████| 104/104 [00:20<00:00,  4.96it/s, loss=0.0134, v_num=0, reduced_train_loss=0.00152, global_step=1949.0, val_loss=0.0708]\n",
      "Epoch 26:   0%|          | 0/104 [00:00<?, ?it/s, loss=0.0134, v_num=0, reduced_train_loss=0.00152, global_step=1949.0, val_loss=0.0708]          "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 22:05:00.037827 140649257482048 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.07081040740013123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25:  72%|███████▏  | 75/104 [00:17<00:06,  4.32it/s, loss=0.1, v_num=0, reduced_train_loss=0.0249, global_step=1949.0, val_loss=0.0866]   ]  \n",
      "Epoch 26:  26%|██▌       | 27/104 [00:06<00:18,  4.26it/s, loss=0.0242, v_num=0, reduced_train_loss=0.000109, global_step=1976.0, val_loss=0.0849]\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 26:  26%|██▌       | 27/104 [00:06<00:17,  4.37it/s, loss=0.0155, v_num=0, reduced_train_loss=0.00076, global_step=1976.0, val_loss=0.0708]\n",
      "Epoch 26:  27%|██▋       | 28/104 [00:06<00:17,  4.37it/s, loss=0.0157, v_num=0, reduced_train_loss=0.00434, global_step=1977.0, val_loss=0.0708]]\n",
      "Epoch 25:  74%|███████▍  | 77/104 [00:17<00:06,  4.36it/s, loss=0.1, v_num=0, reduced_train_loss=0.0249, global_step=1949.0, val_loss=0.0866]\n",
      "Epoch 26:  28%|██▊       | 29/104 [00:06<00:17,  4.37it/s, loss=0.0154, v_num=0, reduced_train_loss=9.84e-5, global_step=1978.0, val_loss=0.0708]]\n",
      "Epoch 25:  76%|███████▌  | 79/104 [00:17<00:05,  4.41it/s, loss=0.1, v_num=0, reduced_train_loss=0.0249, global_step=1949.0, val_loss=0.0866]\n",
      "Epoch 26:  29%|██▉       | 30/104 [00:06<00:16,  4.37it/s, loss=0.0155, v_num=0, reduced_train_loss=0.0024, global_step=1979.0, val_loss=0.0708] ]\n",
      "Epoch 26:  30%|██▉       | 31/104 [00:07<00:17,  4.28it/s, loss=0.0169, v_num=0, reduced_train_loss=7.1e-5, global_step=1980.0, val_loss=0.0849]  \n",
      "Epoch 26:  30%|██▉       | 31/104 [00:07<00:16,  4.37it/s, loss=0.0155, v_num=0, reduced_train_loss=0.00031, global_step=1980.0, val_loss=0.0708]\n",
      "Epoch 26:  31%|███       | 32/104 [00:07<00:16,  4.28it/s, loss=0.0168, v_num=0, reduced_train_loss=0.000589, global_step=1981.0, val_loss=0.0849]\n",
      "Epoch 26:  31%|███       | 32/104 [00:07<00:16,  4.37it/s, loss=0.0157, v_num=0, reduced_train_loss=0.00268, global_step=1981.0, val_loss=0.0708]\n",
      "Epoch 26:  32%|███▏      | 33/104 [00:07<00:16,  4.38it/s, loss=0.0157, v_num=0, reduced_train_loss=0.000685, global_step=1982.0, val_loss=0.0708]\n",
      "Epoch 25:  83%|████████▎ | 86/104 [00:18<00:03,  4.57it/s, loss=0.1, v_num=0, reduced_train_loss=0.0249, global_step=1949.0, val_loss=0.0866]\n",
      "Epoch 26:  33%|███▎      | 34/104 [00:07<00:15,  4.38it/s, loss=0.0159, v_num=0, reduced_train_loss=0.0164, global_step=1983.0, val_loss=0.0708]  \n",
      "Epoch 26:  34%|███▎      | 35/104 [00:08<00:16,  4.29it/s, loss=0.0181, v_num=0, reduced_train_loss=0.0362, global_step=1984.0, val_loss=0.0849]\n",
      "Epoch 26:  34%|███▎      | 35/104 [00:07<00:15,  4.38it/s, loss=0.0159, v_num=0, reduced_train_loss=6.73e-5, global_step=1984.0, val_loss=0.0708]\n",
      "Epoch 26:  35%|███▍      | 36/104 [00:08<00:15,  4.30it/s, loss=0.0185, v_num=0, reduced_train_loss=0.00807, global_step=1985.0, val_loss=0.0849]\n",
      "Epoch 26:  35%|███▍      | 36/104 [00:08<00:15,  4.38it/s, loss=0.0158, v_num=0, reduced_train_loss=0.00243, global_step=1985.0, val_loss=0.0708]\n",
      "Epoch 26:  36%|███▌      | 37/104 [00:08<00:15,  4.30it/s, loss=0.0167, v_num=0, reduced_train_loss=0.00147, global_step=1986.0, val_loss=0.0849]\n",
      "Epoch 26:  36%|███▌      | 37/104 [00:08<00:15,  4.38it/s, loss=0.0158, v_num=0, reduced_train_loss=0.000353, global_step=1986.0, val_loss=0.0708]\n",
      "Epoch 26:  37%|███▋      | 38/104 [00:08<00:15,  4.38it/s, loss=0.0158, v_num=0, reduced_train_loss=0.000528, global_step=1987.0, val_loss=0.0708]\n",
      "Epoch 25:  91%|█████████▏| 95/104 [00:19<00:01,  4.76it/s, loss=0.1, v_num=0, reduced_train_loss=0.0249, global_step=1949.0, val_loss=0.0866]\n",
      "Epoch 26:  38%|███▊      | 39/104 [00:08<00:14,  4.38it/s, loss=0.0153, v_num=0, reduced_train_loss=0.000153, global_step=1988.0, val_loss=0.0708]\n",
      "Epoch 25:  93%|█████████▎| 97/104 [00:20<00:01,  4.80it/s, loss=0.1, v_num=0, reduced_train_loss=0.0249, global_step=1949.0, val_loss=0.0866]\n",
      "Epoch 26:  38%|███▊      | 40/104 [00:09<00:14,  4.38it/s, loss=0.0155, v_num=0, reduced_train_loss=0.00576, global_step=1989.0, val_loss=0.0708] \n",
      "Epoch 26:  39%|███▉      | 41/104 [00:09<00:14,  4.31it/s, loss=0.0316, v_num=0, reduced_train_loss=0.000147, global_step=1990.0, val_loss=0.0849]\n",
      "Epoch 26:  39%|███▉      | 41/104 [00:09<00:14,  4.38it/s, loss=0.0154, v_num=0, reduced_train_loss=0.00107, global_step=1990.0, val_loss=0.0708]\n",
      "Epoch 26:  40%|████      | 42/104 [00:09<00:14,  4.31it/s, loss=0.0315, v_num=0, reduced_train_loss=0.000467, global_step=1991.0, val_loss=0.0849]\n",
      "Epoch 26:  40%|████      | 42/104 [00:09<00:14,  4.38it/s, loss=0.0156, v_num=0, reduced_train_loss=0.00377, global_step=1991.0, val_loss=0.0708]\n",
      "Epoch 26:  41%|████▏     | 43/104 [00:09<00:14,  4.32it/s, loss=0.0258, v_num=0, reduced_train_loss=0.038, global_step=1992.0, val_loss=0.0849]   \n",
      "Epoch 25: 100%|██████████| 104/104 [00:21<00:00,  4.95it/s, loss=0.1, v_num=0, reduced_train_loss=0.0249, global_step=1949.0, val_loss=0.0866]2023-05-03 22:05:09,829 - root - INFO - val_loss: 0.10879460722208023\n",
      "Epoch 25: 100%|██████████| 104/104 [00:21<00:00,  4.95it/s, loss=0.1, v_num=0, reduced_train_loss=0.0249, global_step=1949.0, val_loss=0.109] \n",
      "Epoch 26:  41%|████▏     | 43/104 [00:09<00:13,  4.38it/s, loss=0.00244, v_num=0, reduced_train_loss=0.00177, global_step=1992.0, val_loss=0.0708]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 22:05:09.829492 139755592410944 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.10879460722208023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26:  72%|███████▏  | 75/104 [00:16<00:06,  4.43it/s, loss=0.0567, v_num=0, reduced_train_loss=0.0503, global_step=2024.0, val_loss=0.0849] ] \n",
      "Epoch 26:  29%|██▉       | 30/104 [00:06<00:17,  4.30it/s, loss=0.0467, v_num=0, reduced_train_loss=0.00522, global_step=1979.0, val_loss=0.109]\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 26:  71%|███████   | 74/104 [00:16<00:06,  4.39it/s, loss=0.0164, v_num=0, reduced_train_loss=0.000492, global_step=2023.0, val_loss=0.0708]\n",
      "Epoch 26:  30%|██▉       | 31/104 [00:07<00:16,  4.31it/s, loss=0.0277, v_num=0, reduced_train_loss=0.0107, global_step=1980.0, val_loss=0.109] \n",
      "Epoch 26:  72%|███████▏  | 75/104 [00:17<00:06,  4.39it/s, loss=0.0164, v_num=0, reduced_train_loss=0.00115, global_step=2024.0, val_loss=0.0708] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 26:  31%|███       | 32/104 [00:07<00:16,  4.31it/s, loss=0.035, v_num=0, reduced_train_loss=0.156, global_step=1981.0, val_loss=0.109]  ]\n",
      "Epoch 26:  73%|███████▎  | 76/104 [00:17<00:06,  4.40it/s, loss=0.0164, v_num=0, reduced_train_loss=0.00115, global_step=2024.0, val_loss=0.0708]\n",
      "Epoch 26:  76%|███████▌  | 79/104 [00:17<00:05,  4.52it/s, loss=0.0567, v_num=0, reduced_train_loss=0.0503, global_step=2024.0, val_loss=0.0849]\n",
      "Epoch 26:  74%|███████▍  | 77/104 [00:17<00:06,  4.43it/s, loss=0.0164, v_num=0, reduced_train_loss=0.00115, global_step=2024.0, val_loss=0.0708]\n",
      "Epoch 26:  32%|███▏      | 33/104 [00:07<00:16,  4.31it/s, loss=0.0356, v_num=0, reduced_train_loss=0.0302, global_step=1982.0, val_loss=0.109]]\n",
      "Epoch 26:  78%|███████▊  | 81/104 [00:17<00:05,  4.57it/s, loss=0.0567, v_num=0, reduced_train_loss=0.0503, global_step=2024.0, val_loss=0.0849]\n",
      "Epoch 26:  75%|███████▌  | 78/104 [00:17<00:05,  4.45it/s, loss=0.0164, v_num=0, reduced_train_loss=0.00115, global_step=2024.0, val_loss=0.0708]\n",
      "Epoch 26:  79%|███████▉  | 82/104 [00:17<00:04,  4.60it/s, loss=0.0567, v_num=0, reduced_train_loss=0.0503, global_step=2024.0, val_loss=0.0849]\n",
      "Epoch 26:  33%|███▎      | 34/104 [00:07<00:16,  4.32it/s, loss=0.0332, v_num=0, reduced_train_loss=0.0106, global_step=1983.0, val_loss=0.109]8]\n",
      "Epoch 26:  80%|███████▉  | 83/104 [00:17<00:04,  4.62it/s, loss=0.0567, v_num=0, reduced_train_loss=0.0503, global_step=2024.0, val_loss=0.0849]\n",
      "Epoch 26:  77%|███████▋  | 80/104 [00:17<00:05,  4.50it/s, loss=0.0164, v_num=0, reduced_train_loss=0.00115, global_step=2024.0, val_loss=0.0708]\n",
      "Epoch 26:  34%|███▎      | 35/104 [00:08<00:15,  4.32it/s, loss=0.0331, v_num=0, reduced_train_loss=0.000242, global_step=1984.0, val_loss=0.109]\n",
      "Epoch 26:  78%|███████▊  | 81/104 [00:17<00:05,  4.53it/s, loss=0.0164, v_num=0, reduced_train_loss=0.00115, global_step=2024.0, val_loss=0.0708]\n",
      "Epoch 26:  82%|████████▏ | 85/104 [00:18<00:04,  4.67it/s, loss=0.0567, v_num=0, reduced_train_loss=0.0503, global_step=2024.0, val_loss=0.0849]\n",
      "Epoch 26:  35%|███▍      | 36/104 [00:08<00:15,  4.32it/s, loss=0.029, v_num=0, reduced_train_loss=0.134, global_step=1985.0, val_loss=0.109]    \n",
      "Epoch 26:  83%|████████▎ | 86/104 [00:18<00:03,  4.69it/s, loss=0.0567, v_num=0, reduced_train_loss=0.0503, global_step=2024.0, val_loss=0.0849]\n",
      "Epoch 26:  80%|███████▉  | 83/104 [00:18<00:04,  4.57it/s, loss=0.0164, v_num=0, reduced_train_loss=0.00115, global_step=2024.0, val_loss=0.0708]\n",
      "Epoch 26:  84%|████████▎ | 87/104 [00:18<00:03,  4.72it/s, loss=0.0567, v_num=0, reduced_train_loss=0.0503, global_step=2024.0, val_loss=0.0849]\n",
      "Epoch 26:  36%|███▌      | 37/104 [00:08<00:15,  4.33it/s, loss=0.0366, v_num=0, reduced_train_loss=0.155, global_step=1986.0, val_loss=0.109]08]\n",
      "Epoch 26:  85%|████████▍ | 88/104 [00:18<00:03,  4.74it/s, loss=0.0567, v_num=0, reduced_train_loss=0.0503, global_step=2024.0, val_loss=0.0849]\n",
      "Epoch 26:  82%|████████▏ | 85/104 [00:18<00:04,  4.62it/s, loss=0.0164, v_num=0, reduced_train_loss=0.00115, global_step=2024.0, val_loss=0.0708]\n",
      "Epoch 26:  86%|████████▌ | 89/104 [00:18<00:03,  4.76it/s, loss=0.0567, v_num=0, reduced_train_loss=0.0503, global_step=2024.0, val_loss=0.0849]\n",
      "Epoch 26:  37%|███▋      | 38/104 [00:08<00:15,  4.33it/s, loss=0.029, v_num=0, reduced_train_loss=0.00435, global_step=1987.0, val_loss=0.109]8]\n",
      "Epoch 26:  87%|████████▋ | 90/104 [00:18<00:02,  4.79it/s, loss=0.0567, v_num=0, reduced_train_loss=0.0503, global_step=2024.0, val_loss=0.0849]\n",
      "Epoch 26:  84%|████████▎ | 87/104 [00:18<00:03,  4.66it/s, loss=0.0164, v_num=0, reduced_train_loss=0.00115, global_step=2024.0, val_loss=0.0708]\n",
      "Epoch 26:  88%|████████▊ | 91/104 [00:18<00:02,  4.81it/s, loss=0.0567, v_num=0, reduced_train_loss=0.0503, global_step=2024.0, val_loss=0.0849]\n",
      "Epoch 26:  38%|███▊      | 39/104 [00:09<00:15,  4.32it/s, loss=0.0311, v_num=0, reduced_train_loss=0.0442, global_step=1988.0, val_loss=0.109]8]\n",
      "Epoch 26:  88%|████████▊ | 92/104 [00:19<00:02,  4.83it/s, loss=0.0567, v_num=0, reduced_train_loss=0.0503, global_step=2024.0, val_loss=0.0849]\n",
      "Epoch 26:  86%|████████▌ | 89/104 [00:18<00:03,  4.71it/s, loss=0.0164, v_num=0, reduced_train_loss=0.00115, global_step=2024.0, val_loss=0.0708]\n",
      "Epoch 26:  89%|████████▉ | 93/104 [00:19<00:02,  4.86it/s, loss=0.0567, v_num=0, reduced_train_loss=0.0503, global_step=2024.0, val_loss=0.0849]\n",
      "Epoch 26:  38%|███▊      | 40/104 [00:09<00:14,  4.33it/s, loss=0.0311, v_num=0, reduced_train_loss=0.000403, global_step=1989.0, val_loss=0.109]\n",
      "Epoch 26:  90%|█████████ | 94/104 [00:19<00:02,  4.88it/s, loss=0.0567, v_num=0, reduced_train_loss=0.0503, global_step=2024.0, val_loss=0.0849]\n",
      "Epoch 26:  88%|████████▊ | 91/104 [00:19<00:02,  4.75it/s, loss=0.0164, v_num=0, reduced_train_loss=0.00115, global_step=2024.0, val_loss=0.0708]\n",
      "Epoch 26:  39%|███▉      | 41/104 [00:09<00:14,  4.32it/s, loss=0.0486, v_num=0, reduced_train_loss=0.356, global_step=1990.0, val_loss=0.109]   \n",
      "Epoch 26:  88%|████████▊ | 92/104 [00:19<00:02,  4.77it/s, loss=0.0164, v_num=0, reduced_train_loss=0.00115, global_step=2024.0, val_loss=0.0708]\n",
      "Epoch 26:  92%|█████████▏| 96/104 [00:19<00:01,  4.92it/s, loss=0.0567, v_num=0, reduced_train_loss=0.0503, global_step=2024.0, val_loss=0.0849]\n",
      "Epoch 26:  89%|████████▉ | 93/104 [00:19<00:02,  4.79it/s, loss=0.0164, v_num=0, reduced_train_loss=0.00115, global_step=2024.0, val_loss=0.0708]\n",
      "Epoch 26:  40%|████      | 42/104 [00:09<00:14,  4.33it/s, loss=0.048, v_num=0, reduced_train_loss=0.00927, global_step=1991.0, val_loss=0.109]]\n",
      "Validation DataLoader 0:  79%|███████▉  | 23/29 [00:02<00:00,  8.32it/s]\u001b[A\n",
      "Epoch 26:  90%|█████████ | 94/104 [00:19<00:02,  4.81it/s, loss=0.0164, v_num=0, reduced_train_loss=0.00115, global_step=2024.0, val_loss=0.0708]\n",
      "Epoch 26:  95%|█████████▌| 99/104 [00:19<00:01,  4.99it/s, loss=0.0567, v_num=0, reduced_train_loss=0.0503, global_step=2024.0, val_loss=0.0849]\n",
      "Epoch 26:  41%|████▏     | 43/104 [00:09<00:14,  4.33it/s, loss=0.0478, v_num=0, reduced_train_loss=0.000321, global_step=1992.0, val_loss=0.109]\n",
      "Epoch 26:  96%|█████████▌| 100/104 [00:19<00:00,  5.01it/s, loss=0.0567, v_num=0, reduced_train_loss=0.0503, global_step=2024.0, val_loss=0.0849]\n",
      "Epoch 26:  92%|█████████▏| 96/104 [00:19<00:01,  4.85it/s, loss=0.0164, v_num=0, reduced_train_loss=0.00115, global_step=2024.0, val_loss=0.0708]\n",
      "Epoch 26:  97%|█████████▋| 101/104 [00:20<00:00,  5.03it/s, loss=0.0567, v_num=0, reduced_train_loss=0.0503, global_step=2024.0, val_loss=0.0849]\n",
      "Epoch 26:  42%|████▏     | 44/104 [00:10<00:13,  4.32it/s, loss=0.0478, v_num=0, reduced_train_loss=0.00202, global_step=1993.0, val_loss=0.109] \n",
      "Epoch 26:  98%|█████████▊| 102/104 [00:20<00:00,  5.05it/s, loss=0.0567, v_num=0, reduced_train_loss=0.0503, global_step=2024.0, val_loss=0.0849]\n",
      "Epoch 26:  94%|█████████▍| 98/104 [00:20<00:01,  4.89it/s, loss=0.0164, v_num=0, reduced_train_loss=0.00115, global_step=2024.0, val_loss=0.0708]\n",
      "Epoch 26:  99%|█████████▉| 103/104 [00:20<00:00,  5.07it/s, loss=0.0567, v_num=0, reduced_train_loss=0.0503, global_step=2024.0, val_loss=0.0849]\n",
      "Epoch 26:  95%|█████████▌| 99/104 [00:20<00:01,  4.91it/s, loss=0.0164, v_num=0, reduced_train_loss=0.00115, global_step=2024.0, val_loss=0.0708]\n",
      "Epoch 26: 100%|██████████| 104/104 [00:20<00:00,  5.10it/s, loss=0.0567, v_num=0, reduced_train_loss=0.0503, global_step=2024.0, val_loss=0.0849]2023-05-03 22:05:20,230 - root - INFO - val_loss: 0.09227147698402405\n",
      "Epoch 26: 100%|██████████| 104/104 [00:20<00:00,  5.10it/s, loss=0.0567, v_num=0, reduced_train_loss=0.0503, global_step=2024.0, val_loss=0.0923]\n",
      "Epoch 26:  43%|████▎     | 45/104 [00:10<00:13,  4.32it/s, loss=0.0578, v_num=0, reduced_train_loss=0.213, global_step=1994.0, val_loss=0.109]   \n",
      "Epoch 26:  96%|█████████▌| 100/104 [00:20<00:00,  4.93it/s, loss=0.0164, v_num=0, reduced_train_loss=0.00115, global_step=2024.0, val_loss=0.0708]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 22:05:20.230773 140522389899072 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.09227147698402405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 27:   1%|          | 1/104 [00:00<00:27,  3.76it/s, loss=0.0419, v_num=0, reduced_train_loss=0.000283, global_step=2025.0, val_loss=0.0923]]\n",
      "Epoch 26:  98%|█████████▊| 102/104 [00:20<00:00,  4.97it/s, loss=0.0164, v_num=0, reduced_train_loss=0.00115, global_step=2024.0, val_loss=0.0708]\n",
      "Epoch 27:   2%|▏         | 2/104 [00:00<00:24,  4.13it/s, loss=0.0418, v_num=0, reduced_train_loss=0.000862, global_step=2026.0, val_loss=0.0923]]\n",
      "Epoch 26: 100%|██████████| 104/104 [00:20<00:00,  5.02it/s, loss=0.0164, v_num=0, reduced_train_loss=0.00115, global_step=2024.0, val_loss=0.0708]2023-05-03 22:05:20,767 - root - INFO - val_loss: 0.08090215921401978\n",
      "Epoch 26: 100%|██████████| 104/104 [00:20<00:00,  5.02it/s, loss=0.0164, v_num=0, reduced_train_loss=0.00115, global_step=2024.0, val_loss=0.0809]\n",
      "Epoch 27:   0%|          | 0/104 [00:00<?, ?it/s, loss=0.0164, v_num=0, reduced_train_loss=0.00115, global_step=2024.0, val_loss=0.0809]          "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 22:05:20.767777 140649257482048 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.08090215921401978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26:  72%|███████▏  | 75/104 [00:17<00:06,  4.37it/s, loss=0.0426, v_num=0, reduced_train_loss=0.00393, global_step=2024.0, val_loss=0.109]9] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 27:  27%|██▋       | 28/104 [00:06<00:17,  4.42it/s, loss=0.00142, v_num=0, reduced_train_loss=0.00222, global_step=2052.0, val_loss=0.0809]\n",
      "Epoch 27:  31%|███       | 32/104 [00:07<00:15,  4.52it/s, loss=0.0326, v_num=0, reduced_train_loss=0.000112, global_step=2056.0, val_loss=0.0923]\n",
      "Epoch 27:  28%|██▊       | 29/104 [00:06<00:16,  4.42it/s, loss=0.00146, v_num=0, reduced_train_loss=0.00132, global_step=2053.0, val_loss=0.0809]\n",
      "Epoch 27:  29%|██▉       | 30/104 [00:06<00:16,  4.42it/s, loss=0.00146, v_num=0, reduced_train_loss=0.000283, global_step=2054.0, val_loss=0.0809]\n",
      "Epoch 26:  76%|███████▌  | 79/104 [00:17<00:05,  4.46it/s, loss=0.0426, v_num=0, reduced_train_loss=0.00393, global_step=2024.0, val_loss=0.109]\n",
      "Epoch 27:  30%|██▉       | 31/104 [00:07<00:16,  4.42it/s, loss=0.00215, v_num=0, reduced_train_loss=0.0142, global_step=2055.0, val_loss=0.0809]  \n",
      "Epoch 26:  78%|███████▊  | 81/104 [00:17<00:05,  4.50it/s, loss=0.0426, v_num=0, reduced_train_loss=0.00393, global_step=2024.0, val_loss=0.109]\n",
      "Epoch 27:  31%|███       | 32/104 [00:07<00:16,  4.43it/s, loss=0.00217, v_num=0, reduced_train_loss=0.000369, global_step=2056.0, val_loss=0.0809]\n",
      "Epoch 27:  35%|███▍      | 36/104 [00:07<00:15,  4.53it/s, loss=0.0343, v_num=0, reduced_train_loss=0.00937, global_step=2060.0, val_loss=0.0923]\n",
      "Epoch 27:  32%|███▏      | 33/104 [00:07<00:16,  4.43it/s, loss=0.00221, v_num=0, reduced_train_loss=0.00111, global_step=2057.0, val_loss=0.0809] \n",
      "Epoch 27:  36%|███▌      | 37/104 [00:08<00:14,  4.53it/s, loss=0.0349, v_num=0, reduced_train_loss=0.0137, global_step=2061.0, val_loss=0.0923] \n",
      "Epoch 27:  33%|███▎      | 34/104 [00:07<00:15,  4.43it/s, loss=0.00224, v_num=0, reduced_train_loss=0.00177, global_step=2058.0, val_loss=0.0809]\n",
      "Epoch 27:  34%|███▎      | 35/104 [00:07<00:15,  4.43it/s, loss=0.0023, v_num=0, reduced_train_loss=0.00177, global_step=2059.0, val_loss=0.0809] \n",
      "Epoch 26:  85%|████████▍ | 88/104 [00:18<00:03,  4.67it/s, loss=0.0426, v_num=0, reduced_train_loss=0.00393, global_step=2024.0, val_loss=0.109]\n",
      "Epoch 27:  35%|███▍      | 36/104 [00:08<00:15,  4.43it/s, loss=0.00172, v_num=0, reduced_train_loss=0.000107, global_step=2060.0, val_loss=0.0809]\n",
      "Epoch 26:  87%|████████▋ | 90/104 [00:19<00:02,  4.71it/s, loss=0.0426, v_num=0, reduced_train_loss=0.00393, global_step=2024.0, val_loss=0.109]\n",
      "Epoch 27:  36%|███▌      | 37/104 [00:08<00:15,  4.43it/s, loss=0.00176, v_num=0, reduced_train_loss=0.00133, global_step=2061.0, val_loss=0.0809] \n",
      "Epoch 27:  39%|███▉      | 41/104 [00:09<00:13,  4.53it/s, loss=0.0251, v_num=0, reduced_train_loss=0.000441, global_step=2065.0, val_loss=0.0923]\n",
      "Epoch 27:  37%|███▋      | 38/104 [00:08<00:14,  4.44it/s, loss=0.00288, v_num=0, reduced_train_loss=0.0227, global_step=2062.0, val_loss=0.0809] \n",
      "Epoch 27:  40%|████      | 42/104 [00:09<00:13,  4.53it/s, loss=0.0289, v_num=0, reduced_train_loss=0.0773, global_step=2066.0, val_loss=0.0923]  \n",
      "Epoch 27:  38%|███▊      | 39/104 [00:08<00:14,  4.43it/s, loss=0.00289, v_num=0, reduced_train_loss=0.000758, global_step=2063.0, val_loss=0.0809]\n",
      "Epoch 27:  41%|████▏     | 43/104 [00:09<00:13,  4.53it/s, loss=0.0288, v_num=0, reduced_train_loss=0.000359, global_step=2067.0, val_loss=0.0923]\n",
      "Epoch 27:  38%|███▊      | 40/104 [00:09<00:14,  4.43it/s, loss=0.00297, v_num=0, reduced_train_loss=0.00164, global_step=2064.0, val_loss=0.0809] \n",
      "Epoch 27:  42%|████▏     | 44/104 [00:09<00:13,  4.53it/s, loss=0.0257, v_num=0, reduced_train_loss=0.0103, global_step=2068.0, val_loss=0.0923]  \n",
      "Epoch 27:  39%|███▉      | 41/104 [00:09<00:14,  4.42it/s, loss=0.00316, v_num=0, reduced_train_loss=0.00397, global_step=2065.0, val_loss=0.0809]\n",
      "Epoch 27:  43%|████▎     | 45/104 [00:09<00:13,  4.53it/s, loss=0.0257, v_num=0, reduced_train_loss=0.000678, global_step=2069.0, val_loss=0.0923]\n",
      "Epoch 27:  44%|████▍     | 46/104 [00:10<00:12,  4.53it/s, loss=0.0218, v_num=0, reduced_train_loss=0.130, global_step=2070.0, val_loss=0.0923]   ]\n",
      "Epoch 26:  98%|█████████▊| 102/104 [00:20<00:00,  4.96it/s, loss=0.0426, v_num=0, reduced_train_loss=0.00393, global_step=2024.0, val_loss=0.109]\n",
      "Epoch 27:  41%|████▏     | 43/104 [00:09<00:13,  4.40it/s, loss=0.00318, v_num=0, reduced_train_loss=0.000751, global_step=2067.0, val_loss=0.0809]\n",
      "Epoch 26: 100%|██████████| 104/104 [00:20<00:00,  5.01it/s, loss=0.0426, v_num=0, reduced_train_loss=0.00393, global_step=2024.0, val_loss=0.109]2023-05-03 22:05:30,584 - root - INFO - val_loss: 0.10075750201940536\n",
      "Epoch 26: 100%|██████████| 104/104 [00:20<00:00,  5.01it/s, loss=0.0426, v_num=0, reduced_train_loss=0.00393, global_step=2024.0, val_loss=0.101]\n",
      "                                                                        \u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 22:05:30.584585 139755592410944 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.10075750201940536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27:  72%|███████▏  | 75/104 [00:16<00:06,  4.54it/s, loss=0.0365, v_num=0, reduced_train_loss=0.0878, global_step=2099.0, val_loss=0.0923]   \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 27:  27%|██▋       | 28/104 [00:06<00:17,  4.47it/s, loss=0.0498, v_num=0, reduced_train_loss=0.00133, global_step=2052.0, val_loss=0.101]\n",
      "Epoch 27:  68%|██████▊   | 71/104 [00:16<00:07,  4.40it/s, loss=0.0484, v_num=0, reduced_train_loss=0.000408, global_step=2095.0, val_loss=0.0809]\n",
      "Epoch 27:  69%|██████▉   | 72/104 [00:16<00:07,  4.40it/s, loss=0.0235, v_num=0, reduced_train_loss=0.000211, global_step=2096.0, val_loss=0.0809]\n",
      "Epoch 27:  75%|███████▌  | 78/104 [00:16<00:05,  4.61it/s, loss=0.0365, v_num=0, reduced_train_loss=0.0878, global_step=2099.0, val_loss=0.0923]\n",
      "Epoch 27:  70%|███████   | 73/104 [00:16<00:07,  4.40it/s, loss=0.0225, v_num=0, reduced_train_loss=0.000209, global_step=2097.0, val_loss=0.0809]\n",
      "Epoch 27:  77%|███████▋  | 80/104 [00:17<00:05,  4.66it/s, loss=0.0365, v_num=0, reduced_train_loss=0.0878, global_step=2099.0, val_loss=0.0923]\n",
      "Epoch 27:  71%|███████   | 74/104 [00:16<00:06,  4.40it/s, loss=0.022, v_num=0, reduced_train_loss=0.000714, global_step=2098.0, val_loss=0.0809] \n",
      "Epoch 27:  31%|███       | 32/104 [00:07<00:16,  4.47it/s, loss=0.048, v_num=0, reduced_train_loss=0.000726, global_step=2056.0, val_loss=0.101]\n",
      "Epoch 27:  72%|███████▏  | 75/104 [00:17<00:06,  4.41it/s, loss=0.022, v_num=0, reduced_train_loss=0.000745, global_step=2099.0, val_loss=0.0809]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 27:  32%|███▏      | 33/104 [00:07<00:15,  4.47it/s, loss=0.0511, v_num=0, reduced_train_loss=0.0644, global_step=2057.0, val_loss=0.101] \n",
      "Epoch 27:  73%|███████▎  | 76/104 [00:17<00:06,  4.42it/s, loss=0.022, v_num=0, reduced_train_loss=0.000745, global_step=2099.0, val_loss=0.0809]\n",
      "Epoch 27:  82%|████████▏ | 85/104 [00:17<00:03,  4.78it/s, loss=0.0365, v_num=0, reduced_train_loss=0.0878, global_step=2099.0, val_loss=0.0923]\n",
      "Epoch 27:  74%|███████▍  | 77/104 [00:17<00:06,  4.44it/s, loss=0.022, v_num=0, reduced_train_loss=0.000745, global_step=2099.0, val_loss=0.0809]\n",
      "Epoch 27:  33%|███▎      | 34/104 [00:07<00:15,  4.47it/s, loss=0.0511, v_num=0, reduced_train_loss=0.00231, global_step=2058.0, val_loss=0.101]\n",
      "Epoch 27:  75%|███████▌  | 78/104 [00:17<00:05,  4.47it/s, loss=0.022, v_num=0, reduced_train_loss=0.000745, global_step=2099.0, val_loss=0.0809]\n",
      "Epoch 27:  84%|████████▎ | 87/104 [00:18<00:03,  4.83it/s, loss=0.0365, v_num=0, reduced_train_loss=0.0878, global_step=2099.0, val_loss=0.0923]\n",
      "Epoch 27:  76%|███████▌  | 79/104 [00:17<00:05,  4.50it/s, loss=0.022, v_num=0, reduced_train_loss=0.000745, global_step=2099.0, val_loss=0.0809]\n",
      "Epoch 27:  34%|███▎      | 35/104 [00:07<00:15,  4.48it/s, loss=0.0215, v_num=0, reduced_train_loss=0.00303, global_step=2059.0, val_loss=0.101]\n",
      "Epoch 27:  77%|███████▋  | 80/104 [00:17<00:05,  4.52it/s, loss=0.022, v_num=0, reduced_train_loss=0.000745, global_step=2099.0, val_loss=0.0809]\n",
      "Epoch 27:  86%|████████▌ | 89/104 [00:18<00:03,  4.88it/s, loss=0.0365, v_num=0, reduced_train_loss=0.0878, global_step=2099.0, val_loss=0.0923]\n",
      "Epoch 27:  78%|███████▊  | 81/104 [00:17<00:05,  4.55it/s, loss=0.022, v_num=0, reduced_train_loss=0.000745, global_step=2099.0, val_loss=0.0809]\n",
      "Epoch 27:  35%|███▍      | 36/104 [00:08<00:15,  4.47it/s, loss=0.0472, v_num=0, reduced_train_loss=0.518, global_step=2060.0, val_loss=0.101]  \n",
      "Epoch 27:  79%|███████▉  | 82/104 [00:17<00:04,  4.57it/s, loss=0.022, v_num=0, reduced_train_loss=0.000745, global_step=2099.0, val_loss=0.0809]\n",
      "Epoch 27:  88%|████████▊ | 91/104 [00:18<00:02,  4.92it/s, loss=0.0365, v_num=0, reduced_train_loss=0.0878, global_step=2099.0, val_loss=0.0923]\n",
      "Epoch 27:  80%|███████▉  | 83/104 [00:18<00:04,  4.60it/s, loss=0.022, v_num=0, reduced_train_loss=0.000745, global_step=2099.0, val_loss=0.0809]\n",
      "Epoch 27:  36%|███▌      | 37/104 [00:08<00:14,  4.47it/s, loss=0.0472, v_num=0, reduced_train_loss=0.00604, global_step=2061.0, val_loss=0.101]\n",
      "Epoch 27:  81%|████████  | 84/104 [00:18<00:04,  4.62it/s, loss=0.022, v_num=0, reduced_train_loss=0.000745, global_step=2099.0, val_loss=0.0809]\n",
      "Epoch 27:  89%|████████▉ | 93/104 [00:18<00:02,  4.97it/s, loss=0.0365, v_num=0, reduced_train_loss=0.0878, global_step=2099.0, val_loss=0.0923]\n",
      "Epoch 27:  90%|█████████ | 94/104 [00:18<00:02,  4.99it/s, loss=0.0365, v_num=0, reduced_train_loss=0.0878, global_step=2099.0, val_loss=0.0923]\n",
      "Epoch 27:  37%|███▋      | 38/104 [00:08<00:14,  4.47it/s, loss=0.047, v_num=0, reduced_train_loss=0.00313, global_step=2062.0, val_loss=0.101] ]\n",
      "Epoch 27:  91%|█████████▏| 95/104 [00:18<00:01,  5.01it/s, loss=0.0365, v_num=0, reduced_train_loss=0.0878, global_step=2099.0, val_loss=0.0923]\n",
      "Epoch 27:  83%|████████▎ | 86/104 [00:18<00:03,  4.67it/s, loss=0.022, v_num=0, reduced_train_loss=0.000745, global_step=2099.0, val_loss=0.0809]\n",
      "Epoch 27:  38%|███▊      | 39/104 [00:08<00:14,  4.47it/s, loss=0.0588, v_num=0, reduced_train_loss=0.248, global_step=2063.0, val_loss=0.101] ]\n",
      "Epoch 27:  84%|████████▎ | 87/104 [00:18<00:03,  4.69it/s, loss=0.022, v_num=0, reduced_train_loss=0.000745, global_step=2099.0, val_loss=0.0809]\n",
      "Epoch 27:  93%|█████████▎| 97/104 [00:19<00:01,  5.05it/s, loss=0.0365, v_num=0, reduced_train_loss=0.0878, global_step=2099.0, val_loss=0.0923]\n",
      "Epoch 27:  38%|███▊      | 40/104 [00:08<00:14,  4.47it/s, loss=0.0586, v_num=0, reduced_train_loss=0.00345, global_step=2064.0, val_loss=0.101]]\n",
      "Epoch 27:  94%|█████████▍| 98/104 [00:19<00:01,  5.07it/s, loss=0.0365, v_num=0, reduced_train_loss=0.0878, global_step=2099.0, val_loss=0.0923]\n",
      "Epoch 27:  86%|████████▌ | 89/104 [00:18<00:03,  4.74it/s, loss=0.022, v_num=0, reduced_train_loss=0.000745, global_step=2099.0, val_loss=0.0809]\n",
      "Epoch 27:  95%|█████████▌| 99/104 [00:19<00:00,  5.09it/s, loss=0.0365, v_num=0, reduced_train_loss=0.0878, global_step=2099.0, val_loss=0.0923]\n",
      "Epoch 27:  39%|███▉      | 41/104 [00:09<00:14,  4.47it/s, loss=0.0537, v_num=0, reduced_train_loss=0.0017, global_step=2065.0, val_loss=0.101] ]\n",
      "Epoch 27:  96%|█████████▌| 100/104 [00:19<00:00,  5.11it/s, loss=0.0365, v_num=0, reduced_train_loss=0.0878, global_step=2099.0, val_loss=0.0923]\n",
      "Epoch 27:  88%|████████▊ | 91/104 [00:19<00:02,  4.78it/s, loss=0.022, v_num=0, reduced_train_loss=0.000745, global_step=2099.0, val_loss=0.0809]\n",
      "Epoch 27:  97%|█████████▋| 101/104 [00:19<00:00,  5.13it/s, loss=0.0365, v_num=0, reduced_train_loss=0.0878, global_step=2099.0, val_loss=0.0923]\n",
      "Epoch 27:  40%|████      | 42/104 [00:09<00:13,  4.47it/s, loss=0.0621, v_num=0, reduced_train_loss=0.175, global_step=2066.0, val_loss=0.101] 9]\n",
      "Epoch 27:  98%|█████████▊| 102/104 [00:19<00:00,  5.15it/s, loss=0.0365, v_num=0, reduced_train_loss=0.0878, global_step=2099.0, val_loss=0.0923]\n",
      "Epoch 27:  89%|████████▉ | 93/104 [00:19<00:02,  4.83it/s, loss=0.022, v_num=0, reduced_train_loss=0.000745, global_step=2099.0, val_loss=0.0809]\n",
      "Epoch 27:  99%|█████████▉| 103/104 [00:19<00:00,  5.17it/s, loss=0.0365, v_num=0, reduced_train_loss=0.0878, global_step=2099.0, val_loss=0.0923]\n",
      "Epoch 27:  90%|█████████ | 94/104 [00:19<00:02,  4.85it/s, loss=0.022, v_num=0, reduced_train_loss=0.000745, global_step=2099.0, val_loss=0.0809]\n",
      "Epoch 27: 100%|██████████| 104/104 [00:19<00:00,  5.21it/s, loss=0.0365, v_num=0, reduced_train_loss=0.0878, global_step=2099.0, val_loss=0.0923]2023-05-03 22:05:40,205 - root - INFO - val_loss: 0.1072986051440239\n",
      "Epoch 27: 100%|██████████| 104/104 [00:19<00:00,  5.21it/s, loss=0.0365, v_num=0, reduced_train_loss=0.0878, global_step=2099.0, val_loss=0.107] \n",
      "Epoch 27:  41%|████▏     | 43/104 [00:09<00:13,  4.47it/s, loss=0.0619, v_num=0, reduced_train_loss=0.00451, global_step=2067.0, val_loss=0.101]\n",
      "Epoch 27:  91%|█████████▏| 95/104 [00:19<00:01,  4.87it/s, loss=0.022, v_num=0, reduced_train_loss=0.000745, global_step=2099.0, val_loss=0.0809]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 22:05:40.205705 140522389899072 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.1072986051440239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 28:   1%|          | 1/104 [00:00<00:27,  3.78it/s, loss=0.0309, v_num=0, reduced_train_loss=0.0362, global_step=2100.0, val_loss=0.107]] ]\n",
      "Epoch 27:  93%|█████████▎| 97/104 [00:19<00:01,  4.91it/s, loss=0.022, v_num=0, reduced_train_loss=0.000745, global_step=2099.0, val_loss=0.0809]\n",
      "Epoch 28:   2%|▏         | 2/104 [00:00<00:24,  4.15it/s, loss=0.0309, v_num=0, reduced_train_loss=0.000417, global_step=2101.0, val_loss=0.107]]\n",
      "Epoch 27:  44%|████▍     | 46/104 [00:10<00:12,  4.47it/s, loss=0.0579, v_num=0, reduced_train_loss=0.0198, global_step=2070.0, val_loss=0.101]  \n",
      "Epoch 28:   3%|▎         | 3/104 [00:00<00:23,  4.29it/s, loss=0.031, v_num=0, reduced_train_loss=0.00198, global_step=2102.0, val_loss=0.107]  9]\n",
      "Epoch 27:  45%|████▌     | 47/104 [00:10<00:12,  4.47it/s, loss=0.0579, v_num=0, reduced_train_loss=0.00136, global_step=2071.0, val_loss=0.101]9]\n",
      "Epoch 28:   4%|▍         | 4/104 [00:00<00:22,  4.36it/s, loss=0.0228, v_num=0, reduced_train_loss=0.00668, global_step=2103.0, val_loss=0.107]09]\n",
      "Epoch 27:  99%|█████████▉| 103/104 [00:20<00:00,  5.03it/s, loss=0.022, v_num=0, reduced_train_loss=0.000745, global_step=2099.0, val_loss=0.0809]\n",
      "Epoch 27: 100%|██████████| 104/104 [00:20<00:00,  5.07it/s, loss=0.022, v_num=0, reduced_train_loss=0.000745, global_step=2099.0, val_loss=0.0809]2023-05-03 22:05:41,308 - root - INFO - val_loss: 0.07239259779453278\n",
      "Epoch 27: 100%|██████████| 104/104 [00:20<00:00,  5.06it/s, loss=0.022, v_num=0, reduced_train_loss=0.000745, global_step=2099.0, val_loss=0.0724]\n",
      "Epoch 28:   5%|▍         | 5/104 [00:01<00:22,  4.37it/s, loss=0.023, v_num=0, reduced_train_loss=0.00456, global_step=2104.0, val_loss=0.107]    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 22:05:41.308671 140649257482048 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.07239259779453278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27:  72%|███████▏  | 75/104 [00:16<00:06,  4.44it/s, loss=0.0265, v_num=0, reduced_train_loss=0.0403, global_step=2099.0, val_loss=0.101]24] \n",
      "Epoch 28:  32%|███▏      | 33/104 [00:07<00:15,  4.52it/s, loss=0.00822, v_num=0, reduced_train_loss=0.000432, global_step=2132.0, val_loss=0.107]\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 28:  27%|██▋       | 28/104 [00:06<00:16,  4.48it/s, loss=0.00234, v_num=0, reduced_train_loss=0.00209, global_step=2127.0, val_loss=0.0724]\n",
      "Epoch 28:  28%|██▊       | 29/104 [00:06<00:16,  4.48it/s, loss=0.00243, v_num=0, reduced_train_loss=0.00222, global_step=2128.0, val_loss=0.0724]\n",
      "Epoch 27:  74%|███████▍  | 77/104 [00:17<00:06,  4.47it/s, loss=0.0265, v_num=0, reduced_train_loss=0.0403, global_step=2099.0, val_loss=0.101]\n",
      "Epoch 28:  29%|██▉       | 30/104 [00:06<00:16,  4.48it/s, loss=0.0115, v_num=0, reduced_train_loss=0.182, global_step=2129.0, val_loss=0.0724]   \n",
      "Epoch 28:  35%|███▍      | 36/104 [00:07<00:15,  4.52it/s, loss=0.0176, v_num=0, reduced_train_loss=0.0102, global_step=2135.0, val_loss=0.107] \n",
      "Epoch 28:  30%|██▉       | 31/104 [00:06<00:16,  4.48it/s, loss=0.0115, v_num=0, reduced_train_loss=0.000817, global_step=2130.0, val_loss=0.0724]\n",
      "Epoch 28:  36%|███▌      | 37/104 [00:08<00:14,  4.53it/s, loss=0.0321, v_num=0, reduced_train_loss=0.289, global_step=2136.0, val_loss=0.107] \n",
      "Epoch 28:  31%|███       | 32/104 [00:07<00:16,  4.48it/s, loss=0.0115, v_num=0, reduced_train_loss=0.00046, global_step=2131.0, val_loss=0.0724] \n",
      "Epoch 28:  37%|███▋      | 38/104 [00:08<00:14,  4.53it/s, loss=0.0322, v_num=0, reduced_train_loss=0.00665, global_step=2137.0, val_loss=0.107]\n",
      "Epoch 28:  32%|███▏      | 33/104 [00:07<00:15,  4.48it/s, loss=0.0116, v_num=0, reduced_train_loss=0.00187, global_step=2132.0, val_loss=0.0724]\n",
      "Epoch 28:  33%|███▎      | 34/104 [00:07<00:15,  4.48it/s, loss=0.0116, v_num=0, reduced_train_loss=0.000135, global_step=2133.0, val_loss=0.0724]\n",
      "Epoch 28:  38%|███▊      | 40/104 [00:08<00:14,  4.53it/s, loss=0.0318, v_num=0, reduced_train_loss=0.000671, global_step=2139.0, val_loss=0.107]\n",
      "Epoch 28:  34%|███▎      | 35/104 [00:07<00:15,  4.48it/s, loss=0.0116, v_num=0, reduced_train_loss=0.000533, global_step=2134.0, val_loss=0.0724]\n",
      "Epoch 28:  39%|███▉      | 41/104 [00:09<00:13,  4.53it/s, loss=0.0317, v_num=0, reduced_train_loss=0.000882, global_step=2140.0, val_loss=0.107]\n",
      "Epoch 28:  35%|███▍      | 36/104 [00:08<00:15,  4.48it/s, loss=0.0116, v_num=0, reduced_train_loss=0.00123, global_step=2135.0, val_loss=0.0724] \n",
      "Epoch 28:  40%|████      | 42/104 [00:09<00:13,  4.53it/s, loss=0.0318, v_num=0, reduced_train_loss=0.0016, global_step=2141.0, val_loss=0.107]  \n",
      "Epoch 28:  36%|███▌      | 37/104 [00:08<00:14,  4.48it/s, loss=0.0114, v_num=0, reduced_train_loss=8.32e-5, global_step=2136.0, val_loss=0.0724]\n",
      "Epoch 28:  41%|████▏     | 43/104 [00:09<00:13,  4.53it/s, loss=0.0318, v_num=0, reduced_train_loss=0.000866, global_step=2142.0, val_loss=0.107]\n",
      "Epoch 28:  37%|███▋      | 38/104 [00:08<00:14,  4.48it/s, loss=0.0115, v_num=0, reduced_train_loss=0.00117, global_step=2137.0, val_loss=0.0724]\n",
      "Epoch 28:  38%|███▊      | 39/104 [00:08<00:14,  4.48it/s, loss=0.0116, v_num=0, reduced_train_loss=0.00187, global_step=2138.0, val_loss=0.0724]\n",
      "Epoch 27:  91%|█████████▏| 95/104 [00:19<00:01,  4.89it/s, loss=0.0265, v_num=0, reduced_train_loss=0.0403, global_step=2099.0, val_loss=0.101]\n",
      "Epoch 28:  38%|███▊      | 40/104 [00:08<00:14,  4.48it/s, loss=0.0108, v_num=0, reduced_train_loss=0.000742, global_step=2139.0, val_loss=0.0724]\n",
      "Epoch 28:  44%|████▍     | 46/104 [00:10<00:12,  4.53it/s, loss=0.0324, v_num=0, reduced_train_loss=0.0105, global_step=2145.0, val_loss=0.107]  \n",
      "Epoch 28:  39%|███▉      | 41/104 [00:09<00:14,  4.49it/s, loss=0.0108, v_num=0, reduced_train_loss=0.000113, global_step=2140.0, val_loss=0.0724]\n",
      "Epoch 28:  45%|████▌     | 47/104 [00:10<00:12,  4.53it/s, loss=0.0347, v_num=0, reduced_train_loss=0.0474, global_step=2146.0, val_loss=0.107]\n",
      "Epoch 28:  40%|████      | 42/104 [00:09<00:13,  4.48it/s, loss=0.0107, v_num=0, reduced_train_loss=0.000243, global_step=2141.0, val_loss=0.0724]\n",
      "Epoch 28:  46%|████▌     | 48/104 [00:10<00:12,  4.53it/s, loss=0.0488, v_num=0, reduced_train_loss=0.301, global_step=2147.0, val_loss=0.107] ]\n",
      "Epoch 28:  41%|████▏     | 43/104 [00:09<00:13,  4.48it/s, loss=0.0115, v_num=0, reduced_train_loss=0.0148, global_step=2142.0, val_loss=0.0724]  \n",
      "Epoch 28:  47%|████▋     | 49/104 [00:10<00:12,  4.53it/s, loss=0.0486, v_num=0, reduced_train_loss=0.000838, global_step=2148.0, val_loss=0.107]\n",
      "Epoch 27: 100%|██████████| 104/104 [00:20<00:00,  5.09it/s, loss=0.0265, v_num=0, reduced_train_loss=0.0403, global_step=2099.0, val_loss=0.101]2023-05-03 22:05:51,044 - root - INFO - val_loss: 0.10182167589664459\n",
      "Epoch 27: 100%|██████████| 104/104 [00:20<00:00,  5.08it/s, loss=0.0265, v_num=0, reduced_train_loss=0.0403, global_step=2099.0, val_loss=0.102]\n",
      "Epoch 28:  42%|████▏     | 44/104 [00:09<00:13,  4.48it/s, loss=0.0117, v_num=0, reduced_train_loss=0.00534, global_step=2143.0, val_loss=0.0724]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 22:05:51.044439 139755592410944 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.10182167589664459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28:  66%|██████▋   | 69/104 [00:15<00:07,  4.49it/s, loss=0.0377, v_num=0, reduced_train_loss=0.00179, global_step=2168.0, val_loss=0.0724]  \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 28:  24%|██▍       | 25/104 [00:05<00:18,  4.31it/s, loss=0.072, v_num=0, reduced_train_loss=0.463, global_step=2123.0, val_loss=0.102]\n",
      "Epoch 28:  67%|██████▋   | 70/104 [00:15<00:07,  4.49it/s, loss=0.0365, v_num=0, reduced_train_loss=0.00203, global_step=2169.0, val_loss=0.0724]\n",
      "Epoch 28:  25%|██▌       | 26/104 [00:06<00:18,  4.31it/s, loss=0.075, v_num=0, reduced_train_loss=0.0482, global_step=2125.0, val_loss=0.102] ]\n",
      "Epoch 28:  68%|██████▊   | 71/104 [00:15<00:07,  4.49it/s, loss=0.0421, v_num=0, reduced_train_loss=0.118, global_step=2170.0, val_loss=0.0724]  \n",
      "Epoch 28:  69%|██████▉   | 72/104 [00:16<00:07,  4.49it/s, loss=0.0421, v_num=0, reduced_train_loss=0.000289, global_step=2171.0, val_loss=0.0724]\n",
      "Epoch 28:  77%|███████▋  | 80/104 [00:17<00:05,  4.67it/s, loss=0.018, v_num=0, reduced_train_loss=0.000129, global_step=2174.0, val_loss=0.107]\n",
      "Epoch 28:  70%|███████   | 73/104 [00:16<00:06,  4.49it/s, loss=0.0404, v_num=0, reduced_train_loss=0.000547, global_step=2172.0, val_loss=0.0724]\n",
      "Epoch 28:  79%|███████▉  | 82/104 [00:17<00:04,  4.71it/s, loss=0.018, v_num=0, reduced_train_loss=0.000129, global_step=2174.0, val_loss=0.107]\n",
      "Epoch 28:  71%|███████   | 74/104 [00:16<00:06,  4.49it/s, loss=0.0401, v_num=0, reduced_train_loss=0.000316, global_step=2173.0, val_loss=0.0724]\n",
      "Epoch 28:  81%|████████  | 84/104 [00:17<00:04,  4.76it/s, loss=0.018, v_num=0, reduced_train_loss=0.000129, global_step=2174.0, val_loss=0.107]\n",
      "Epoch 28:  72%|███████▏  | 75/104 [00:16<00:06,  4.49it/s, loss=0.0401, v_num=0, reduced_train_loss=0.000621, global_step=2174.0, val_loss=0.0724]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 28:  83%|████████▎ | 86/104 [00:17<00:03,  4.80it/s, loss=0.018, v_num=0, reduced_train_loss=0.000129, global_step=2174.0, val_loss=0.107]\n",
      "Epoch 28:  73%|███████▎  | 76/104 [00:16<00:06,  4.50it/s, loss=0.0401, v_num=0, reduced_train_loss=0.000621, global_step=2174.0, val_loss=0.0724]\n",
      "Epoch 28:  30%|██▉       | 31/104 [00:07<00:16,  4.31it/s, loss=0.0753, v_num=0, reduced_train_loss=0.0145, global_step=2130.0, val_loss=0.102]]\n",
      "Epoch 28:  74%|███████▍  | 77/104 [00:17<00:05,  4.53it/s, loss=0.0401, v_num=0, reduced_train_loss=0.000621, global_step=2174.0, val_loss=0.0724]\n",
      "Epoch 28:  85%|████████▍ | 88/104 [00:18<00:03,  4.85it/s, loss=0.018, v_num=0, reduced_train_loss=0.000129, global_step=2174.0, val_loss=0.107]\n",
      "Epoch 28:  31%|███       | 32/104 [00:07<00:16,  4.31it/s, loss=0.0793, v_num=0, reduced_train_loss=0.0816, global_step=2131.0, val_loss=0.102]24]\n",
      "Epoch 28:  86%|████████▌ | 89/104 [00:18<00:03,  4.87it/s, loss=0.018, v_num=0, reduced_train_loss=0.000129, global_step=2174.0, val_loss=0.107]\n",
      "Epoch 28:  76%|███████▌  | 79/104 [00:17<00:05,  4.58it/s, loss=0.0401, v_num=0, reduced_train_loss=0.000621, global_step=2174.0, val_loss=0.0724]\n",
      "Epoch 28:  87%|████████▋ | 90/104 [00:18<00:02,  4.89it/s, loss=0.018, v_num=0, reduced_train_loss=0.000129, global_step=2174.0, val_loss=0.107]\n",
      "Epoch 28:  32%|███▏      | 33/104 [00:07<00:16,  4.31it/s, loss=0.0792, v_num=0, reduced_train_loss=0.000287, global_step=2132.0, val_loss=0.102]]\n",
      "Epoch 28:  88%|████████▊ | 91/104 [00:18<00:02,  4.91it/s, loss=0.018, v_num=0, reduced_train_loss=0.000129, global_step=2174.0, val_loss=0.107]\n",
      "Epoch 28:  78%|███████▊  | 81/104 [00:17<00:04,  4.62it/s, loss=0.0401, v_num=0, reduced_train_loss=0.000621, global_step=2174.0, val_loss=0.0724]\n",
      "Epoch 28:  33%|███▎      | 34/104 [00:07<00:16,  4.31it/s, loss=0.0693, v_num=0, reduced_train_loss=0.00478, global_step=2133.0, val_loss=0.102] \n",
      "Epoch 28:  79%|███████▉  | 82/104 [00:17<00:04,  4.65it/s, loss=0.0401, v_num=0, reduced_train_loss=0.000621, global_step=2174.0, val_loss=0.0724]\n",
      "Epoch 28:  89%|████████▉ | 93/104 [00:18<00:02,  4.96it/s, loss=0.018, v_num=0, reduced_train_loss=0.000129, global_step=2174.0, val_loss=0.107]\n",
      "Epoch 28:  80%|███████▉  | 83/104 [00:17<00:04,  4.67it/s, loss=0.0401, v_num=0, reduced_train_loss=0.000621, global_step=2174.0, val_loss=0.0724]\n",
      "Epoch 28:  34%|███▎      | 35/104 [00:08<00:15,  4.32it/s, loss=0.0707, v_num=0, reduced_train_loss=0.0288, global_step=2134.0, val_loss=0.102] \n",
      "Epoch 28:  81%|████████  | 84/104 [00:17<00:04,  4.70it/s, loss=0.0401, v_num=0, reduced_train_loss=0.000621, global_step=2174.0, val_loss=0.0724]\n",
      "Epoch 28:  91%|█████████▏| 95/104 [00:19<00:01,  5.00it/s, loss=0.018, v_num=0, reduced_train_loss=0.000129, global_step=2174.0, val_loss=0.107]\n",
      "Epoch 28:  82%|████████▏ | 85/104 [00:18<00:04,  4.72it/s, loss=0.0401, v_num=0, reduced_train_loss=0.000621, global_step=2174.0, val_loss=0.0724]\n",
      "Epoch 28:  35%|███▍      | 36/104 [00:08<00:15,  4.32it/s, loss=0.0707, v_num=0, reduced_train_loss=0.000558, global_step=2135.0, val_loss=0.102]\n",
      "Epoch 28:  83%|████████▎ | 86/104 [00:18<00:03,  4.74it/s, loss=0.0401, v_num=0, reduced_train_loss=0.000621, global_step=2174.0, val_loss=0.0724]\n",
      "Epoch 28:  93%|█████████▎| 97/104 [00:19<00:01,  5.04it/s, loss=0.018, v_num=0, reduced_train_loss=0.000129, global_step=2174.0, val_loss=0.107]\n",
      "Epoch 28:  84%|████████▎ | 87/104 [00:18<00:03,  4.76it/s, loss=0.0401, v_num=0, reduced_train_loss=0.000621, global_step=2174.0, val_loss=0.0724]\n",
      "Epoch 28:  36%|███▌      | 37/104 [00:08<00:15,  4.32it/s, loss=0.0517, v_num=0, reduced_train_loss=0.00108, global_step=2136.0, val_loss=0.102] \n",
      "Epoch 28:  85%|████████▍ | 88/104 [00:18<00:03,  4.79it/s, loss=0.0401, v_num=0, reduced_train_loss=0.000621, global_step=2174.0, val_loss=0.0724]\n",
      "Epoch 28:  95%|█████████▌| 99/104 [00:19<00:00,  5.08it/s, loss=0.018, v_num=0, reduced_train_loss=0.000129, global_step=2174.0, val_loss=0.107]\n",
      "Epoch 28:  86%|████████▌ | 89/104 [00:18<00:03,  4.81it/s, loss=0.0401, v_num=0, reduced_train_loss=0.000621, global_step=2174.0, val_loss=0.0724]\n",
      "Epoch 28:  37%|███▋      | 38/104 [00:08<00:15,  4.32it/s, loss=0.0556, v_num=0, reduced_train_loss=0.125, global_step=2137.0, val_loss=0.102]  ]\n",
      "Epoch 28:  97%|█████████▋| 101/104 [00:19<00:00,  5.11it/s, loss=0.018, v_num=0, reduced_train_loss=0.000129, global_step=2174.0, val_loss=0.107]\n",
      "Epoch 28:  87%|████████▋ | 90/104 [00:18<00:02,  4.83it/s, loss=0.0401, v_num=0, reduced_train_loss=0.000621, global_step=2174.0, val_loss=0.0724]\n",
      "Epoch 28:  98%|█████████▊| 102/104 [00:19<00:00,  5.13it/s, loss=0.018, v_num=0, reduced_train_loss=0.000129, global_step=2174.0, val_loss=0.107]\n",
      "Epoch 28:  38%|███▊      | 39/104 [00:09<00:15,  4.32it/s, loss=0.058, v_num=0, reduced_train_loss=0.0638, global_step=2138.0, val_loss=0.102]724]\n",
      "Epoch 28:  99%|█████████▉| 103/104 [00:19<00:00,  5.15it/s, loss=0.018, v_num=0, reduced_train_loss=0.000129, global_step=2174.0, val_loss=0.107]\n",
      "Epoch 28:  88%|████████▊ | 92/104 [00:18<00:02,  4.87it/s, loss=0.0401, v_num=0, reduced_train_loss=0.000621, global_step=2174.0, val_loss=0.0724]\n",
      "Epoch 28: 100%|██████████| 104/104 [00:20<00:00,  5.19it/s, loss=0.018, v_num=0, reduced_train_loss=0.000129, global_step=2174.0, val_loss=0.107]2023-05-03 22:06:00,268 - root - INFO - val_loss: 0.12369856238365173\n",
      "Epoch 28: 100%|██████████| 104/104 [00:20<00:00,  5.18it/s, loss=0.018, v_num=0, reduced_train_loss=0.000129, global_step=2174.0, val_loss=0.124]\n",
      "Epoch 29:   0%|          | 0/104 [00:00<?, ?it/s, loss=0.018, v_num=0, reduced_train_loss=0.000129, global_step=2174.0, val_loss=0.124]          "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 22:06:00.268648 140522389899072 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.12369856238365173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28:  38%|███▊      | 40/104 [00:09<00:14,  4.31it/s, loss=0.059, v_num=0, reduced_train_loss=0.021, global_step=2139.0, val_loss=0.102] \n",
      "Epoch 28:  89%|████████▉ | 93/104 [00:19<00:02,  4.89it/s, loss=0.0401, v_num=0, reduced_train_loss=0.000621, global_step=2174.0, val_loss=0.0724]\n",
      "Epoch 28:  39%|███▉      | 41/104 [00:09<00:14,  4.31it/s, loss=0.0586, v_num=0, reduced_train_loss=0.000635, global_step=2140.0, val_loss=0.102]]\n",
      "Epoch 28:  91%|█████████▏| 95/104 [00:19<00:01,  4.93it/s, loss=0.0401, v_num=0, reduced_train_loss=0.000621, global_step=2174.0, val_loss=0.0724]\n",
      "Epoch 28:  40%|████      | 42/104 [00:09<00:14,  4.31it/s, loss=0.0706, v_num=0, reduced_train_loss=0.242, global_step=2141.0, val_loss=0.102]   ]\n",
      "Epoch 28:  93%|█████████▎| 97/104 [00:19<00:01,  4.97it/s, loss=0.0401, v_num=0, reduced_train_loss=0.000621, global_step=2174.0, val_loss=0.0724]\n",
      "Epoch 28:  41%|████▏     | 43/104 [00:09<00:14,  4.32it/s, loss=0.0694, v_num=0, reduced_train_loss=0.00593, global_step=2142.0, val_loss=0.102]4]\n",
      "Epoch 28:  95%|█████████▌| 99/104 [00:19<00:00,  5.01it/s, loss=0.0401, v_num=0, reduced_train_loss=0.000621, global_step=2174.0, val_loss=0.0724]\n",
      "Epoch 28:  42%|████▏     | 44/104 [00:10<00:13,  4.32it/s, loss=0.0463, v_num=0, reduced_train_loss=0.000562, global_step=2143.0, val_loss=0.102]4]\n",
      "Epoch 29:   5%|▍         | 5/104 [00:01<00:23,  4.29it/s, loss=0.0137, v_num=0, reduced_train_loss=0.000365, global_step=2179.0, val_loss=0.124]24]\n",
      "Epoch 28:  43%|████▎     | 45/104 [00:10<00:13,  4.32it/s, loss=0.0456, v_num=0, reduced_train_loss=0.000956, global_step=2144.0, val_loss=0.102]4]\n",
      "Epoch 28:  99%|█████████▉| 103/104 [00:20<00:00,  5.09it/s, loss=0.0401, v_num=0, reduced_train_loss=0.000621, global_step=2174.0, val_loss=0.0724]\n",
      "Epoch 28: 100%|██████████| 104/104 [00:20<00:00,  5.12it/s, loss=0.0401, v_num=0, reduced_train_loss=0.000621, global_step=2174.0, val_loss=0.0724]2023-05-03 22:06:01,631 - root - INFO - val_loss: 0.058132462203502655\n",
      "Epoch 28: 100%|██████████| 104/104 [00:20<00:00,  5.12it/s, loss=0.0401, v_num=0, reduced_train_loss=0.000621, global_step=2174.0, val_loss=0.0581]\n",
      "Epoch 29:   0%|          | 0/104 [00:00<?, ?it/s, loss=0.0401, v_num=0, reduced_train_loss=0.000621, global_step=2174.0, val_loss=0.0581]          "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 22:06:01.631435 140649257482048 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.058132462203502655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28:  72%|███████▏  | 75/104 [00:17<00:06,  4.35it/s, loss=0.0319, v_num=0, reduced_train_loss=0.00532, global_step=2174.0, val_loss=0.102]  \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 29:  34%|███▎      | 35/104 [00:08<00:16,  4.24it/s, loss=0.0242, v_num=0, reduced_train_loss=0.0258, global_step=2209.0, val_loss=0.124] 1]\n",
      "Epoch 29:  30%|██▉       | 31/104 [00:07<00:16,  4.38it/s, loss=0.0139, v_num=0, reduced_train_loss=0.000729, global_step=2205.0, val_loss=0.0581]\n",
      "Epoch 29:  35%|███▍      | 36/104 [00:08<00:16,  4.23it/s, loss=0.0332, v_num=0, reduced_train_loss=0.231, global_step=2210.0, val_loss=0.124] ]\n",
      "Epoch 29:  31%|███       | 32/104 [00:07<00:16,  4.38it/s, loss=0.014, v_num=0, reduced_train_loss=0.00111, global_step=2206.0, val_loss=0.0581]  \n",
      "Epoch 29:  36%|███▌      | 37/104 [00:08<00:15,  4.22it/s, loss=0.0332, v_num=0, reduced_train_loss=0.000456, global_step=2211.0, val_loss=0.124]\n",
      "Epoch 29:  32%|███▏      | 33/104 [00:07<00:16,  4.38it/s, loss=0.0141, v_num=0, reduced_train_loss=0.00252, global_step=2207.0, val_loss=0.0581]\n",
      "Epoch 29:  37%|███▋      | 38/104 [00:09<00:15,  4.22it/s, loss=0.0332, v_num=0, reduced_train_loss=0.000526, global_step=2212.0, val_loss=0.124]\n",
      "Epoch 29:  33%|███▎      | 34/104 [00:07<00:15,  4.38it/s, loss=0.0119, v_num=0, reduced_train_loss=0.00228, global_step=2208.0, val_loss=0.0581]\n",
      "Epoch 29:  38%|███▊      | 39/104 [00:09<00:15,  4.21it/s, loss=0.035, v_num=0, reduced_train_loss=0.0376, global_step=2213.0, val_loss=0.124]   \n",
      "Epoch 29:  34%|███▎      | 35/104 [00:07<00:15,  4.38it/s, loss=0.0121, v_num=0, reduced_train_loss=0.00416, global_step=2209.0, val_loss=0.0581]\n",
      "Epoch 29:  38%|███▊      | 40/104 [00:09<00:15,  4.21it/s, loss=0.0348, v_num=0, reduced_train_loss=0.00113, global_step=2214.0, val_loss=0.124]\n",
      "Epoch 29:  35%|███▍      | 36/104 [00:08<00:15,  4.38it/s, loss=0.0121, v_num=0, reduced_train_loss=0.00256, global_step=2210.0, val_loss=0.0581]\n",
      "Epoch 29:  36%|███▌      | 37/104 [00:08<00:15,  4.39it/s, loss=0.0121, v_num=0, reduced_train_loss=0.000291, global_step=2211.0, val_loss=0.0581]\n",
      "Epoch 28:  86%|████████▌ | 89/104 [00:19<00:03,  4.68it/s, loss=0.0319, v_num=0, reduced_train_loss=0.00532, global_step=2174.0, val_loss=0.102]\n",
      "Epoch 29:  37%|███▋      | 38/104 [00:08<00:15,  4.39it/s, loss=0.0121, v_num=0, reduced_train_loss=0.000103, global_step=2212.0, val_loss=0.0581]\n",
      "Epoch 28:  88%|████████▊ | 91/104 [00:19<00:02,  4.72it/s, loss=0.0319, v_num=0, reduced_train_loss=0.00532, global_step=2174.0, val_loss=0.102]\n",
      "Epoch 29:  41%|████▏     | 43/104 [00:10<00:14,  4.19it/s, loss=0.0196, v_num=0, reduced_train_loss=0.00015, global_step=2217.0, val_loss=0.124]  \n",
      "Epoch 28:  89%|████████▉ | 93/104 [00:19<00:02,  4.77it/s, loss=0.0319, v_num=0, reduced_train_loss=0.00532, global_step=2174.0, val_loss=0.102]\n",
      "Epoch 29:  42%|████▏     | 44/104 [00:10<00:14,  4.19it/s, loss=0.0175, v_num=0, reduced_train_loss=0.000864, global_step=2218.0, val_loss=0.124]\n",
      "Epoch 28:  91%|█████████▏| 95/104 [00:19<00:01,  4.81it/s, loss=0.0319, v_num=0, reduced_train_loss=0.00532, global_step=2174.0, val_loss=0.102]\n",
      "Epoch 29:  43%|████▎     | 45/104 [00:10<00:14,  4.19it/s, loss=0.0174, v_num=0, reduced_train_loss=0.000913, global_step=2219.0, val_loss=0.124]]\n",
      "Epoch 28:  93%|█████████▎| 97/104 [00:19<00:01,  4.85it/s, loss=0.0319, v_num=0, reduced_train_loss=0.00532, global_step=2174.0, val_loss=0.102]\n",
      "Epoch 29:  44%|████▍     | 46/104 [00:10<00:13,  4.19it/s, loss=0.0177, v_num=0, reduced_train_loss=0.0111, global_step=2220.0, val_loss=0.124]  ]\n",
      "Epoch 28:  95%|█████████▌| 99/104 [00:20<00:01,  4.89it/s, loss=0.0319, v_num=0, reduced_train_loss=0.00532, global_step=2174.0, val_loss=0.102]\n",
      "Epoch 29:  45%|████▌     | 47/104 [00:11<00:13,  4.18it/s, loss=0.0177, v_num=0, reduced_train_loss=0.00112, global_step=2221.0, val_loss=0.124]] \n",
      "Epoch 29:  42%|████▏     | 44/104 [00:09<00:13,  4.40it/s, loss=0.00112, v_num=0, reduced_train_loss=0.000981, global_step=2218.0, val_loss=0.0581]\n",
      "Epoch 29:  46%|████▌     | 48/104 [00:11<00:13,  4.18it/s, loss=0.0178, v_num=0, reduced_train_loss=0.00378, global_step=2222.0, val_loss=0.124]]\n",
      "Epoch 28:  99%|█████████▉| 103/104 [00:20<00:00,  4.97it/s, loss=0.0319, v_num=0, reduced_train_loss=0.00532, global_step=2174.0, val_loss=0.102]\n",
      "Epoch 28: 100%|██████████| 104/104 [00:20<00:00,  5.00it/s, loss=0.0319, v_num=0, reduced_train_loss=0.00532, global_step=2174.0, val_loss=0.102]2023-05-03 22:06:11,835 - root - INFO - val_loss: 0.12704403698444366\n",
      "Epoch 28: 100%|██████████| 104/104 [00:20<00:00,  5.00it/s, loss=0.0319, v_num=0, reduced_train_loss=0.00532, global_step=2174.0, val_loss=0.127]\n",
      "Epoch 29:  47%|████▋     | 49/104 [00:11<00:13,  4.19it/s, loss=0.0178, v_num=0, reduced_train_loss=0.00114, global_step=2223.0, val_loss=0.124]1] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 22:06:11.835184 139755592410944 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.12704403698444366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29:  72%|███████▏  | 75/104 [00:17<00:06,  4.29it/s, loss=0.0644, v_num=0, reduced_train_loss=0.00022, global_step=2249.0, val_loss=0.124] 1]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 29:  26%|██▌       | 27/104 [00:06<00:17,  4.46it/s, loss=0.0606, v_num=0, reduced_train_loss=0.00242, global_step=2201.0, val_loss=0.127]81]\n",
      "Epoch 29:  73%|███████▎  | 76/104 [00:17<00:06,  4.30it/s, loss=0.0644, v_num=0, reduced_train_loss=0.00022, global_step=2249.0, val_loss=0.124]\n",
      "Epoch 29:  27%|██▋       | 28/104 [00:06<00:17,  4.46it/s, loss=0.0605, v_num=0, reduced_train_loss=0.00174, global_step=2202.0, val_loss=0.127]1] \n",
      "Epoch 29:  75%|███████▌  | 78/104 [00:17<00:05,  4.35it/s, loss=0.0644, v_num=0, reduced_train_loss=0.00022, global_step=2249.0, val_loss=0.124]\n",
      "Epoch 29:  28%|██▊       | 29/104 [00:06<00:16,  4.46it/s, loss=0.0607, v_num=0, reduced_train_loss=0.0384, global_step=2203.0, val_loss=0.127] 81]\n",
      "Epoch 29:  72%|███████▏  | 75/104 [00:16<00:06,  4.43it/s, loss=0.0226, v_num=0, reduced_train_loss=0.320, global_step=2249.0, val_loss=0.0581]    \n",
      "Epoch 29:  29%|██▉       | 30/104 [00:06<00:16,  4.46it/s, loss=0.0592, v_num=0, reduced_train_loss=0.133, global_step=2204.0, val_loss=0.127] \n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 29:  78%|███████▊  | 81/104 [00:18<00:05,  4.43it/s, loss=0.0644, v_num=0, reduced_train_loss=0.00022, global_step=2249.0, val_loss=0.124]\n",
      "Epoch 29:  79%|███████▉  | 82/104 [00:18<00:04,  4.45it/s, loss=0.0644, v_num=0, reduced_train_loss=0.00022, global_step=2249.0, val_loss=0.124]\n",
      "Epoch 29:  30%|██▉       | 31/104 [00:06<00:16,  4.46it/s, loss=0.0602, v_num=0, reduced_train_loss=0.0236, global_step=2205.0, val_loss=0.127]\n",
      "Epoch 29:  80%|███████▉  | 83/104 [00:18<00:04,  4.47it/s, loss=0.0644, v_num=0, reduced_train_loss=0.00022, global_step=2249.0, val_loss=0.124]\n",
      "Epoch 29:  74%|███████▍  | 77/104 [00:17<00:06,  4.47it/s, loss=0.0226, v_num=0, reduced_train_loss=0.320, global_step=2249.0, val_loss=0.0581]\n",
      "Epoch 29:  81%|████████  | 84/104 [00:18<00:04,  4.50it/s, loss=0.0644, v_num=0, reduced_train_loss=0.00022, global_step=2249.0, val_loss=0.124]\n",
      "Epoch 29:  31%|███       | 32/104 [00:07<00:16,  4.46it/s, loss=0.0473, v_num=0, reduced_train_loss=0.00198, global_step=2206.0, val_loss=0.127]\n",
      "Epoch 29:  82%|████████▏ | 85/104 [00:18<00:04,  4.52it/s, loss=0.0644, v_num=0, reduced_train_loss=0.00022, global_step=2249.0, val_loss=0.124]\n",
      "Epoch 29:  76%|███████▌  | 79/104 [00:17<00:05,  4.52it/s, loss=0.0226, v_num=0, reduced_train_loss=0.320, global_step=2249.0, val_loss=0.0581]\n",
      "Epoch 29:  83%|████████▎ | 86/104 [00:18<00:03,  4.54it/s, loss=0.0644, v_num=0, reduced_train_loss=0.00022, global_step=2249.0, val_loss=0.124]\n",
      "Epoch 29:  32%|███▏      | 33/104 [00:07<00:15,  4.47it/s, loss=0.0477, v_num=0, reduced_train_loss=0.0102, global_step=2207.0, val_loss=0.127] \n",
      "Epoch 29:  84%|████████▎ | 87/104 [00:19<00:03,  4.57it/s, loss=0.0644, v_num=0, reduced_train_loss=0.00022, global_step=2249.0, val_loss=0.124]\n",
      "Epoch 29:  78%|███████▊  | 81/104 [00:17<00:05,  4.57it/s, loss=0.0226, v_num=0, reduced_train_loss=0.320, global_step=2249.0, val_loss=0.0581]\n",
      "Epoch 29:  33%|███▎      | 34/104 [00:07<00:15,  4.47it/s, loss=0.0482, v_num=0, reduced_train_loss=0.0117, global_step=2208.0, val_loss=0.127]]\n",
      "Epoch 29:  79%|███████▉  | 82/104 [00:17<00:04,  4.60it/s, loss=0.0226, v_num=0, reduced_train_loss=0.320, global_step=2249.0, val_loss=0.0581]\n",
      "Epoch 29:  86%|████████▌ | 89/104 [00:19<00:03,  4.61it/s, loss=0.0644, v_num=0, reduced_train_loss=0.00022, global_step=2249.0, val_loss=0.124]\n",
      "Epoch 29:  34%|███▎      | 35/104 [00:07<00:15,  4.47it/s, loss=0.0458, v_num=0, reduced_train_loss=0.0131, global_step=2209.0, val_loss=0.127]\n",
      "Epoch 29:  87%|████████▋ | 90/104 [00:19<00:03,  4.64it/s, loss=0.0644, v_num=0, reduced_train_loss=0.00022, global_step=2249.0, val_loss=0.124]\n",
      "Epoch 29:  81%|████████  | 84/104 [00:18<00:04,  4.64it/s, loss=0.0226, v_num=0, reduced_train_loss=0.320, global_step=2249.0, val_loss=0.0581]\n",
      "Epoch 29:  88%|████████▊ | 91/104 [00:19<00:02,  4.66it/s, loss=0.0644, v_num=0, reduced_train_loss=0.00022, global_step=2249.0, val_loss=0.124]\n",
      "Epoch 29:  35%|███▍      | 36/104 [00:08<00:15,  4.47it/s, loss=0.0461, v_num=0, reduced_train_loss=0.00872, global_step=2210.0, val_loss=0.127]\n",
      "Epoch 29:  88%|████████▊ | 92/104 [00:19<00:02,  4.68it/s, loss=0.0644, v_num=0, reduced_train_loss=0.00022, global_step=2249.0, val_loss=0.124]\n",
      "Epoch 29:  83%|████████▎ | 86/104 [00:18<00:03,  4.69it/s, loss=0.0226, v_num=0, reduced_train_loss=0.320, global_step=2249.0, val_loss=0.0581]\n",
      "Epoch 29:  89%|████████▉ | 93/104 [00:19<00:02,  4.70it/s, loss=0.0644, v_num=0, reduced_train_loss=0.00022, global_step=2249.0, val_loss=0.124]\n",
      "Epoch 29:  36%|███▌      | 37/104 [00:08<00:14,  4.47it/s, loss=0.0451, v_num=0, reduced_train_loss=0.000759, global_step=2211.0, val_loss=0.127]\n",
      "Epoch 29:  90%|█████████ | 94/104 [00:19<00:02,  4.72it/s, loss=0.0644, v_num=0, reduced_train_loss=0.00022, global_step=2249.0, val_loss=0.124]\n",
      "Epoch 29:  85%|████████▍ | 88/104 [00:18<00:03,  4.74it/s, loss=0.0226, v_num=0, reduced_train_loss=0.320, global_step=2249.0, val_loss=0.0581]\n",
      "Epoch 29:  91%|█████████▏| 95/104 [00:20<00:01,  4.74it/s, loss=0.0644, v_num=0, reduced_train_loss=0.00022, global_step=2249.0, val_loss=0.124]\n",
      "Epoch 29:  37%|███▋      | 38/104 [00:08<00:14,  4.47it/s, loss=0.0448, v_num=0, reduced_train_loss=0.00544, global_step=2212.0, val_loss=0.127] \n",
      "Epoch 29:  92%|█████████▏| 96/104 [00:20<00:01,  4.76it/s, loss=0.0644, v_num=0, reduced_train_loss=0.00022, global_step=2249.0, val_loss=0.124]\n",
      "Epoch 29:  87%|████████▋ | 90/104 [00:18<00:02,  4.78it/s, loss=0.0226, v_num=0, reduced_train_loss=0.320, global_step=2249.0, val_loss=0.0581]\n",
      "Epoch 29:  38%|███▊      | 39/104 [00:08<00:14,  4.47it/s, loss=0.0446, v_num=0, reduced_train_loss=0.00155, global_step=2213.0, val_loss=0.127]\n",
      "Epoch 29:  88%|████████▊ | 91/104 [00:18<00:02,  4.80it/s, loss=0.0226, v_num=0, reduced_train_loss=0.320, global_step=2249.0, val_loss=0.0581]\n",
      "Epoch 29:  94%|█████████▍| 98/104 [00:20<00:01,  4.81it/s, loss=0.0644, v_num=0, reduced_train_loss=0.00022, global_step=2249.0, val_loss=0.124]\n",
      "Epoch 29:  38%|███▊      | 40/104 [00:08<00:14,  4.47it/s, loss=0.0446, v_num=0, reduced_train_loss=0.00155, global_step=2213.0, val_loss=0.127]\n",
      "Epoch 29:  38%|███▊      | 40/104 [00:08<00:14,  4.47it/s, loss=0.0404, v_num=0, reduced_train_loss=0.00112, global_step=2214.0, val_loss=0.127]\n",
      "Epoch 29:  89%|████████▉ | 93/104 [00:19<00:02,  4.84it/s, loss=0.0226, v_num=0, reduced_train_loss=0.320, global_step=2249.0, val_loss=0.0581]\n",
      "Epoch 29:  96%|█████████▌| 100/104 [00:20<00:00,  4.85it/s, loss=0.0644, v_num=0, reduced_train_loss=0.00022, global_step=2249.0, val_loss=0.124]\n",
      "Epoch 29:  39%|███▉      | 41/104 [00:09<00:14,  4.47it/s, loss=0.0404, v_num=0, reduced_train_loss=0.00272, global_step=2215.0, val_loss=0.127]\n",
      "Epoch 29:  97%|█████████▋| 101/104 [00:20<00:00,  4.87it/s, loss=0.0644, v_num=0, reduced_train_loss=0.00022, global_step=2249.0, val_loss=0.124]\n",
      "Epoch 29:  91%|█████████▏| 95/104 [00:19<00:01,  4.89it/s, loss=0.0226, v_num=0, reduced_train_loss=0.320, global_step=2249.0, val_loss=0.0581]\n",
      "Epoch 29:  98%|█████████▊| 102/104 [00:20<00:00,  4.88it/s, loss=0.0644, v_num=0, reduced_train_loss=0.00022, global_step=2249.0, val_loss=0.124]\n",
      "Epoch 29:  40%|████      | 42/104 [00:09<00:13,  4.47it/s, loss=0.0475, v_num=0, reduced_train_loss=0.284, global_step=2216.0, val_loss=0.127]  \n",
      "Epoch 29:  99%|█████████▉| 103/104 [00:21<00:00,  4.90it/s, loss=0.0644, v_num=0, reduced_train_loss=0.00022, global_step=2249.0, val_loss=0.124]\n",
      "Epoch 29:  93%|█████████▎| 97/104 [00:19<00:01,  4.93it/s, loss=0.0226, v_num=0, reduced_train_loss=0.320, global_step=2249.0, val_loss=0.0581]\n",
      "Epoch 29: 100%|██████████| 104/104 [00:21<00:00,  4.94it/s, loss=0.0644, v_num=0, reduced_train_loss=0.00022, global_step=2249.0, val_loss=0.124]2023-05-03 22:06:21,345 - root - INFO - val_loss: 0.13158762454986572\n",
      "Epoch 29: 100%|██████████| 104/104 [00:21<00:00,  4.94it/s, loss=0.0644, v_num=0, reduced_train_loss=0.00022, global_step=2249.0, val_loss=0.132]\n",
      "Epoch 30:   0%|          | 0/104 [00:00<?, ?it/s, loss=0.0644, v_num=0, reduced_train_loss=0.00022, global_step=2249.0, val_loss=0.132]          "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 22:06:21.345245 140522389899072 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.13158762454986572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 29:  41%|████▏     | 43/104 [00:09<00:13,  4.47it/s, loss=0.0567, v_num=0, reduced_train_loss=0.326, global_step=2217.0, val_loss=0.127]]\n",
      "Epoch 29:  42%|████▏     | 44/104 [00:09<00:13,  4.47it/s, loss=0.0571, v_num=0, reduced_train_loss=0.00784, global_step=2218.0, val_loss=0.127]\n",
      "Epoch 29:  96%|█████████▌| 100/104 [00:20<00:00,  4.99it/s, loss=0.0226, v_num=0, reduced_train_loss=0.320, global_step=2249.0, val_loss=0.0581]\n",
      "Epoch 29:  43%|████▎     | 45/104 [00:10<00:13,  4.47it/s, loss=0.0534, v_num=0, reduced_train_loss=0.178, global_step=2219.0, val_loss=0.127]  \n",
      "Epoch 29:  98%|█████████▊| 102/104 [00:20<00:00,  5.03it/s, loss=0.0226, v_num=0, reduced_train_loss=0.320, global_step=2249.0, val_loss=0.0581]\n",
      "Epoch 30:   3%|▎         | 3/104 [00:00<00:24,  4.19it/s, loss=0.0452, v_num=0, reduced_train_loss=0.00283, global_step=2252.0, val_loss=0.132]]\n",
      "Epoch 29: 100%|██████████| 104/104 [00:20<00:00,  5.08it/s, loss=0.0226, v_num=0, reduced_train_loss=0.320, global_step=2249.0, val_loss=0.0581]2023-05-03 22:06:22,121 - root - INFO - val_loss: 0.06653852760791779\n",
      "Epoch 29: 100%|██████████| 104/104 [00:20<00:00,  5.08it/s, loss=0.0226, v_num=0, reduced_train_loss=0.320, global_step=2249.0, val_loss=0.0665]\n",
      "Epoch 30:   4%|▍         | 4/104 [00:00<00:23,  4.25it/s, loss=0.0452, v_num=0, reduced_train_loss=0.000736, global_step=2253.0, val_loss=0.132]]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 22:06:22.121835 140649257482048 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.06653852760791779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29:  72%|███████▏  | 75/104 [00:16<00:06,  4.49it/s, loss=0.0912, v_num=0, reduced_train_loss=0.180, global_step=2249.0, val_loss=0.127]65]  \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 30:  28%|██▊       | 29/104 [00:06<00:16,  4.44it/s, loss=0.0186, v_num=0, reduced_train_loss=0.00183, global_step=2278.0, val_loss=0.0665]\n",
      "Epoch 30:  32%|███▏      | 33/104 [00:07<00:15,  4.44it/s, loss=0.052, v_num=0, reduced_train_loss=0.00214, global_step=2282.0, val_loss=0.132]\n",
      "Epoch 30:  29%|██▉       | 30/104 [00:06<00:16,  4.44it/s, loss=0.0186, v_num=0, reduced_train_loss=6.37e-5, global_step=2279.0, val_loss=0.0665]\n",
      "Epoch 30:  33%|███▎      | 34/104 [00:07<00:15,  4.44it/s, loss=0.0522, v_num=0, reduced_train_loss=0.00413, global_step=2283.0, val_loss=0.132]\n",
      "Epoch 30:  30%|██▉       | 31/104 [00:06<00:16,  4.44it/s, loss=0.0186, v_num=0, reduced_train_loss=0.00155, global_step=2280.0, val_loss=0.0665]\n",
      "Epoch 30:  34%|███▎      | 35/104 [00:07<00:15,  4.44it/s, loss=0.0491, v_num=0, reduced_train_loss=0.000389, global_step=2284.0, val_loss=0.132]\n",
      "Epoch 30:  31%|███       | 32/104 [00:07<00:16,  4.45it/s, loss=0.0186, v_num=0, reduced_train_loss=0.00112, global_step=2281.0, val_loss=0.0665]\n",
      "Epoch 30:  32%|███▏      | 33/104 [00:07<00:15,  4.45it/s, loss=0.0224, v_num=0, reduced_train_loss=0.0769, global_step=2282.0, val_loss=0.0665] \n",
      "Epoch 30:  36%|███▌      | 37/104 [00:08<00:15,  4.45it/s, loss=0.0245, v_num=0, reduced_train_loss=0.00642, global_step=2286.0, val_loss=0.132] \n",
      "Epoch 30:  33%|███▎      | 34/104 [00:07<00:15,  4.45it/s, loss=0.0223, v_num=0, reduced_train_loss=0.000303, global_step=2283.0, val_loss=0.0665]\n",
      "Epoch 30:  37%|███▋      | 38/104 [00:08<00:14,  4.45it/s, loss=0.0245, v_num=0, reduced_train_loss=0.00133, global_step=2287.0, val_loss=0.132]\n",
      "Epoch 30:  34%|███▎      | 35/104 [00:07<00:15,  4.45it/s, loss=0.0228, v_num=0, reduced_train_loss=0.00959, global_step=2284.0, val_loss=0.0665] \n",
      "Epoch 30:  38%|███▊      | 39/104 [00:08<00:14,  4.45it/s, loss=0.0232, v_num=0, reduced_train_loss=0.00436, global_step=2288.0, val_loss=0.132]\n",
      "Epoch 30:  35%|███▍      | 36/104 [00:08<00:15,  4.45it/s, loss=0.0229, v_num=0, reduced_train_loss=0.00118, global_step=2285.0, val_loss=0.0665]\n",
      "Epoch 30:  38%|███▊      | 40/104 [00:08<00:14,  4.45it/s, loss=0.0232, v_num=0, reduced_train_loss=0.000599, global_step=2289.0, val_loss=0.132]\n",
      "Epoch 30:  36%|███▌      | 37/104 [00:08<00:15,  4.45it/s, loss=0.0227, v_num=0, reduced_train_loss=4.79e-5, global_step=2286.0, val_loss=0.0665]\n",
      "Epoch 30:  37%|███▋      | 38/104 [00:08<00:14,  4.45it/s, loss=0.0227, v_num=0, reduced_train_loss=0.000522, global_step=2287.0, val_loss=0.0665]\n",
      "Epoch 29:  88%|████████▊ | 92/104 [00:18<00:02,  4.88it/s, loss=0.0912, v_num=0, reduced_train_loss=0.180, global_step=2249.0, val_loss=0.127]\n",
      "Epoch 30:  38%|███▊      | 39/104 [00:08<00:14,  4.45it/s, loss=0.0227, v_num=0, reduced_train_loss=0.000175, global_step=2288.0, val_loss=0.0665]\n",
      "Epoch 30:  41%|████▏     | 43/104 [00:09<00:13,  4.44it/s, loss=0.0195, v_num=0, reduced_train_loss=0.000289, global_step=2292.0, val_loss=0.132]\n",
      "Epoch 30:  38%|███▊      | 40/104 [00:08<00:14,  4.45it/s, loss=0.0315, v_num=0, reduced_train_loss=0.177, global_step=2289.0, val_loss=0.0665]   \n",
      "Epoch 30:  42%|████▏     | 44/104 [00:09<00:13,  4.45it/s, loss=0.0143, v_num=0, reduced_train_loss=0.00129, global_step=2293.0, val_loss=0.132] \n",
      "Epoch 30:  39%|███▉      | 41/104 [00:09<00:14,  4.45it/s, loss=0.0145, v_num=0, reduced_train_loss=0.000254, global_step=2290.0, val_loss=0.0665]\n",
      "Epoch 30:  43%|████▎     | 45/104 [00:10<00:13,  4.45it/s, loss=0.0129, v_num=0, reduced_train_loss=0.000979, global_step=2294.0, val_loss=0.132]\n",
      "Epoch 30:  40%|████      | 42/104 [00:09<00:13,  4.46it/s, loss=0.0145, v_num=0, reduced_train_loss=0.000368, global_step=2291.0, val_loss=0.0665]\n",
      "Epoch 30:  44%|████▍     | 46/104 [00:10<00:13,  4.45it/s, loss=0.0123, v_num=0, reduced_train_loss=0.0123, global_step=2295.0, val_loss=0.132]  \n",
      "Epoch 30:  41%|████▏     | 43/104 [00:09<00:13,  4.46it/s, loss=0.0138, v_num=0, reduced_train_loss=0.000104, global_step=2292.0, val_loss=0.0665]\n",
      "Epoch 30:  45%|████▌     | 47/104 [00:10<00:12,  4.45it/s, loss=0.0149, v_num=0, reduced_train_loss=0.0564, global_step=2296.0, val_loss=0.132]\n",
      "Epoch 30:  42%|████▏     | 44/104 [00:09<00:13,  4.46it/s, loss=0.0139, v_num=0, reduced_train_loss=0.00221, global_step=2293.0, val_loss=0.0665] \n",
      "Epoch 29: 100%|██████████| 104/104 [00:20<00:00,  5.15it/s, loss=0.0912, v_num=0, reduced_train_loss=0.180, global_step=2249.0, val_loss=0.127]2023-05-03 22:06:32,054 - root - INFO - val_loss: 0.12139938771724701\n",
      "Epoch 29: 100%|██████████| 104/104 [00:20<00:00,  5.14it/s, loss=0.0912, v_num=0, reduced_train_loss=0.180, global_step=2249.0, val_loss=0.121]\n",
      "Epoch 30:   0%|          | 0/104 [00:00<?, ?it/s, loss=0.0912, v_num=0, reduced_train_loss=0.180, global_step=2249.0, val_loss=0.121]          "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 22:06:32.054435 139755592410944 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.12139938771724701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30:  72%|███████▏  | 75/104 [00:16<00:06,  4.47it/s, loss=0.0294, v_num=0, reduced_train_loss=0.00433, global_step=2324.0, val_loss=0.132]  \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 30:  26%|██▌       | 27/104 [00:06<00:17,  4.34it/s, loss=0.0419, v_num=0, reduced_train_loss=0.00396, global_step=2276.0, val_loss=0.121]5]\n",
      "Epoch 30:  70%|███████   | 73/104 [00:16<00:06,  4.48it/s, loss=0.0128, v_num=0, reduced_train_loss=0.000194, global_step=2322.0, val_loss=0.0665]\n",
      "Epoch 30:  27%|██▋       | 28/104 [00:06<00:17,  4.34it/s, loss=0.0485, v_num=0, reduced_train_loss=0.144, global_step=2277.0, val_loss=0.121]  \n",
      "Epoch 30:  71%|███████   | 74/104 [00:16<00:06,  4.48it/s, loss=0.013, v_num=0, reduced_train_loss=0.00407, global_step=2323.0, val_loss=0.0665]  \n",
      "Epoch 30:  28%|██▊       | 29/104 [00:06<00:17,  4.34it/s, loss=0.0488, v_num=0, reduced_train_loss=0.00594, global_step=2278.0, val_loss=0.121]\n",
      "Epoch 30:  72%|███████▏  | 75/104 [00:16<00:06,  4.48it/s, loss=0.013, v_num=0, reduced_train_loss=0.000626, global_step=2324.0, val_loss=0.0665]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 30:  29%|██▉       | 30/104 [00:06<00:17,  4.34it/s, loss=0.0488, v_num=0, reduced_train_loss=0.00189, global_step=2279.0, val_loss=0.121]\n",
      "Epoch 30:  73%|███████▎  | 76/104 [00:16<00:06,  4.50it/s, loss=0.013, v_num=0, reduced_train_loss=0.000626, global_step=2324.0, val_loss=0.0665]\n",
      "Epoch 30:  79%|███████▉  | 82/104 [00:17<00:04,  4.63it/s, loss=0.0294, v_num=0, reduced_train_loss=0.00433, global_step=2324.0, val_loss=0.132]\n",
      "Epoch 30:  74%|███████▍  | 77/104 [00:17<00:05,  4.52it/s, loss=0.013, v_num=0, reduced_train_loss=0.000626, global_step=2324.0, val_loss=0.0665]\n",
      "Epoch 30:  30%|██▉       | 31/104 [00:07<00:16,  4.34it/s, loss=0.0491, v_num=0, reduced_train_loss=0.00681, global_step=2280.0, val_loss=0.121]\n",
      "Epoch 30:  75%|███████▌  | 78/104 [00:17<00:05,  4.55it/s, loss=0.013, v_num=0, reduced_train_loss=0.000626, global_step=2324.0, val_loss=0.0665]\n",
      "Epoch 30:  81%|████████  | 84/104 [00:17<00:04,  4.68it/s, loss=0.0294, v_num=0, reduced_train_loss=0.00433, global_step=2324.0, val_loss=0.132]\n",
      "Epoch 30:  76%|███████▌  | 79/104 [00:17<00:05,  4.58it/s, loss=0.013, v_num=0, reduced_train_loss=0.000626, global_step=2324.0, val_loss=0.0665]\n",
      "Epoch 30:  31%|███       | 32/104 [00:07<00:16,  4.34it/s, loss=0.0499, v_num=0, reduced_train_loss=0.0181, global_step=2281.0, val_loss=0.121] \n",
      "Epoch 30:  77%|███████▋  | 80/104 [00:17<00:05,  4.60it/s, loss=0.013, v_num=0, reduced_train_loss=0.000626, global_step=2324.0, val_loss=0.0665]\n",
      "Epoch 30:  83%|████████▎ | 86/104 [00:18<00:03,  4.72it/s, loss=0.0294, v_num=0, reduced_train_loss=0.00433, global_step=2324.0, val_loss=0.132]\n",
      "Epoch 30:  32%|███▏      | 33/104 [00:07<00:16,  4.34it/s, loss=0.0368, v_num=0, reduced_train_loss=0.122, global_step=2282.0, val_loss=0.121] 5]\n",
      "Epoch 30:  84%|████████▎ | 87/104 [00:18<00:03,  4.75it/s, loss=0.0294, v_num=0, reduced_train_loss=0.00433, global_step=2324.0, val_loss=0.132]\n",
      "Epoch 30:  79%|███████▉  | 82/104 [00:17<00:04,  4.65it/s, loss=0.013, v_num=0, reduced_train_loss=0.000626, global_step=2324.0, val_loss=0.0665]\n",
      "Epoch 30:  85%|████████▍ | 88/104 [00:18<00:03,  4.77it/s, loss=0.0294, v_num=0, reduced_train_loss=0.00433, global_step=2324.0, val_loss=0.132]\n",
      "Epoch 30:  33%|███▎      | 34/104 [00:07<00:16,  4.34it/s, loss=0.0572, v_num=0, reduced_train_loss=0.483, global_step=2283.0, val_loss=0.121]65]\n",
      "Epoch 30:  86%|████████▌ | 89/104 [00:18<00:03,  4.79it/s, loss=0.0294, v_num=0, reduced_train_loss=0.00433, global_step=2324.0, val_loss=0.132]\n",
      "Epoch 30:  81%|████████  | 84/104 [00:17<00:04,  4.70it/s, loss=0.013, v_num=0, reduced_train_loss=0.000626, global_step=2324.0, val_loss=0.0665]\n",
      "Epoch 30:  34%|███▎      | 35/104 [00:08<00:15,  4.34it/s, loss=0.0575, v_num=0, reduced_train_loss=0.0202, global_step=2284.0, val_loss=0.121]]\n",
      "Epoch 30:  82%|████████▏ | 85/104 [00:18<00:04,  4.72it/s, loss=0.013, v_num=0, reduced_train_loss=0.000626, global_step=2324.0, val_loss=0.0665]\n",
      "Epoch 30:  88%|████████▊ | 91/104 [00:18<00:02,  4.83it/s, loss=0.0294, v_num=0, reduced_train_loss=0.00433, global_step=2324.0, val_loss=0.132]\n",
      "Epoch 30:  83%|████████▎ | 86/104 [00:18<00:03,  4.74it/s, loss=0.013, v_num=0, reduced_train_loss=0.000626, global_step=2324.0, val_loss=0.0665]\n",
      "Epoch 30:  35%|███▍      | 36/104 [00:08<00:15,  4.34it/s, loss=0.0567, v_num=0, reduced_train_loss=0.00537, global_step=2285.0, val_loss=0.121]\n",
      "Epoch 30:  84%|████████▎ | 87/104 [00:18<00:03,  4.77it/s, loss=0.013, v_num=0, reduced_train_loss=0.000626, global_step=2324.0, val_loss=0.0665]\n",
      "Epoch 30:  89%|████████▉ | 93/104 [00:19<00:02,  4.88it/s, loss=0.0294, v_num=0, reduced_train_loss=0.00433, global_step=2324.0, val_loss=0.132]\n",
      "Epoch 30:  85%|████████▍ | 88/104 [00:18<00:03,  4.79it/s, loss=0.013, v_num=0, reduced_train_loss=0.000626, global_step=2324.0, val_loss=0.0665]\n",
      "Epoch 30:  36%|███▌      | 37/104 [00:08<00:15,  4.34it/s, loss=0.0573, v_num=0, reduced_train_loss=0.0155, global_step=2286.0, val_loss=0.121] \n",
      "Epoch 30:  86%|████████▌ | 89/104 [00:18<00:03,  4.81it/s, loss=0.013, v_num=0, reduced_train_loss=0.000626, global_step=2324.0, val_loss=0.0665]\n",
      "Epoch 30:  91%|█████████▏| 95/104 [00:19<00:01,  4.92it/s, loss=0.0294, v_num=0, reduced_train_loss=0.00433, global_step=2324.0, val_loss=0.132]\n",
      "Epoch 30:  87%|████████▋ | 90/104 [00:18<00:02,  4.84it/s, loss=0.013, v_num=0, reduced_train_loss=0.000626, global_step=2324.0, val_loss=0.0665]\n",
      "Epoch 30:  37%|███▋      | 38/104 [00:08<00:15,  4.34it/s, loss=0.0566, v_num=0, reduced_train_loss=0.00358, global_step=2287.0, val_loss=0.121]\n",
      "Epoch 30:  88%|████████▊ | 91/104 [00:18<00:02,  4.86it/s, loss=0.013, v_num=0, reduced_train_loss=0.000626, global_step=2324.0, val_loss=0.0665]\n",
      "Epoch 30:  93%|█████████▎| 97/104 [00:19<00:01,  4.96it/s, loss=0.0294, v_num=0, reduced_train_loss=0.00433, global_step=2324.0, val_loss=0.132]\n",
      "Epoch 30:  88%|████████▊ | 92/104 [00:18<00:02,  4.88it/s, loss=0.013, v_num=0, reduced_train_loss=0.000626, global_step=2324.0, val_loss=0.0665]\n",
      "Epoch 30:  38%|███▊      | 39/104 [00:08<00:14,  4.35it/s, loss=0.0596, v_num=0, reduced_train_loss=0.0604, global_step=2288.0, val_loss=0.121] \n",
      "Epoch 30:  89%|████████▉ | 93/104 [00:18<00:02,  4.90it/s, loss=0.013, v_num=0, reduced_train_loss=0.000626, global_step=2324.0, val_loss=0.0665]\n",
      "Epoch 30:  95%|█████████▌| 99/104 [00:19<00:00,  5.00it/s, loss=0.0294, v_num=0, reduced_train_loss=0.00433, global_step=2324.0, val_loss=0.132]\n",
      "Epoch 30:  38%|███▊      | 40/104 [00:09<00:14,  4.35it/s, loss=0.0594, v_num=0, reduced_train_loss=0.000828, global_step=2289.0, val_loss=0.121]\n",
      "Epoch 30:  96%|█████████▌| 100/104 [00:19<00:00,  5.02it/s, loss=0.0294, v_num=0, reduced_train_loss=0.00433, global_step=2324.0, val_loss=0.132]\n",
      "Epoch 30:  91%|█████████▏| 95/104 [00:19<00:01,  4.95it/s, loss=0.013, v_num=0, reduced_train_loss=0.000626, global_step=2324.0, val_loss=0.0665]\n",
      "Epoch 30:  97%|█████████▋| 101/104 [00:20<00:00,  5.04it/s, loss=0.0294, v_num=0, reduced_train_loss=0.00433, global_step=2324.0, val_loss=0.132]\n",
      "Epoch 30:  39%|███▉      | 41/104 [00:09<00:14,  4.35it/s, loss=0.0595, v_num=0, reduced_train_loss=0.00249, global_step=2290.0, val_loss=0.121] \n",
      "Epoch 30:  98%|█████████▊| 102/104 [00:20<00:00,  5.06it/s, loss=0.0294, v_num=0, reduced_train_loss=0.00433, global_step=2324.0, val_loss=0.132]\n",
      "Epoch 30:  93%|█████████▎| 97/104 [00:19<00:01,  4.99it/s, loss=0.013, v_num=0, reduced_train_loss=0.000626, global_step=2324.0, val_loss=0.0665]\n",
      "Epoch 30:  99%|█████████▉| 103/104 [00:20<00:00,  5.08it/s, loss=0.0294, v_num=0, reduced_train_loss=0.00433, global_step=2324.0, val_loss=0.132]\n",
      "Epoch 30:  94%|█████████▍| 98/104 [00:19<00:01,  5.01it/s, loss=0.013, v_num=0, reduced_train_loss=0.000626, global_step=2324.0, val_loss=0.0665]\n",
      "Epoch 30: 100%|██████████| 104/104 [00:20<00:00,  5.11it/s, loss=0.0294, v_num=0, reduced_train_loss=0.00433, global_step=2324.0, val_loss=0.132]2023-05-03 22:06:41,707 - root - INFO - val_loss: 0.11343726515769958\n",
      "Epoch 30: 100%|██████████| 104/104 [00:20<00:00,  5.11it/s, loss=0.0294, v_num=0, reduced_train_loss=0.00433, global_step=2324.0, val_loss=0.113]\n",
      "Epoch 30:  40%|████      | 42/104 [00:09<00:14,  4.34it/s, loss=0.0589, v_num=0, reduced_train_loss=0.000692, global_step=2291.0, val_loss=0.121]\n",
      "Epoch 30:  95%|█████████▌| 99/104 [00:19<00:00,  5.03it/s, loss=0.013, v_num=0, reduced_train_loss=0.000626, global_step=2324.0, val_loss=0.0665]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 22:06:41.707571 140522389899072 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.11343726515769958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 31:   1%|          | 1/104 [00:00<00:27,  3.75it/s, loss=0.0294, v_num=0, reduced_train_loss=0.000232, global_step=2325.0, val_loss=0.113] ]\n",
      "Epoch 30:  97%|█████████▋| 101/104 [00:19<00:00,  5.07it/s, loss=0.013, v_num=0, reduced_train_loss=0.000626, global_step=2324.0, val_loss=0.0665]\n",
      "Epoch 31:   2%|▏         | 2/104 [00:00<00:24,  4.11it/s, loss=0.0295, v_num=0, reduced_train_loss=0.0018, global_step=2326.0, val_loss=0.113]  5]\n",
      "Epoch 30:  99%|█████████▉| 103/104 [00:20<00:00,  5.11it/s, loss=0.013, v_num=0, reduced_train_loss=0.000626, global_step=2324.0, val_loss=0.0665]\n",
      "Epoch 30: 100%|██████████| 104/104 [00:20<00:00,  5.14it/s, loss=0.013, v_num=0, reduced_train_loss=0.000626, global_step=2324.0, val_loss=0.0665]2023-05-03 22:06:42,364 - root - INFO - val_loss: 0.07450435310602188\n",
      "Epoch 30: 100%|██████████| 104/104 [00:20<00:00,  5.14it/s, loss=0.013, v_num=0, reduced_train_loss=0.000626, global_step=2324.0, val_loss=0.0745]\n",
      "Epoch 31:   0%|          | 0/104 [00:00<?, ?it/s, loss=0.013, v_num=0, reduced_train_loss=0.000626, global_step=2324.0, val_loss=0.0745]          "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 22:06:42.364561 140649257482048 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.07450435310602188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30:  72%|███████▏  | 75/104 [00:17<00:06,  4.37it/s, loss=0.0502, v_num=0, reduced_train_loss=0.0728, global_step=2324.0, val_loss=0.121] ]] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 31:  33%|███▎      | 34/104 [00:07<00:15,  4.43it/s, loss=0.00779, v_num=0, reduced_train_loss=0.000661, global_step=2358.0, val_loss=0.113]\n",
      "Epoch 31:  30%|██▉       | 31/104 [00:07<00:16,  4.36it/s, loss=0.0118, v_num=0, reduced_train_loss=0.00227, global_step=2355.0, val_loss=0.0745] \n",
      "Epoch 31:  34%|███▎      | 35/104 [00:07<00:15,  4.43it/s, loss=0.00777, v_num=0, reduced_train_loss=0.000491, global_step=2359.0, val_loss=0.113]\n",
      "Epoch 31:  31%|███       | 32/104 [00:07<00:16,  4.36it/s, loss=0.012, v_num=0, reduced_train_loss=0.00262, global_step=2356.0, val_loss=0.0745] \n",
      "Epoch 31:  35%|███▍      | 36/104 [00:08<00:15,  4.44it/s, loss=0.00861, v_num=0, reduced_train_loss=0.0169, global_step=2360.0, val_loss=0.113]  \n",
      "Epoch 31:  32%|███▏      | 33/104 [00:07<00:16,  4.36it/s, loss=0.0119, v_num=0, reduced_train_loss=6.16e-5, global_step=2357.0, val_loss=0.0745]\n",
      "Epoch 31:  36%|███▌      | 37/104 [00:08<00:15,  4.44it/s, loss=0.00864, v_num=0, reduced_train_loss=0.000739, global_step=2361.0, val_loss=0.113]\n",
      "Epoch 31:  33%|███▎      | 34/104 [00:07<00:16,  4.36it/s, loss=0.0126, v_num=0, reduced_train_loss=0.0142, global_step=2358.0, val_loss=0.0745] \n",
      "Epoch 31:  37%|███▋      | 38/104 [00:08<00:14,  4.44it/s, loss=0.00874, v_num=0, reduced_train_loss=0.00239, global_step=2362.0, val_loss=0.113] \n",
      "Epoch 31:  38%|███▊      | 39/104 [00:08<00:14,  4.44it/s, loss=0.00874, v_num=0, reduced_train_loss=0.00239, global_step=2362.0, val_loss=0.113]]\n",
      "Epoch 31:  38%|███▊      | 39/104 [00:08<00:14,  4.44it/s, loss=0.00876, v_num=0, reduced_train_loss=0.00044, global_step=2363.0, val_loss=0.113]\n",
      "Epoch 31:  38%|███▊      | 40/104 [00:09<00:14,  4.44it/s, loss=0.009, v_num=0, reduced_train_loss=0.00509, global_step=2364.0, val_loss=0.113]   \n",
      "Epoch 31:  36%|███▌      | 37/104 [00:08<00:15,  4.36it/s, loss=0.0128, v_num=0, reduced_train_loss=0.000406, global_step=2361.0, val_loss=0.0745]\n",
      "Epoch 31:  39%|███▉      | 41/104 [00:09<00:14,  4.44it/s, loss=0.00342, v_num=0, reduced_train_loss=0.00223, global_step=2365.0, val_loss=0.113]\n",
      "Epoch 31:  37%|███▋      | 38/104 [00:08<00:15,  4.36it/s, loss=0.0128, v_num=0, reduced_train_loss=0.000122, global_step=2362.0, val_loss=0.0745]\n",
      "Epoch 31:  40%|████      | 42/104 [00:09<00:13,  4.44it/s, loss=0.00977, v_num=0, reduced_train_loss=0.127, global_step=2366.0, val_loss=0.113]  \n",
      "Epoch 31:  38%|███▊      | 39/104 [00:08<00:14,  4.37it/s, loss=0.0128, v_num=0, reduced_train_loss=0.00092, global_step=2363.0, val_loss=0.0745] \n",
      "Epoch 31:  41%|████▏     | 43/104 [00:09<00:13,  4.44it/s, loss=0.00977, v_num=0, reduced_train_loss=0.000197, global_step=2367.0, val_loss=0.113]\n",
      "Epoch 31:  38%|███▊      | 40/104 [00:09<00:14,  4.36it/s, loss=0.0128, v_num=0, reduced_train_loss=0.000233, global_step=2364.0, val_loss=0.0745]\n",
      "Epoch 31:  42%|████▏     | 44/104 [00:09<00:13,  4.44it/s, loss=0.00988, v_num=0, reduced_train_loss=0.00241, global_step=2368.0, val_loss=0.113] \n",
      "Epoch 31:  39%|███▉      | 41/104 [00:09<00:14,  4.35it/s, loss=0.0128, v_num=0, reduced_train_loss=0.000296, global_step=2365.0, val_loss=0.0745]\n",
      "Epoch 31:  43%|████▎     | 45/104 [00:10<00:13,  4.44it/s, loss=0.00985, v_num=0, reduced_train_loss=0.000305, global_step=2369.0, val_loss=0.113]\n",
      "Epoch 31:  40%|████      | 42/104 [00:09<00:14,  4.35it/s, loss=0.0128, v_num=0, reduced_train_loss=0.000101, global_step=2366.0, val_loss=0.0745]\n",
      "Epoch 31:  44%|████▍     | 46/104 [00:10<00:13,  4.44it/s, loss=0.00984, v_num=0, reduced_train_loss=9.76e-5, global_step=2370.0, val_loss=0.113] \n",
      "Epoch 31:  45%|████▌     | 47/104 [00:10<00:12,  4.44it/s, loss=0.00979, v_num=0, reduced_train_loss=0.000481, global_step=2371.0, val_loss=0.113]\n",
      "Epoch 30:  96%|█████████▌| 100/104 [00:20<00:00,  4.94it/s, loss=0.0502, v_num=0, reduced_train_loss=0.0728, global_step=2324.0, val_loss=0.121]\n",
      "Epoch 31:  46%|████▌     | 48/104 [00:10<00:12,  4.44it/s, loss=0.00977, v_num=0, reduced_train_loss=0.000235, global_step=2372.0, val_loss=0.113]\n",
      "Epoch 30:  98%|█████████▊| 102/104 [00:20<00:00,  4.98it/s, loss=0.0502, v_num=0, reduced_train_loss=0.0728, global_step=2324.0, val_loss=0.121]\n",
      "Epoch 30:  99%|█████████▉| 103/104 [00:20<00:00,  4.99it/s, loss=0.0502, v_num=0, reduced_train_loss=0.0728, global_step=2324.0, val_loss=0.121]\n",
      "Epoch 30: 100%|██████████| 104/104 [00:20<00:00,  5.03it/s, loss=0.0502, v_num=0, reduced_train_loss=0.0728, global_step=2324.0, val_loss=0.121]2023-05-03 22:06:52,747 - root - INFO - val_loss: 0.09422073513269424\n",
      "Epoch 30: 100%|██████████| 104/104 [00:20<00:00,  5.03it/s, loss=0.0502, v_num=0, reduced_train_loss=0.0728, global_step=2324.0, val_loss=0.0942]\n",
      "Epoch 31:  43%|████▎     | 45/104 [00:10<00:13,  4.32it/s, loss=0.012, v_num=0, reduced_train_loss=0.0019, global_step=2369.0, val_loss=0.0745]   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 22:06:52.747304 139755592410944 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.09422073513269424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31:  25%|██▌       | 26/104 [00:05<00:17,  4.46it/s, loss=0.0327, v_num=0, reduced_train_loss=0.00106, global_step=2350.0, val_loss=0.0942]] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 31:  26%|██▌       | 27/104 [00:06<00:17,  4.46it/s, loss=0.0182, v_num=0, reduced_train_loss=0.000504, global_step=2351.0, val_loss=0.0942]\n",
      "Epoch 31:  74%|███████▍  | 77/104 [00:17<00:06,  4.49it/s, loss=0.0223, v_num=0, reduced_train_loss=0.00135, global_step=2399.0, val_loss=0.113]\n",
      "Epoch 31:  27%|██▋       | 28/104 [00:06<00:17,  4.46it/s, loss=0.0197, v_num=0, reduced_train_loss=0.0434, global_step=2352.0, val_loss=0.0942]  \n",
      "Epoch 31:  68%|██████▊   | 71/104 [00:16<00:07,  4.21it/s, loss=0.0418, v_num=0, reduced_train_loss=0.000209, global_step=2395.0, val_loss=0.0745]\n",
      "Epoch 31:  77%|███████▋  | 80/104 [00:17<00:05,  4.56it/s, loss=0.0223, v_num=0, reduced_train_loss=0.00135, global_step=2399.0, val_loss=0.113]\n",
      "Epoch 31:  69%|██████▉   | 72/104 [00:17<00:07,  4.20it/s, loss=0.0418, v_num=0, reduced_train_loss=0.000124, global_step=2396.0, val_loss=0.0745]\n",
      "Epoch 31:  79%|███████▉  | 82/104 [00:17<00:04,  4.61it/s, loss=0.0223, v_num=0, reduced_train_loss=0.00135, global_step=2399.0, val_loss=0.113]\n",
      "Epoch 31:  70%|███████   | 73/104 [00:17<00:07,  4.20it/s, loss=0.0417, v_num=0, reduced_train_loss=0.000233, global_step=2397.0, val_loss=0.0745]\n",
      "Epoch 31:  81%|████████  | 84/104 [00:18<00:04,  4.66it/s, loss=0.0223, v_num=0, reduced_train_loss=0.00135, global_step=2399.0, val_loss=0.113]\n",
      "Epoch 31:  71%|███████   | 74/104 [00:17<00:07,  4.20it/s, loss=0.0408, v_num=0, reduced_train_loss=5.92e-5, global_step=2398.0, val_loss=0.0745] \n",
      "Epoch 31:  83%|████████▎ | 86/104 [00:18<00:03,  4.70it/s, loss=0.0223, v_num=0, reduced_train_loss=0.00135, global_step=2399.0, val_loss=0.113]\n",
      "Epoch 31:  72%|███████▏  | 75/104 [00:17<00:06,  4.20it/s, loss=0.0408, v_num=0, reduced_train_loss=0.000167, global_step=2399.0, val_loss=0.0745]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31:  85%|████████▍ | 88/104 [00:18<00:03,  4.75it/s, loss=0.0223, v_num=0, reduced_train_loss=0.00135, global_step=2399.0, val_loss=0.113]\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 31:  33%|███▎      | 34/104 [00:07<00:15,  4.47it/s, loss=0.0253, v_num=0, reduced_train_loss=0.0747, global_step=2358.0, val_loss=0.0942]\n",
      "Epoch 31:  86%|████████▌ | 89/104 [00:18<00:03,  4.77it/s, loss=0.0223, v_num=0, reduced_train_loss=0.00135, global_step=2399.0, val_loss=0.113]\n",
      "Epoch 31:  73%|███████▎  | 76/104 [00:18<00:06,  4.21it/s, loss=0.0408, v_num=0, reduced_train_loss=0.000167, global_step=2399.0, val_loss=0.0745]\n",
      "Epoch 31:  34%|███▎      | 35/104 [00:07<00:15,  4.47it/s, loss=0.0255, v_num=0, reduced_train_loss=0.00503, global_step=2359.0, val_loss=0.0942]\n",
      "Epoch 31:  74%|███████▍  | 77/104 [00:18<00:06,  4.23it/s, loss=0.0408, v_num=0, reduced_train_loss=0.000167, global_step=2399.0, val_loss=0.0745]\n",
      "Epoch 31:  88%|████████▊ | 91/104 [00:18<00:02,  4.81it/s, loss=0.0223, v_num=0, reduced_train_loss=0.00135, global_step=2399.0, val_loss=0.113]\n",
      "Epoch 31:  75%|███████▌  | 78/104 [00:18<00:06,  4.25it/s, loss=0.0408, v_num=0, reduced_train_loss=0.000167, global_step=2399.0, val_loss=0.0745]\n",
      "Epoch 31:  35%|███▍      | 36/104 [00:08<00:15,  4.47it/s, loss=0.0255, v_num=0, reduced_train_loss=0.00101, global_step=2360.0, val_loss=0.0942]\n",
      "Epoch 31:  89%|████████▉ | 93/104 [00:19<00:02,  4.85it/s, loss=0.0223, v_num=0, reduced_train_loss=0.00135, global_step=2399.0, val_loss=0.113]\n",
      "Epoch 31:  76%|███████▌  | 79/104 [00:18<00:05,  4.27it/s, loss=0.0408, v_num=0, reduced_train_loss=0.000167, global_step=2399.0, val_loss=0.0745]\n",
      "Epoch 31:  90%|█████████ | 94/104 [00:19<00:02,  4.87it/s, loss=0.0223, v_num=0, reduced_train_loss=0.00135, global_step=2399.0, val_loss=0.113]\n",
      "Epoch 31:  36%|███▌      | 37/104 [00:08<00:14,  4.47it/s, loss=0.027, v_num=0, reduced_train_loss=0.0321, global_step=2361.0, val_loss=0.0942]  ]\n",
      "Epoch 31:  91%|█████████▏| 95/104 [00:19<00:01,  4.89it/s, loss=0.0223, v_num=0, reduced_train_loss=0.00135, global_step=2399.0, val_loss=0.113]\n",
      "Epoch 31:  78%|███████▊  | 81/104 [00:18<00:05,  4.31it/s, loss=0.0408, v_num=0, reduced_train_loss=0.000167, global_step=2399.0, val_loss=0.0745]\n",
      "Epoch 31:  37%|███▋      | 38/104 [00:08<00:14,  4.47it/s, loss=0.0258, v_num=0, reduced_train_loss=0.00094, global_step=2362.0, val_loss=0.0942]\n",
      "Epoch 31:  79%|███████▉  | 82/104 [00:18<00:05,  4.33it/s, loss=0.0408, v_num=0, reduced_train_loss=0.000167, global_step=2399.0, val_loss=0.0745]\n",
      "Epoch 31:  93%|█████████▎| 97/104 [00:19<00:01,  4.94it/s, loss=0.0223, v_num=0, reduced_train_loss=0.00135, global_step=2399.0, val_loss=0.113]\n",
      "Epoch 31:  38%|███▊      | 39/104 [00:08<00:14,  4.47it/s, loss=0.0254, v_num=0, reduced_train_loss=0.00725, global_step=2363.0, val_loss=0.0942]]\n",
      "Epoch 31:  94%|█████████▍| 98/104 [00:19<00:01,  4.96it/s, loss=0.0223, v_num=0, reduced_train_loss=0.00135, global_step=2399.0, val_loss=0.113]\n",
      "Epoch 31:  81%|████████  | 84/104 [00:19<00:04,  4.37it/s, loss=0.0408, v_num=0, reduced_train_loss=0.000167, global_step=2399.0, val_loss=0.0745]\n",
      "Epoch 31:  38%|███▊      | 40/104 [00:08<00:14,  4.47it/s, loss=0.019, v_num=0, reduced_train_loss=0.043, global_step=2364.0, val_loss=0.0942]   \n",
      "Epoch 31:  96%|█████████▌| 100/104 [00:20<00:00,  4.99it/s, loss=0.0223, v_num=0, reduced_train_loss=0.00135, global_step=2399.0, val_loss=0.113]\n",
      "Epoch 31:  82%|████████▏ | 85/104 [00:19<00:04,  4.39it/s, loss=0.0408, v_num=0, reduced_train_loss=0.000167, global_step=2399.0, val_loss=0.0745]\n",
      "Epoch 31:  97%|█████████▋| 101/104 [00:20<00:00,  5.01it/s, loss=0.0223, v_num=0, reduced_train_loss=0.00135, global_step=2399.0, val_loss=0.113]\n",
      "Epoch 31:  39%|███▉      | 41/104 [00:09<00:14,  4.48it/s, loss=0.0189, v_num=0, reduced_train_loss=0.00237, global_step=2365.0, val_loss=0.0942]]\n",
      "Epoch 31:  98%|█████████▊| 102/104 [00:20<00:00,  5.03it/s, loss=0.0223, v_num=0, reduced_train_loss=0.00135, global_step=2399.0, val_loss=0.113]\n",
      "Epoch 31:  84%|████████▎ | 87/104 [00:19<00:03,  4.42it/s, loss=0.0408, v_num=0, reduced_train_loss=0.000167, global_step=2399.0, val_loss=0.0745]\n",
      "Epoch 31:  40%|████      | 42/104 [00:09<00:13,  4.48it/s, loss=0.0187, v_num=0, reduced_train_loss=0.00122, global_step=2366.0, val_loss=0.0942]\n",
      "Epoch 31: 100%|██████████| 104/104 [00:20<00:00,  5.09it/s, loss=0.0223, v_num=0, reduced_train_loss=0.00135, global_step=2399.0, val_loss=0.113]2023-05-03 22:07:02,166 - root - INFO - val_loss: 0.11151871085166931\n",
      "Epoch 31: 100%|██████████| 104/104 [00:20<00:00,  5.08it/s, loss=0.0223, v_num=0, reduced_train_loss=0.00135, global_step=2399.0, val_loss=0.112]\n",
      "Epoch 32:   0%|          | 0/104 [00:00<?, ?it/s, loss=0.0223, v_num=0, reduced_train_loss=0.00135, global_step=2399.0, val_loss=0.112]          \n",
      "Epoch 31:  85%|████████▍ | 88/104 [00:19<00:03,  4.44it/s, loss=0.0408, v_num=0, reduced_train_loss=0.000167, global_step=2399.0, val_loss=0.0745]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 22:07:02.166963 140522389899072 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.11151871085166931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 32:   1%|          | 1/104 [00:00<00:27,  3.75it/s, loss=0.0212, v_num=0, reduced_train_loss=0.000181, global_step=2400.0, val_loss=0.112]]]\n",
      "Epoch 31:  42%|████▏     | 44/104 [00:09<00:13,  4.48it/s, loss=0.0206, v_num=0, reduced_train_loss=0.0787, global_step=2368.0, val_loss=0.0942] ]\n",
      "Epoch 32:   2%|▏         | 2/104 [00:00<00:25,  4.05it/s, loss=0.0138, v_num=0, reduced_train_loss=0.000512, global_step=2401.0, val_loss=0.112]5]\n",
      "Epoch 32:   3%|▎         | 3/104 [00:00<00:24,  4.18it/s, loss=0.0138, v_num=0, reduced_train_loss=0.000192, global_step=2402.0, val_loss=0.112]5]\n",
      "Epoch 31:  44%|████▍     | 46/104 [00:10<00:12,  4.48it/s, loss=0.0219, v_num=0, reduced_train_loss=0.000846, global_step=2370.0, val_loss=0.0942]\n",
      "Epoch 32:   4%|▍         | 4/104 [00:00<00:23,  4.25it/s, loss=0.0082, v_num=0, reduced_train_loss=0.000328, global_step=2403.0, val_loss=0.112]5]\n",
      "Epoch 31:  45%|████▌     | 47/104 [00:10<00:12,  4.48it/s, loss=0.022, v_num=0, reduced_train_loss=0.00415, global_step=2371.0, val_loss=0.0942]  \n",
      "Epoch 31:  46%|████▌     | 48/104 [00:10<00:12,  4.48it/s, loss=0.0199, v_num=0, reduced_train_loss=0.000816, global_step=2372.0, val_loss=0.0942]\n",
      "Epoch 32:   6%|▌         | 6/104 [00:01<00:22,  4.31it/s, loss=0.00814, v_num=0, reduced_train_loss=0.000108, global_step=2405.0, val_loss=0.112]]\n",
      "Epoch 31:  47%|████▋     | 49/104 [00:10<00:12,  4.48it/s, loss=0.0194, v_num=0, reduced_train_loss=0.000853, global_step=2373.0, val_loss=0.0942]\n",
      "Epoch 32:   7%|▋         | 7/104 [00:01<00:22,  4.33it/s, loss=0.00816, v_num=0, reduced_train_loss=0.000455, global_step=2406.0, val_loss=0.112]]\n",
      "Epoch 32:   8%|▊         | 8/104 [00:01<00:22,  4.34it/s, loss=0.00816, v_num=0, reduced_train_loss=0.000524, global_step=2407.0, val_loss=0.112] ]\n",
      "Epoch 31:  49%|████▉     | 51/104 [00:11<00:11,  4.47it/s, loss=0.0506, v_num=0, reduced_train_loss=0.158, global_step=2375.0, val_loss=0.0942]745]\n",
      "Epoch 32:   9%|▊         | 9/104 [00:02<00:21,  4.36it/s, loss=0.005, v_num=0, reduced_train_loss=0.000221, global_step=2408.0, val_loss=0.112]  5]\n",
      "Epoch 31:  50%|█████     | 52/104 [00:11<00:11,  4.47it/s, loss=0.051, v_num=0, reduced_train_loss=0.00763, global_step=2376.0, val_loss=0.0942]45]\n",
      "Epoch 31: 100%|██████████| 104/104 [00:22<00:00,  4.72it/s, loss=0.0408, v_num=0, reduced_train_loss=0.000167, global_step=2399.0, val_loss=0.0745]2023-05-03 22:07:04,414 - root - INFO - val_loss: 0.05444490537047386\n",
      "Epoch 31: 100%|██████████| 104/104 [00:22<00:00,  4.72it/s, loss=0.0408, v_num=0, reduced_train_loss=0.000167, global_step=2399.0, val_loss=0.0544]\n",
      "Epoch 32:  10%|▉         | 10/104 [00:02<00:21,  4.36it/s, loss=0.00108, v_num=0, reduced_train_loss=0.00027, global_step=2409.0, val_loss=0.112]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 22:07:04.414193 140649257482048 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.05444490537047386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31:  72%|███████▏  | 75/104 [00:16<00:06,  4.45it/s, loss=0.0422, v_num=0, reduced_train_loss=0.0292, global_step=2399.0, val_loss=0.0942]]  \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 32:  21%|██        | 22/104 [00:05<00:19,  4.17it/s, loss=0.0381, v_num=0, reduced_train_loss=0.00132, global_step=2421.0, val_loss=0.0544]\n",
      "Epoch 32:  31%|███       | 32/104 [00:07<00:17,  4.14it/s, loss=0.0177, v_num=0, reduced_train_loss=0.00403, global_step=2431.0, val_loss=0.112] \n",
      "Epoch 32:  22%|██▏       | 23/104 [00:05<00:19,  4.19it/s, loss=0.000638, v_num=0, reduced_train_loss=0.000148, global_step=2422.0, val_loss=0.0544]\n",
      "Epoch 32:  32%|███▏      | 33/104 [00:07<00:17,  4.14it/s, loss=0.0176, v_num=0, reduced_train_loss=0.0012, global_step=2432.0, val_loss=0.112] 44] \n",
      "Epoch 31:  76%|███████▌  | 79/104 [00:17<00:05,  4.54it/s, loss=0.0422, v_num=0, reduced_train_loss=0.0292, global_step=2399.0, val_loss=0.0942]\n",
      "Epoch 32:  33%|███▎      | 34/104 [00:08<00:16,  4.14it/s, loss=0.0177, v_num=0, reduced_train_loss=0.000898, global_step=2433.0, val_loss=0.112]44]\n",
      "Epoch 31:  78%|███████▊  | 81/104 [00:17<00:05,  4.59it/s, loss=0.0422, v_num=0, reduced_train_loss=0.0292, global_step=2399.0, val_loss=0.0942]\n",
      "Epoch 32:  34%|███▎      | 35/104 [00:08<00:16,  4.14it/s, loss=0.0162, v_num=0, reduced_train_loss=0.00112, global_step=2434.0, val_loss=0.112] 4] \n",
      "Epoch 31:  80%|███████▉  | 83/104 [00:17<00:04,  4.63it/s, loss=0.0422, v_num=0, reduced_train_loss=0.0292, global_step=2399.0, val_loss=0.0942]\n",
      "Epoch 32:  35%|███▍      | 36/104 [00:08<00:16,  4.13it/s, loss=0.0162, v_num=0, reduced_train_loss=0.000119, global_step=2435.0, val_loss=0.112]44]\n",
      "Epoch 32:  27%|██▋       | 28/104 [00:06<00:17,  4.24it/s, loss=0.000861, v_num=0, reduced_train_loss=0.000653, global_step=2427.0, val_loss=0.0544]\n",
      "Epoch 32:  36%|███▌      | 37/104 [00:08<00:16,  4.13it/s, loss=0.0163, v_num=0, reduced_train_loss=0.00281, global_step=2436.0, val_loss=0.112] \n",
      "Epoch 32:  28%|██▊       | 29/104 [00:06<00:17,  4.25it/s, loss=0.00106, v_num=0, reduced_train_loss=0.00749, global_step=2428.0, val_loss=0.0544]  \n",
      "Epoch 32:  37%|███▋      | 38/104 [00:09<00:15,  4.13it/s, loss=0.0153, v_num=0, reduced_train_loss=0.000215, global_step=2437.0, val_loss=0.112]\n",
      "Epoch 32:  29%|██▉       | 30/104 [00:07<00:17,  4.25it/s, loss=0.00107, v_num=0, reduced_train_loss=0.000188, global_step=2429.0, val_loss=0.0544]\n",
      "Epoch 32:  38%|███▊      | 39/104 [00:09<00:15,  4.13it/s, loss=0.0153, v_num=0, reduced_train_loss=0.0011, global_step=2438.0, val_loss=0.112]  \n",
      "Epoch 32:  30%|██▉       | 31/104 [00:07<00:17,  4.26it/s, loss=0.00108, v_num=0, reduced_train_loss=0.000362, global_step=2430.0, val_loss=0.0544]\n",
      "Epoch 32:  38%|███▊      | 40/104 [00:09<00:15,  4.13it/s, loss=0.0153, v_num=0, reduced_train_loss=0.00061, global_step=2439.0, val_loss=0.112]\n",
      "Epoch 32:  31%|███       | 32/104 [00:07<00:16,  4.26it/s, loss=0.00109, v_num=0, reduced_train_loss=0.000363, global_step=2431.0, val_loss=0.0544]\n",
      "Epoch 32:  32%|███▏      | 33/104 [00:07<00:16,  4.27it/s, loss=0.00111, v_num=0, reduced_train_loss=0.0011, global_step=2432.0, val_loss=0.0544]  \n",
      "Epoch 31:  91%|█████████▏| 95/104 [00:19<00:01,  4.89it/s, loss=0.0422, v_num=0, reduced_train_loss=0.0292, global_step=2399.0, val_loss=0.0942]\n",
      "Epoch 32:  33%|███▎      | 34/104 [00:07<00:16,  4.28it/s, loss=0.00103, v_num=0, reduced_train_loss=0.000239, global_step=2433.0, val_loss=0.0544]\n",
      "Epoch 31:  93%|█████████▎| 97/104 [00:19<00:01,  4.93it/s, loss=0.0422, v_num=0, reduced_train_loss=0.0292, global_step=2399.0, val_loss=0.0942]\n",
      "Epoch 32:  41%|████▏     | 43/104 [00:10<00:14,  4.12it/s, loss=0.0154, v_num=0, reduced_train_loss=0.00807, global_step=2442.0, val_loss=0.112] ] \n",
      "Epoch 31:  95%|█████████▌| 99/104 [00:19<00:01,  4.97it/s, loss=0.0422, v_num=0, reduced_train_loss=0.0292, global_step=2399.0, val_loss=0.0942]\n",
      "Epoch 32:  42%|████▏     | 44/104 [00:10<00:14,  4.11it/s, loss=0.0143, v_num=0, reduced_train_loss=0.00108, global_step=2443.0, val_loss=0.112]44]\n",
      "Epoch 31:  97%|█████████▋| 101/104 [00:20<00:00,  5.01it/s, loss=0.0422, v_num=0, reduced_train_loss=0.0292, global_step=2399.0, val_loss=0.0942]\n",
      "Epoch 32:  43%|████▎     | 45/104 [00:10<00:14,  4.11it/s, loss=0.00272, v_num=0, reduced_train_loss=0.000229, global_step=2444.0, val_loss=0.112] \n",
      "Epoch 31:  99%|█████████▉| 103/104 [00:20<00:00,  5.05it/s, loss=0.0422, v_num=0, reduced_train_loss=0.0292, global_step=2399.0, val_loss=0.0942]\n",
      "Epoch 31: 100%|██████████| 104/104 [00:20<00:00,  5.08it/s, loss=0.0422, v_num=0, reduced_train_loss=0.0292, global_step=2399.0, val_loss=0.0942]2023-05-03 22:07:13,227 - root - INFO - val_loss: 0.11029471457004547\n",
      "Epoch 31: 100%|██████████| 104/104 [00:20<00:00,  5.08it/s, loss=0.0422, v_num=0, reduced_train_loss=0.0292, global_step=2399.0, val_loss=0.110] \n",
      "Epoch 32:   0%|          | 0/104 [00:00<?, ?it/s, loss=0.0422, v_num=0, reduced_train_loss=0.0292, global_step=2399.0, val_loss=0.110]          "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 22:07:13.227687 139755592410944 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.11029471457004547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32:  72%|███████▏  | 75/104 [00:18<00:07,  4.10it/s, loss=0.00466, v_num=0, reduced_train_loss=0.000199, global_step=2474.0, val_loss=0.112]  \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 32:  31%|███       | 32/104 [00:07<00:16,  4.40it/s, loss=0.0467, v_num=0, reduced_train_loss=0.00111, global_step=2431.0, val_loss=0.110]\n",
      "Epoch 32:  32%|███▏      | 33/104 [00:07<00:16,  4.40it/s, loss=0.0387, v_num=0, reduced_train_loss=0.00182, global_step=2432.0, val_loss=0.110]44]\n",
      "Epoch 32:  69%|██████▉   | 72/104 [00:16<00:07,  4.37it/s, loss=0.00202, v_num=0, reduced_train_loss=0.000132, global_step=2471.0, val_loss=0.0544]\n",
      "Epoch 32:  33%|███▎      | 34/104 [00:07<00:15,  4.40it/s, loss=0.0197, v_num=0, reduced_train_loss=0.0768, global_step=2433.0, val_loss=0.110] 2]\n",
      "Epoch 32:  34%|███▎      | 35/104 [00:07<00:15,  4.40it/s, loss=0.0224, v_num=0, reduced_train_loss=0.055, global_step=2434.0, val_loss=0.110] 544]\n",
      "Epoch 32:  71%|███████   | 74/104 [00:16<00:06,  4.37it/s, loss=0.00201, v_num=0, reduced_train_loss=9.13e-5, global_step=2473.0, val_loss=0.0544] \n",
      "Epoch 32:  35%|███▍      | 36/104 [00:08<00:15,  4.40it/s, loss=0.024, v_num=0, reduced_train_loss=0.0381, global_step=2435.0, val_loss=0.110]112]\n",
      "Epoch 32:  72%|███████▏  | 75/104 [00:17<00:06,  4.37it/s, loss=0.00201, v_num=0, reduced_train_loss=0.000289, global_step=2474.0, val_loss=0.0544]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 32:  36%|███▌      | 37/104 [00:08<00:15,  4.40it/s, loss=0.0237, v_num=0, reduced_train_loss=0.000491, global_step=2436.0, val_loss=0.110]\n",
      "Epoch 32:  80%|███████▉  | 83/104 [00:19<00:04,  4.26it/s, loss=0.00466, v_num=0, reduced_train_loss=0.000199, global_step=2474.0, val_loss=0.112]\n",
      "Epoch 32:  73%|███████▎  | 76/104 [00:17<00:06,  4.38it/s, loss=0.00201, v_num=0, reduced_train_loss=0.000289, global_step=2474.0, val_loss=0.0544]\n",
      "Epoch 32:  37%|███▋      | 38/104 [00:08<00:15,  4.40it/s, loss=0.0227, v_num=0, reduced_train_loss=0.017, global_step=2437.0, val_loss=0.110]   ]\n",
      "Epoch 32:  74%|███████▍  | 77/104 [00:17<00:06,  4.41it/s, loss=0.00201, v_num=0, reduced_train_loss=0.000289, global_step=2474.0, val_loss=0.0544]\n",
      "Epoch 32:  82%|████████▏ | 85/104 [00:19<00:04,  4.29it/s, loss=0.00466, v_num=0, reduced_train_loss=0.000199, global_step=2474.0, val_loss=0.112]\n",
      "Epoch 32:  38%|███▊      | 39/104 [00:08<00:14,  4.40it/s, loss=0.0246, v_num=0, reduced_train_loss=0.0387, global_step=2438.0, val_loss=0.110]544]\n",
      "Epoch 32:  83%|████████▎ | 86/104 [00:19<00:04,  4.31it/s, loss=0.00466, v_num=0, reduced_train_loss=0.000199, global_step=2474.0, val_loss=0.112]\n",
      "Epoch 32:  76%|███████▌  | 79/104 [00:17<00:05,  4.46it/s, loss=0.00201, v_num=0, reduced_train_loss=0.000289, global_step=2474.0, val_loss=0.0544]\n",
      "Epoch 32:  77%|███████▋  | 80/104 [00:17<00:05,  4.49it/s, loss=0.00201, v_num=0, reduced_train_loss=0.000289, global_step=2474.0, val_loss=0.0544]\n",
      "Epoch 32:  38%|███▊      | 40/104 [00:09<00:14,  4.40it/s, loss=0.0251, v_num=0, reduced_train_loss=0.0133, global_step=2439.0, val_loss=0.110]12]\n",
      "Epoch 32:  78%|███████▊  | 81/104 [00:17<00:05,  4.51it/s, loss=0.00201, v_num=0, reduced_train_loss=0.000289, global_step=2474.0, val_loss=0.0544]\n",
      "Epoch 32:  85%|████████▍ | 88/104 [00:20<00:03,  4.35it/s, loss=0.00466, v_num=0, reduced_train_loss=0.000199, global_step=2474.0, val_loss=0.112]\n",
      "Epoch 32:  79%|███████▉  | 82/104 [00:18<00:04,  4.54it/s, loss=0.00201, v_num=0, reduced_train_loss=0.000289, global_step=2474.0, val_loss=0.0544]\n",
      "Epoch 32:  39%|███▉      | 41/104 [00:09<00:14,  4.40it/s, loss=0.024, v_num=0, reduced_train_loss=0.00724, global_step=2440.0, val_loss=0.110]12]\n",
      "Epoch 32:  80%|███████▉  | 83/104 [00:18<00:04,  4.56it/s, loss=0.00201, v_num=0, reduced_train_loss=0.000289, global_step=2474.0, val_loss=0.0544]\n",
      "Epoch 32:  87%|████████▋ | 90/104 [00:20<00:03,  4.39it/s, loss=0.00466, v_num=0, reduced_train_loss=0.000199, global_step=2474.0, val_loss=0.112]\n",
      "Epoch 32:  40%|████      | 42/104 [00:09<00:14,  4.40it/s, loss=0.0241, v_num=0, reduced_train_loss=0.00545, global_step=2441.0, val_loss=0.110]44]\n",
      "Epoch 32:  88%|████████▊ | 91/104 [00:20<00:02,  4.41it/s, loss=0.00466, v_num=0, reduced_train_loss=0.000199, global_step=2474.0, val_loss=0.112]\n",
      "Epoch 32:  82%|████████▏ | 85/104 [00:18<00:04,  4.61it/s, loss=0.00201, v_num=0, reduced_train_loss=0.000289, global_step=2474.0, val_loss=0.0544]\n",
      "Epoch 32:  88%|████████▊ | 92/104 [00:20<00:02,  4.43it/s, loss=0.00466, v_num=0, reduced_train_loss=0.000199, global_step=2474.0, val_loss=0.112]\n",
      "Epoch 32:  41%|████▏     | 43/104 [00:09<00:13,  4.40it/s, loss=0.0239, v_num=0, reduced_train_loss=0.00693, global_step=2442.0, val_loss=0.110]44]\n",
      "Epoch 32:  84%|████████▎ | 87/104 [00:18<00:03,  4.66it/s, loss=0.00201, v_num=0, reduced_train_loss=0.000289, global_step=2474.0, val_loss=0.0544]\n",
      "Epoch 32:  89%|████████▉ | 93/104 [00:20<00:02,  4.44it/s, loss=0.00466, v_num=0, reduced_train_loss=0.000199, global_step=2474.0, val_loss=0.112]\n",
      "Epoch 32:  42%|████▏     | 44/104 [00:10<00:13,  4.40it/s, loss=0.0245, v_num=0, reduced_train_loss=0.0143, global_step=2443.0, val_loss=0.110] 44]\n",
      "Epoch 32:  90%|█████████ | 94/104 [00:21<00:02,  4.46it/s, loss=0.00466, v_num=0, reduced_train_loss=0.000199, global_step=2474.0, val_loss=0.112]\n",
      "Epoch 32:  86%|████████▌ | 89/104 [00:18<00:03,  4.70it/s, loss=0.00201, v_num=0, reduced_train_loss=0.000289, global_step=2474.0, val_loss=0.0544]\n",
      "Epoch 32:  91%|█████████▏| 95/104 [00:21<00:02,  4.48it/s, loss=0.00466, v_num=0, reduced_train_loss=0.000199, global_step=2474.0, val_loss=0.112]\n",
      "Epoch 32:  43%|████▎     | 45/104 [00:10<00:13,  4.40it/s, loss=0.0245, v_num=0, reduced_train_loss=0.00385, global_step=2444.0, val_loss=0.110]44]\n",
      "Epoch 32:  92%|█████████▏| 96/104 [00:21<00:01,  4.50it/s, loss=0.00466, v_num=0, reduced_train_loss=0.000199, global_step=2474.0, val_loss=0.112]\n",
      "Epoch 32:  88%|████████▊ | 91/104 [00:19<00:02,  4.75it/s, loss=0.00201, v_num=0, reduced_train_loss=0.000289, global_step=2474.0, val_loss=0.0544]\n",
      "Epoch 32:  44%|████▍     | 46/104 [00:10<00:13,  4.40it/s, loss=0.0245, v_num=0, reduced_train_loss=0.000799, global_step=2445.0, val_loss=0.110]]\n",
      "Epoch 32:  88%|████████▊ | 92/104 [00:19<00:02,  4.77it/s, loss=0.00201, v_num=0, reduced_train_loss=0.000289, global_step=2474.0, val_loss=0.0544]\n",
      "Epoch 32:  94%|█████████▍| 98/104 [00:21<00:01,  4.53it/s, loss=0.00466, v_num=0, reduced_train_loss=0.000199, global_step=2474.0, val_loss=0.112]\n",
      "Epoch 32:  45%|████▌     | 47/104 [00:10<00:12,  4.40it/s, loss=0.0347, v_num=0, reduced_train_loss=0.207, global_step=2446.0, val_loss=0.110]   4]\n",
      "Epoch 32:  95%|█████████▌| 99/104 [00:21<00:01,  4.55it/s, loss=0.00466, v_num=0, reduced_train_loss=0.000199, global_step=2474.0, val_loss=0.112]\n",
      "Epoch 32:  90%|█████████ | 94/104 [00:19<00:02,  4.81it/s, loss=0.00201, v_num=0, reduced_train_loss=0.000289, global_step=2474.0, val_loss=0.0544]\n",
      "Epoch 32:  91%|█████████▏| 95/104 [00:19<00:01,  4.84it/s, loss=0.00201, v_num=0, reduced_train_loss=0.000289, global_step=2474.0, val_loss=0.0544]\n",
      "Epoch 32:  46%|████▌     | 48/104 [00:10<00:12,  4.40it/s, loss=0.0252, v_num=0, reduced_train_loss=0.00344, global_step=2447.0, val_loss=0.110]12]\n",
      "Epoch 32:  92%|█████████▏| 96/104 [00:19<00:01,  4.86it/s, loss=0.00201, v_num=0, reduced_train_loss=0.000289, global_step=2474.0, val_loss=0.0544]\n",
      "Epoch 32:  97%|█████████▋| 101/104 [00:22<00:00,  4.58it/s, loss=0.00466, v_num=0, reduced_train_loss=0.000199, global_step=2474.0, val_loss=0.112]\n",
      "Epoch 32:  47%|████▋     | 49/104 [00:11<00:12,  4.40it/s, loss=0.0248, v_num=0, reduced_train_loss=0.000206, global_step=2448.0, val_loss=0.110]4]\n",
      "Epoch 32:  98%|█████████▊| 102/104 [00:22<00:00,  4.59it/s, loss=0.00466, v_num=0, reduced_train_loss=0.000199, global_step=2474.0, val_loss=0.112]\n",
      "Epoch 32:  94%|█████████▍| 98/104 [00:20<00:01,  4.90it/s, loss=0.00201, v_num=0, reduced_train_loss=0.000289, global_step=2474.0, val_loss=0.0544]\n",
      "Epoch 32:  99%|█████████▉| 103/104 [00:22<00:00,  4.61it/s, loss=0.00466, v_num=0, reduced_train_loss=0.000199, global_step=2474.0, val_loss=0.112]\n",
      "Epoch 32:  95%|█████████▌| 99/104 [00:20<00:01,  4.92it/s, loss=0.00201, v_num=0, reduced_train_loss=0.000289, global_step=2474.0, val_loss=0.0544]\n",
      "Epoch 32: 100%|██████████| 104/104 [00:22<00:00,  4.64it/s, loss=0.00466, v_num=0, reduced_train_loss=0.000199, global_step=2474.0, val_loss=0.112]2023-05-03 22:07:24,590 - root - INFO - val_loss: 0.11463280022144318\n",
      "Epoch 32: 100%|██████████| 104/104 [00:22<00:00,  4.64it/s, loss=0.00466, v_num=0, reduced_train_loss=0.000199, global_step=2474.0, val_loss=0.115]\n",
      "Epoch 33:   0%|          | 0/104 [00:00<?, ?it/s, loss=0.00466, v_num=0, reduced_train_loss=0.000199, global_step=2474.0, val_loss=0.115]          \n",
      "Epoch 32:  96%|█████████▌| 100/104 [00:20<00:00,  4.94it/s, loss=0.00201, v_num=0, reduced_train_loss=0.000289, global_step=2474.0, val_loss=0.0544]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 22:07:24.590756 140522389899072 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.11463280022144318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 33:   1%|          | 1/104 [00:00<00:29,  3.46it/s, loss=0.00463, v_num=0, reduced_train_loss=0.000308, global_step=2475.0, val_loss=0.115]44]\n",
      "Epoch 32:  98%|█████████▊| 102/104 [00:20<00:00,  4.98it/s, loss=0.00201, v_num=0, reduced_train_loss=0.000289, global_step=2474.0, val_loss=0.0544]\n",
      "Epoch 32:  50%|█████     | 52/104 [00:11<00:11,  4.40it/s, loss=0.0381, v_num=0, reduced_train_loss=0.000347, global_step=2451.0, val_loss=0.110]44]\n",
      "Epoch 32: 100%|██████████| 104/104 [00:20<00:00,  5.03it/s, loss=0.00201, v_num=0, reduced_train_loss=0.000289, global_step=2474.0, val_loss=0.0544]2023-05-03 22:07:25,108 - root - INFO - val_loss: 0.06812835484743118\n",
      "Epoch 32: 100%|██████████| 104/104 [00:20<00:00,  5.03it/s, loss=0.00201, v_num=0, reduced_train_loss=0.000289, global_step=2474.0, val_loss=0.0681]\n",
      "Epoch 33:   2%|▏         | 2/104 [00:00<00:27,  3.69it/s, loss=0.00466, v_num=0, reduced_train_loss=0.000796, global_step=2476.0, val_loss=0.115]   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 22:07:25.108293 140649257482048 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.06812835484743118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32:  72%|███████▏  | 75/104 [00:17<00:06,  4.41it/s, loss=0.0387, v_num=0, reduced_train_loss=0.000769, global_step=2474.0, val_loss=0.110]1]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 33:  22%|██▏       | 23/104 [00:05<00:18,  4.40it/s, loss=0.00188, v_num=0, reduced_train_loss=0.000954, global_step=2497.0, val_loss=0.0681]\n",
      "Epoch 32:  73%|███████▎  | 76/104 [00:17<00:06,  4.42it/s, loss=0.0387, v_num=0, reduced_train_loss=0.000769, global_step=2474.0, val_loss=0.110]\n",
      "Epoch 33:  23%|██▎       | 24/104 [00:05<00:19,  4.01it/s, loss=0.0104, v_num=0, reduced_train_loss=0.00362, global_step=2498.0, val_loss=0.115] 1]\n",
      "Epoch 32:  75%|███████▌  | 78/104 [00:17<00:05,  4.47it/s, loss=0.0387, v_num=0, reduced_train_loss=0.000769, global_step=2474.0, val_loss=0.110]\n",
      "Epoch 33:  24%|██▍       | 25/104 [00:06<00:19,  4.01it/s, loss=0.0105, v_num=0, reduced_train_loss=0.00647, global_step=2499.0, val_loss=0.115]81]\n",
      "Epoch 33:  25%|██▌       | 26/104 [00:05<00:17,  4.40it/s, loss=0.00183, v_num=0, reduced_train_loss=5.89e-5, global_step=2500.0, val_loss=0.0681] \n",
      "Epoch 33:  25%|██▌       | 26/104 [00:06<00:19,  4.01it/s, loss=0.0105, v_num=0, reduced_train_loss=0.000141, global_step=2500.0, val_loss=0.115]\n",
      "Epoch 33:  26%|██▌       | 27/104 [00:06<00:17,  4.40it/s, loss=0.00183, v_num=0, reduced_train_loss=0.000276, global_step=2501.0, val_loss=0.0681]\n",
      "Epoch 33:  26%|██▌       | 27/104 [00:06<00:19,  4.02it/s, loss=0.0105, v_num=0, reduced_train_loss=0.000366, global_step=2501.0, val_loss=0.115]\n",
      "Epoch 33:  27%|██▋       | 28/104 [00:06<00:17,  4.40it/s, loss=0.0018, v_num=0, reduced_train_loss=6.37e-5, global_step=2502.0, val_loss=0.0681]  \n",
      "Epoch 33:  27%|██▋       | 28/104 [00:06<00:18,  4.02it/s, loss=0.0105, v_num=0, reduced_train_loss=0.00105, global_step=2502.0, val_loss=0.115] \n",
      "Epoch 33:  28%|██▊       | 29/104 [00:06<00:17,  4.41it/s, loss=0.00175, v_num=0, reduced_train_loss=6.14e-5, global_step=2503.0, val_loss=0.0681]\n",
      "Epoch 33:  29%|██▉       | 30/104 [00:06<00:16,  4.41it/s, loss=0.00175, v_num=0, reduced_train_loss=0.000247, global_step=2504.0, val_loss=0.0681]\n",
      "Epoch 32:  85%|████████▍ | 88/104 [00:18<00:03,  4.71it/s, loss=0.0387, v_num=0, reduced_train_loss=0.000769, global_step=2474.0, val_loss=0.110]\n",
      "Epoch 33:  30%|██▉       | 31/104 [00:07<00:16,  4.41it/s, loss=0.000595, v_num=0, reduced_train_loss=0.00029, global_step=2505.0, val_loss=0.0681]\n",
      "Epoch 32:  87%|████████▋ | 90/104 [00:18<00:02,  4.75it/s, loss=0.0387, v_num=0, reduced_train_loss=0.000769, global_step=2474.0, val_loss=0.110]\n",
      "Epoch 33:  31%|███       | 32/104 [00:07<00:16,  4.41it/s, loss=0.000586, v_num=0, reduced_train_loss=3.86e-5, global_step=2506.0, val_loss=0.0681]\n",
      "Epoch 32:  88%|████████▊ | 92/104 [00:19<00:02,  4.80it/s, loss=0.0387, v_num=0, reduced_train_loss=0.000769, global_step=2474.0, val_loss=0.110]\n",
      "Epoch 33:  32%|███▏      | 33/104 [00:07<00:16,  4.41it/s, loss=0.0125, v_num=0, reduced_train_loss=0.239, global_step=2507.0, val_loss=0.0681]    \n",
      "Epoch 32:  90%|█████████ | 94/104 [00:19<00:02,  4.84it/s, loss=0.0387, v_num=0, reduced_train_loss=0.000769, global_step=2474.0, val_loss=0.110]\n",
      "Epoch 33:  33%|███▎      | 34/104 [00:07<00:15,  4.41it/s, loss=0.0125, v_num=0, reduced_train_loss=0.000179, global_step=2508.0, val_loss=0.0681]\n",
      "Epoch 32:  92%|█████████▏| 96/104 [00:19<00:01,  4.88it/s, loss=0.0387, v_num=0, reduced_train_loss=0.000769, global_step=2474.0, val_loss=0.110]\n",
      "Epoch 33:  34%|███▎      | 35/104 [00:07<00:15,  4.42it/s, loss=0.0125, v_num=0, reduced_train_loss=5.34e-5, global_step=2509.0, val_loss=0.0681] \n",
      "Epoch 33:  35%|███▍      | 36/104 [00:08<00:15,  4.42it/s, loss=0.0125, v_num=0, reduced_train_loss=0.000184, global_step=2510.0, val_loss=0.0681]\n",
      "Epoch 33:  34%|███▎      | 35/104 [00:08<00:17,  4.03it/s, loss=0.029, v_num=0, reduced_train_loss=0.000213, global_step=2509.0, val_loss=0.115]]\n",
      "Epoch 33:  36%|███▌      | 37/104 [00:08<00:15,  4.42it/s, loss=0.0125, v_num=0, reduced_train_loss=9.34e-5, global_step=2511.0, val_loss=0.0681] \n",
      "Epoch 33:  35%|███▍      | 36/104 [00:08<00:16,  4.03it/s, loss=0.0288, v_num=0, reduced_train_loss=0.000497, global_step=2510.0, val_loss=0.115]]\n",
      "Epoch 33:  37%|███▋      | 38/104 [00:08<00:14,  4.41it/s, loss=0.0125, v_num=0, reduced_train_loss=0.000195, global_step=2512.0, val_loss=0.0681]\n",
      "Epoch 33:  36%|███▌      | 37/104 [00:09<00:16,  4.03it/s, loss=0.0313, v_num=0, reduced_train_loss=0.0693, global_step=2511.0, val_loss=0.115]  ]\n",
      "Epoch 32: 100%|██████████| 104/104 [00:20<00:00,  5.05it/s, loss=0.0387, v_num=0, reduced_train_loss=0.000769, global_step=2474.0, val_loss=0.110]2023-05-03 22:07:33,842 - root - INFO - val_loss: 0.14431501924991608\n",
      "Epoch 32: 100%|██████████| 104/104 [00:20<00:00,  5.05it/s, loss=0.0387, v_num=0, reduced_train_loss=0.000769, global_step=2474.0, val_loss=0.144]\n",
      "Epoch 33:   0%|          | 0/104 [00:00<?, ?it/s, loss=0.0387, v_num=0, reduced_train_loss=0.000769, global_step=2474.0, val_loss=0.144]          "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 22:07:33.842105 139755592410944 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.14431501924991608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33:  72%|███████▏  | 75/104 [00:17<00:06,  4.38it/s, loss=0.00541, v_num=0, reduced_train_loss=0.00601, global_step=2549.0, val_loss=0.0681] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 33:  69%|██████▉   | 72/104 [00:17<00:07,  4.05it/s, loss=0.0169, v_num=0, reduced_train_loss=0.000106, global_step=2546.0, val_loss=0.115]\n",
      "Epoch 33:  73%|███████▎  | 76/104 [00:17<00:06,  4.40it/s, loss=0.00541, v_num=0, reduced_train_loss=0.00601, global_step=2549.0, val_loss=0.0681]\n",
      "Epoch 33:  70%|███████   | 73/104 [00:18<00:07,  4.05it/s, loss=0.0167, v_num=0, reduced_train_loss=0.000265, global_step=2547.0, val_loss=0.115]]\n",
      "Epoch 33:  75%|███████▌  | 78/104 [00:17<00:05,  4.45it/s, loss=0.00541, v_num=0, reduced_train_loss=0.00601, global_step=2549.0, val_loss=0.0681]\n",
      "Epoch 33:  71%|███████   | 74/104 [00:18<00:07,  4.05it/s, loss=0.017, v_num=0, reduced_train_loss=0.00493, global_step=2548.0, val_loss=0.115]  ]\n",
      "Epoch 33:  77%|███████▋  | 80/104 [00:17<00:05,  4.50it/s, loss=0.00541, v_num=0, reduced_train_loss=0.00601, global_step=2549.0, val_loss=0.0681]\n",
      "Epoch 33:  72%|███████▏  | 75/104 [00:18<00:07,  4.05it/s, loss=0.017, v_num=0, reduced_train_loss=0.000299, global_step=2549.0, val_loss=0.115] ]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33:  79%|███████▉  | 82/104 [00:18<00:04,  4.55it/s, loss=0.00541, v_num=0, reduced_train_loss=0.00601, global_step=2549.0, val_loss=0.0681]\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 33:  39%|███▉      | 41/104 [00:09<00:14,  4.34it/s, loss=0.0364, v_num=0, reduced_train_loss=0.0528, global_step=2515.0, val_loss=0.144]81]\n",
      "Epoch 33:  73%|███████▎  | 76/104 [00:18<00:06,  4.06it/s, loss=0.017, v_num=0, reduced_train_loss=0.000299, global_step=2549.0, val_loss=0.115]\n",
      "Epoch 33:  81%|████████  | 84/104 [00:18<00:04,  4.60it/s, loss=0.00541, v_num=0, reduced_train_loss=0.00601, global_step=2549.0, val_loss=0.0681]\n",
      "Epoch 33:  74%|███████▍  | 77/104 [00:18<00:06,  4.08it/s, loss=0.017, v_num=0, reduced_train_loss=0.000299, global_step=2549.0, val_loss=0.115]\n",
      "Epoch 33:  40%|████      | 42/104 [00:09<00:14,  4.34it/s, loss=0.0373, v_num=0, reduced_train_loss=0.0194, global_step=2516.0, val_loss=0.144]81]\n",
      "Epoch 33:  75%|███████▌  | 78/104 [00:19<00:06,  4.10it/s, loss=0.017, v_num=0, reduced_train_loss=0.000299, global_step=2549.0, val_loss=0.115]\n",
      "Epoch 33:  83%|████████▎ | 86/104 [00:18<00:03,  4.65it/s, loss=0.00541, v_num=0, reduced_train_loss=0.00601, global_step=2549.0, val_loss=0.0681]\n",
      "Epoch 33:  76%|███████▌  | 79/104 [00:19<00:06,  4.13it/s, loss=0.017, v_num=0, reduced_train_loss=0.000299, global_step=2549.0, val_loss=0.115]\n",
      "Epoch 33:  41%|████▏     | 43/104 [00:09<00:14,  4.34it/s, loss=0.0373, v_num=0, reduced_train_loss=0.000188, global_step=2517.0, val_loss=0.144]]\n",
      "Epoch 33:  85%|████████▍ | 88/104 [00:18<00:03,  4.69it/s, loss=0.00541, v_num=0, reduced_train_loss=0.00601, global_step=2549.0, val_loss=0.0681]\n",
      "Epoch 33:  42%|████▏     | 44/104 [00:10<00:13,  4.34it/s, loss=0.0372, v_num=0, reduced_train_loss=0.00728, global_step=2518.0, val_loss=0.144] \n",
      "Epoch 33:  86%|████████▌ | 89/104 [00:18<00:03,  4.72it/s, loss=0.00541, v_num=0, reduced_train_loss=0.00601, global_step=2549.0, val_loss=0.0681]\n",
      "Epoch 33:  78%|███████▊  | 81/104 [00:19<00:05,  4.17it/s, loss=0.017, v_num=0, reduced_train_loss=0.000299, global_step=2549.0, val_loss=0.115]\n",
      "Epoch 33:  87%|████████▋ | 90/104 [00:18<00:02,  4.74it/s, loss=0.00541, v_num=0, reduced_train_loss=0.00601, global_step=2549.0, val_loss=0.0681]\n",
      "Epoch 33:  43%|████▎     | 45/104 [00:10<00:13,  4.34it/s, loss=0.037, v_num=0, reduced_train_loss=0.000683, global_step=2519.0, val_loss=0.144]\n",
      "Epoch 33:  88%|████████▊ | 91/104 [00:19<00:02,  4.76it/s, loss=0.00541, v_num=0, reduced_train_loss=0.00601, global_step=2549.0, val_loss=0.0681]\n",
      "Epoch 33:  80%|███████▉  | 83/104 [00:19<00:04,  4.21it/s, loss=0.017, v_num=0, reduced_train_loss=0.000299, global_step=2549.0, val_loss=0.115]\n",
      "Epoch 33:  44%|████▍     | 46/104 [00:10<00:13,  4.34it/s, loss=0.0305, v_num=0, reduced_train_loss=0.00371, global_step=2520.0, val_loss=0.144]1]\n",
      "Epoch 33:  81%|████████  | 84/104 [00:19<00:04,  4.23it/s, loss=0.017, v_num=0, reduced_train_loss=0.000299, global_step=2549.0, val_loss=0.115]\n",
      "Epoch 33:  89%|████████▉ | 93/104 [00:19<00:02,  4.80it/s, loss=0.00541, v_num=0, reduced_train_loss=0.00601, global_step=2549.0, val_loss=0.0681]\n",
      "Epoch 33:  90%|█████████ | 94/104 [00:19<00:02,  4.83it/s, loss=0.00541, v_num=0, reduced_train_loss=0.00601, global_step=2549.0, val_loss=0.0681]\n",
      "Epoch 33:  45%|████▌     | 47/104 [00:10<00:13,  4.34it/s, loss=0.0292, v_num=0, reduced_train_loss=0.000177, global_step=2521.0, val_loss=0.144]\n",
      "Epoch 33:  91%|█████████▏| 95/104 [00:19<00:01,  4.85it/s, loss=0.00541, v_num=0, reduced_train_loss=0.00601, global_step=2549.0, val_loss=0.0681]\n",
      "Epoch 33:  83%|████████▎ | 86/104 [00:20<00:04,  4.27it/s, loss=0.017, v_num=0, reduced_train_loss=0.000299, global_step=2549.0, val_loss=0.115]\n",
      "Epoch 33:  92%|█████████▏| 96/104 [00:19<00:01,  4.87it/s, loss=0.00541, v_num=0, reduced_train_loss=0.00601, global_step=2549.0, val_loss=0.0681]\n",
      "Epoch 33:  46%|████▌     | 48/104 [00:11<00:12,  4.34it/s, loss=0.0337, v_num=0, reduced_train_loss=0.0991, global_step=2522.0, val_loss=0.144]  \n",
      "Epoch 33:  93%|█████████▎| 97/104 [00:19<00:01,  4.89it/s, loss=0.00541, v_num=0, reduced_train_loss=0.00601, global_step=2549.0, val_loss=0.0681]\n",
      "Epoch 33:  85%|████████▍ | 88/104 [00:20<00:03,  4.31it/s, loss=0.017, v_num=0, reduced_train_loss=0.000299, global_step=2549.0, val_loss=0.115]\n",
      "Epoch 33:  47%|████▋     | 49/104 [00:11<00:12,  4.34it/s, loss=0.0328, v_num=0, reduced_train_loss=0.016, global_step=2523.0, val_loss=0.144] 81]\n",
      "Epoch 33:  86%|████████▌ | 89/104 [00:20<00:03,  4.32it/s, loss=0.017, v_num=0, reduced_train_loss=0.000299, global_step=2549.0, val_loss=0.115]\n",
      "Epoch 33:  95%|█████████▌| 99/104 [00:20<00:01,  4.93it/s, loss=0.00541, v_num=0, reduced_train_loss=0.00601, global_step=2549.0, val_loss=0.0681]\n",
      "Epoch 33:  96%|█████████▌| 100/104 [00:20<00:00,  4.95it/s, loss=0.00541, v_num=0, reduced_train_loss=0.00601, global_step=2549.0, val_loss=0.0681]\n",
      "Epoch 33:  48%|████▊     | 50/104 [00:11<00:12,  4.34it/s, loss=0.0328, v_num=0, reduced_train_loss=0.000328, global_step=2524.0, val_loss=0.144]\n",
      "Epoch 33:  97%|█████████▋| 101/104 [00:20<00:00,  4.97it/s, loss=0.00541, v_num=0, reduced_train_loss=0.00601, global_step=2549.0, val_loss=0.0681]\n",
      "Epoch 33:  88%|████████▊ | 91/104 [00:20<00:02,  4.36it/s, loss=0.017, v_num=0, reduced_train_loss=0.000299, global_step=2549.0, val_loss=0.115]\n",
      "Epoch 33:  49%|████▉     | 51/104 [00:11<00:12,  4.34it/s, loss=0.0277, v_num=0, reduced_train_loss=0.00175, global_step=2525.0, val_loss=0.144] 1]\n",
      "Epoch 33:  88%|████████▊ | 92/104 [00:21<00:02,  4.38it/s, loss=0.017, v_num=0, reduced_train_loss=0.000299, global_step=2549.0, val_loss=0.115]\n",
      "Epoch 33:  99%|█████████▉| 103/104 [00:20<00:00,  5.01it/s, loss=0.00541, v_num=0, reduced_train_loss=0.00601, global_step=2549.0, val_loss=0.0681]\n",
      "Epoch 33: 100%|██████████| 104/104 [00:20<00:00,  5.04it/s, loss=0.00541, v_num=0, reduced_train_loss=0.00601, global_step=2549.0, val_loss=0.0681]2023-05-03 22:07:45,738 - root - INFO - val_loss: 0.06158381327986717\n",
      "Epoch 33: 100%|██████████| 104/104 [00:20<00:00,  5.04it/s, loss=0.00541, v_num=0, reduced_train_loss=0.00601, global_step=2549.0, val_loss=0.0616]\n",
      "Epoch 34:   0%|          | 0/104 [00:00<?, ?it/s, loss=0.00541, v_num=0, reduced_train_loss=0.00601, global_step=2549.0, val_loss=0.0616]          \n",
      "Epoch 33:  89%|████████▉ | 93/104 [00:21<00:02,  4.40it/s, loss=0.017, v_num=0, reduced_train_loss=0.000299, global_step=2549.0, val_loss=0.115]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 22:07:45.738073 140649257482048 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.06158381327986717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33:  50%|█████     | 52/104 [00:11<00:11,  4.34it/s, loss=0.0274, v_num=0, reduced_train_loss=0.000326, global_step=2526.0, val_loss=0.144]\n",
      "Epoch 34:   1%|          | 1/104 [00:00<00:27,  3.72it/s, loss=0.00544, v_num=0, reduced_train_loss=0.000586, global_step=2550.0, val_loss=0.0616]\n",
      "Epoch 33:  51%|█████     | 53/104 [00:12<00:11,  4.34it/s, loss=0.0263, v_num=0, reduced_train_loss=0.00525, global_step=2527.0, val_loss=0.144] \n",
      "Epoch 33:  52%|█████▏    | 54/104 [00:12<00:11,  4.34it/s, loss=0.0262, v_num=0, reduced_train_loss=0.000132, global_step=2528.0, val_loss=0.144]]\n",
      "Epoch 33:  93%|█████████▎| 97/104 [00:21<00:01,  4.47it/s, loss=0.017, v_num=0, reduced_train_loss=0.000299, global_step=2549.0, val_loss=0.115]\n",
      "Epoch 33:  53%|█████▎    | 55/104 [00:12<00:11,  4.34it/s, loss=0.0131, v_num=0, reduced_train_loss=0.00216, global_step=2529.0, val_loss=0.144] ]\n",
      "Epoch 34:   4%|▍         | 4/104 [00:00<00:23,  4.25it/s, loss=0.00536, v_num=0, reduced_train_loss=0.000897, global_step=2553.0, val_loss=0.0616]\n",
      "Epoch 33:  54%|█████▍    | 56/104 [00:12<00:11,  4.34it/s, loss=0.0133, v_num=0, reduced_train_loss=0.0302, global_step=2530.0, val_loss=0.144] ]\n",
      "Epoch 33:  55%|█████▍    | 57/104 [00:13<00:10,  4.34it/s, loss=0.0122, v_num=0, reduced_train_loss=0.00155, global_step=2531.0, val_loss=0.144]6]\n",
      "Epoch 34:   6%|▌         | 6/104 [00:01<00:22,  4.32it/s, loss=0.00179, v_num=0, reduced_train_loss=8e-5, global_step=2555.0, val_loss=0.0616]    \n",
      "Epoch 33:  56%|█████▌    | 58/104 [00:13<00:10,  4.34it/s, loss=0.014, v_num=0, reduced_train_loss=0.0361, global_step=2532.0, val_loss=0.144]  ]\n",
      "Epoch 33: 100%|██████████| 104/104 [00:22<00:00,  4.60it/s, loss=0.017, v_num=0, reduced_train_loss=0.000299, global_step=2549.0, val_loss=0.115]2023-05-03 22:07:47,232 - root - INFO - val_loss: 0.10936349630355835\n",
      "Epoch 33: 100%|██████████| 104/104 [00:22<00:00,  4.59it/s, loss=0.017, v_num=0, reduced_train_loss=0.000299, global_step=2549.0, val_loss=0.109]\n",
      "Epoch 34:   0%|          | 0/104 [00:00<?, ?it/s, loss=0.017, v_num=0, reduced_train_loss=0.000299, global_step=2549.0, val_loss=0.109]          "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 22:07:47.232909 140522389899072 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.10936349630355835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33:  72%|███████▏  | 75/104 [00:17<00:06,  4.36it/s, loss=0.0527, v_num=0, reduced_train_loss=0.00284, global_step=2549.0, val_loss=0.144]]]  \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 34:  23%|██▎       | 24/104 [00:05<00:18,  4.44it/s, loss=0.00936, v_num=0, reduced_train_loss=2.09e-5, global_step=2573.0, val_loss=0.0616]\n",
      "Epoch 34:  15%|█▌        | 16/104 [00:04<00:22,  3.94it/s, loss=0.00717, v_num=0, reduced_train_loss=0.000112, global_step=2565.0, val_loss=0.109]\n",
      "Epoch 34:  24%|██▍       | 25/104 [00:05<00:17,  4.44it/s, loss=0.00936, v_num=0, reduced_train_loss=0.000156, global_step=2574.0, val_loss=0.0616]\n",
      "Epoch 34:  25%|██▌       | 26/104 [00:05<00:17,  4.44it/s, loss=0.00938, v_num=0, reduced_train_loss=0.000485, global_step=2575.0, val_loss=0.0616]\n",
      "Epoch 33:  76%|███████▌  | 79/104 [00:17<00:05,  4.45it/s, loss=0.0527, v_num=0, reduced_train_loss=0.00284, global_step=2549.0, val_loss=0.144]\n",
      "Epoch 34:  26%|██▌       | 27/104 [00:06<00:17,  4.44it/s, loss=0.00939, v_num=0, reduced_train_loss=0.000431, global_step=2576.0, val_loss=0.0616]\n",
      "Epoch 33:  78%|███████▊  | 81/104 [00:18<00:05,  4.50it/s, loss=0.0527, v_num=0, reduced_train_loss=0.00284, global_step=2549.0, val_loss=0.144]\n",
      "Epoch 34:  18%|█▊        | 19/104 [00:04<00:21,  3.94it/s, loss=0.00723, v_num=0, reduced_train_loss=0.000167, global_step=2568.0, val_loss=0.109] \n",
      "Epoch 33:  80%|███████▉  | 83/104 [00:18<00:04,  4.55it/s, loss=0.0527, v_num=0, reduced_train_loss=0.00284, global_step=2549.0, val_loss=0.144]\n",
      "Epoch 34:  19%|█▉        | 20/104 [00:05<00:21,  3.95it/s, loss=0.00737, v_num=0, reduced_train_loss=0.00304, global_step=2569.0, val_loss=0.109] \n",
      "Epoch 33:  82%|████████▏ | 85/104 [00:18<00:04,  4.59it/s, loss=0.0527, v_num=0, reduced_train_loss=0.00284, global_step=2549.0, val_loss=0.144]\n",
      "Epoch 34:  20%|██        | 21/104 [00:05<00:21,  3.95it/s, loss=0.00735, v_num=0, reduced_train_loss=0.00059, global_step=2570.0, val_loss=0.109]]\n",
      "Epoch 34:  30%|██▉       | 31/104 [00:06<00:16,  4.44it/s, loss=0.00963, v_num=0, reduced_train_loss=0.00291, global_step=2579.0, val_loss=0.0616]\n",
      "Epoch 34:  21%|██        | 22/104 [00:05<00:20,  3.94it/s, loss=0.00154, v_num=0, reduced_train_loss=0.000371, global_step=2571.0, val_loss=0.109]\n",
      "Epoch 34:  31%|███       | 32/104 [00:07<00:16,  4.44it/s, loss=0.00954, v_num=0, reduced_train_loss=9.94e-5, global_step=2581.0, val_loss=0.0616]\n",
      "Epoch 34:  22%|██▏       | 23/104 [00:05<00:20,  3.95it/s, loss=0.00156, v_num=0, reduced_train_loss=0.000378, global_step=2572.0, val_loss=0.109]\n",
      "Epoch 34:  32%|███▏      | 33/104 [00:07<00:15,  4.44it/s, loss=0.00952, v_num=0, reduced_train_loss=4.43e-5, global_step=2582.0, val_loss=0.0616]\n",
      "Epoch 34:  23%|██▎       | 24/104 [00:06<00:20,  3.95it/s, loss=0.00151, v_num=0, reduced_train_loss=0.00013, global_step=2573.0, val_loss=0.109] \n",
      "Epoch 34:  33%|███▎      | 34/104 [00:07<00:15,  4.45it/s, loss=0.00952, v_num=0, reduced_train_loss=6.76e-5, global_step=2583.0, val_loss=0.0616]\n",
      "Epoch 34:  24%|██▍       | 25/104 [00:06<00:19,  3.95it/s, loss=0.00159, v_num=0, reduced_train_loss=0.00155, global_step=2574.0, val_loss=0.109]\n",
      "Epoch 34:  34%|███▎      | 35/104 [00:07<00:15,  4.45it/s, loss=0.0096, v_num=0, reduced_train_loss=0.00182, global_step=2584.0, val_loss=0.0616] \n",
      "Epoch 33:  92%|█████████▏| 96/104 [00:19<00:01,  4.84it/s, loss=0.0527, v_num=0, reduced_train_loss=0.00284, global_step=2549.0, val_loss=0.144]\n",
      "Epoch 34:  35%|███▍      | 36/104 [00:08<00:15,  4.45it/s, loss=0.0119, v_num=0, reduced_train_loss=0.0471, global_step=2585.0, val_loss=0.0616] ]\n",
      "Epoch 33:  94%|█████████▍| 98/104 [00:20<00:01,  4.88it/s, loss=0.0527, v_num=0, reduced_train_loss=0.00284, global_step=2549.0, val_loss=0.144]\n",
      "Epoch 34:  26%|██▌       | 27/104 [00:06<00:19,  3.95it/s, loss=0.00241, v_num=0, reduced_train_loss=0.0175, global_step=2576.0, val_loss=0.109]  ]\n",
      "Epoch 34:  37%|███▋      | 38/104 [00:08<00:14,  4.45it/s, loss=0.00467, v_num=0, reduced_train_loss=0.000116, global_step=2587.0, val_loss=0.0616]\n",
      "Epoch 34:  27%|██▋       | 28/104 [00:07<00:19,  3.96it/s, loss=0.00265, v_num=0, reduced_train_loss=0.00507, global_step=2577.0, val_loss=0.109]\n",
      "Epoch 34:  38%|███▊      | 39/104 [00:08<00:14,  4.45it/s, loss=0.00467, v_num=0, reduced_train_loss=0.000214, global_step=2588.0, val_loss=0.0616]\n",
      "Epoch 34:  28%|██▊       | 29/104 [00:07<00:18,  3.96it/s, loss=0.0023, v_num=0, reduced_train_loss=0.000219, global_step=2578.0, val_loss=0.109]\n",
      "Epoch 33: 100%|██████████| 104/104 [00:20<00:00,  5.01it/s, loss=0.0527, v_num=0, reduced_train_loss=0.00284, global_step=2549.0, val_loss=0.144]2023-05-03 22:07:54,600 - root - INFO - val_loss: 0.12539149820804596\n",
      "Epoch 33: 100%|██████████| 104/104 [00:20<00:00,  5.01it/s, loss=0.0527, v_num=0, reduced_train_loss=0.00284, global_step=2549.0, val_loss=0.125]\n",
      "Epoch 34:   0%|          | 0/104 [00:00<?, ?it/s, loss=0.0527, v_num=0, reduced_train_loss=0.00284, global_step=2549.0, val_loss=0.125]          "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 22:07:54.600974 139755592410944 fed_megatron_gpt_prompt_learning_model.py:99] val_loss: 0.12539149820804596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34:  33%|███▎      | 34/104 [00:08<00:17,  3.96it/s, loss=0.00691, v_num=0, reduced_train_loss=0.055, global_step=2583.0, val_loss=0.109]    "
     ]
    }
   ],
   "source": [
    "from nvflare import SimulatorRunner    \n",
    "\n",
    "simulator = SimulatorRunner(\n",
    "    job_folder=\"jobs/gpt_p-tuning_local\",\n",
    "    workspace=\"/tmp/nvflare/nemo/gpt_p-tuning_local\",\n",
    "    n_clients=3,\n",
    "    threads=3\n",
    ")\n",
    "run_status = simulator.run()\n",
    "print(\"Simulator finished with run_status\", run_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e626f7",
   "metadata": {},
   "source": [
    "#### 2. Federated P-Tuning\n",
    "We use the [FedAvg](https://arxiv.org/abs/1602.05629) algorithm to p-tune the model in a federated scenario. First, modify the configuration files again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8a07ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set ROOT_DIR to /workspace/Code/nvflare/nemo_nvflare/integration/nemo/examples/prompt_learning\n"
     ]
    }
   ],
   "source": [
    "!python3 modify_configs.py --job_folder \"jobs/gpt_p-tuning_fedavg\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401e4da1",
   "metadata": {},
   "source": [
    "Next, simulate the federated p-tuning using FedAvg. Here, each client p-tunes for one local epoch before sending their local model updates to the server for aggregation. This is repeated for 50 FL rounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9744398",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nvflare import SimulatorRunner    \n",
    "\n",
    "simulator = SimulatorRunner(\n",
    "    job_folder=\"jobs/gpt_p-tuning_fedavg\",\n",
    "    workspace=\"/tmp/nvflare/nemo/gpt_p-tuning_fedavg\",\n",
    "    n_clients=3,\n",
    "    threads=3\n",
    ")\n",
    "run_status = simulator.run()\n",
    "print(\"Simulator finished with run_status\", run_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17908b9e",
   "metadata": {},
   "source": [
    "You can visualize the training process using TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47c3b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir /tmp/nvflare/nemo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79718c43",
   "metadata": {},
   "source": [
    "## Results\n",
    "In this scenario, all clients utilize the same validation set, allowing for a direct comparison between the locally p-tuned and federated global models. As anticipated, the FedAvg-trained global model exhibits lower validation loss than the models trained solely on their local datasets. This is because the global model has access to all client datasets and can, consequently, generalize better.\n",
    "\n",
    "![validation loss](./val_loss.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a602f19",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "We can use `model.generate()` to run inference after p-tuning the model. \n",
    "Let's define some test examples to feed to the p-tuned model to see its predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c725d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_examples = [\n",
    "    {\"taskname\": \"sentiment\", \"sentence\": \"The products have a low salt and fat content .\"},\n",
    "    {\"taskname\": \"sentiment\", \"sentence\": \"The agreement is valid for four years .\"},\n",
    "    {\"taskname\": \"sentiment\", \"sentence\": \"Diluted EPS rose to EUR3 .68 from EUR0 .50 .\"},\n",
    "    {\"taskname\": \"sentiment\", \"sentence\": \"The company is well positioned in Brazil and Uruguay .\"},\n",
    "    {\"taskname\": \"sentiment\", \"sentence\": \"Profit before taxes decreased by 9 % to EUR 187.8 mn in the first nine months of 2008 , compared to EUR 207.1 mn a year earlier .\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffce9e08",
   "metadata": {},
   "source": [
    "Next, we will load the global model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d28cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from nemo_nvflare.fed_megatron_gpt_prompt_learning_model import FedMegatronGPTPromptLearningModel\n",
    "from nemo_nvflare.utils import load_weights\n",
    "from omegaconf import OmegaConf\n",
    "from nemo.collections.nlp.parts.nlp_overrides import NLPDDPStrategy\n",
    "from pytorch_lightning.plugins.environments import TorchElasticEnvironment\n",
    "\n",
    "# Load model configuration used by one of the clients\n",
    "config = OmegaConf.load(\"jobs/gpt_p-tuning_fedavg/app1/config/megatron_gpt_prompt_learning_config.yaml\")\n",
    "\n",
    "# Set GPT model path\n",
    "config.model.language_model_path = \"/workspace/Code/NeMo/upstream/tutorials/nlp/megatron_gpt_345m.nemo\"\n",
    "\n",
    "# Load task templates\n",
    "config.model.task_templates = OmegaConf.load(\"jobs/gpt_p-tuning_fedavg/app1/config/task_templates.json\")\n",
    "\n",
    "# Set task that were learned\n",
    "config.model.new_tasks = [\"sentiment\"]\n",
    "\n",
    "# Setup cluster environment parameters\n",
    "# use torch elastic cluster environment so `create_process_externally` is True\n",
    "# the launcher is set to None. It will not try to spawn new processes.\n",
    "# It won't create the misconfiguration error because of the `interactive session`\n",
    "os.environ[\"LOCAL_RANK\"] = '0'\n",
    "os.environ[\"RANK\"] = '0'\n",
    "os.environ[\"WORLD_SIZE\"] = '1'\n",
    "strategy = NLPDDPStrategy(find_unused_parameters=False, no_ddp_communication_hook=True)\n",
    "plugins = [TorchElasticEnvironment()]\n",
    "\n",
    "# Set up the trainer and load the model that was used for p-tuning\n",
    "trainer = pl.Trainer(plugins=plugins, strategy=strategy, **config.trainer)\n",
    "model = FedMegatronGPTPromptLearningModel(cfg=config.model, trainer=trainer)\n",
    "model.init_prompt_encoder()\n",
    "\n",
    "print(\"Model initialized\", type(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9020dc9d",
   "metadata": {},
   "source": [
    "Overwrite the prompt encoder with the best global model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec9cfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load(\"/tmp/nvflare/nemo/gpt_p-tuning_fedavg/simulate_job/app_server/best_FL_global_model.pt\")\n",
    "global_weights = ckpt[\"model\"]\n",
    "\n",
    "n_loaded = load_weights(model, global_weights, device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"))\n",
    "print(f\"Loaded {n_loaded} of {len(global_weights)} weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09143b3",
   "metadata": {},
   "source": [
    "Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f162e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.generate(inputs=test_examples, length_params=None)\n",
    "\n",
    "print('The prediction results of some sample queries with the trained model:')\n",
    "for result in response['sentences']:\n",
    "    print(result)\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cab13a",
   "metadata": {},
   "source": [
    "The expected output predictions look something like this\n",
    "\n",
    ">      The products have a low salt and fat content . sentiment: neutral\n",
    ">      ------------------------------\n",
    ">      The agreement is valid for four years . sentiment: neutral\n",
    ">      ------------------------------\n",
    ">      Diluted EPS rose to EUR3 .68 from EUR0 .50 . sentiment: positive\n",
    ">      ------------------------------\n",
    ">      The company is well positioned in Brazil and Uruguay . sentiment: positive\n",
    ">      ------------------------------\n",
    ">      Profit before taxes decreased by 9 % to EUR 187.8 mn in the first nine months of 2008 , compared to EUR 207.1 mn a year earlier . sentiment: negative\n",
    ">      ------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64846327",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
